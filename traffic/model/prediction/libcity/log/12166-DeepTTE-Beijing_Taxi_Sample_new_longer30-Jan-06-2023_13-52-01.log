2023-01-06 13:52:01,891 - INFO - Log directory: ./libcity/log
2023-01-06 13:52:01,891 - INFO - Begin pipeline, task=eta, model_name=DeepTTE, dataset_name=Beijing_Taxi_Sample_new_longer30, exp_id=12166
2023-01-06 13:52:01,892 - INFO - {'task': 'eta', 'model': 'DeepTTE', 'dataset': 'Beijing_Taxi_Sample_new_longer30', 'saved_model': True, 'train': True, 'seed': 0, 'batch_size': 64, 'dataset_class': 'ETADataset', 'eta_encoder': 'DeeptteEncoder', 'executor': 'ETAExecutor', 'evaluator': 'ETAEvaluator', 'uid_emb_size': 16, 'weekid_emb_size': 3, 'timdid_emb_size': 8, 'kernel_size': 3, 'num_filter': 32, 'pooling_method': 'attention', 'num_final_fcs': 4, 'final_fc_size': 128, 'alpha': 0.1, 'rnn_type': 'LSTM', 'rnn_num_layers': 1, 'hidden_size': 128, 'max_epoch': 100, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': False, 'use_early_stop': False, 'patience': 20, 'num_workers': 0, 'min_session_len': 5, 'max_session_len': 50, 'min_sessions': 0, 'window_size': 1, 'cut_method': 'time_interval', 'pad_with_last_sample': True, 'sort_by_traj_len': True, 'cache_dataset': True, 'train_rate': 0.7, 'eval_rate': 0.1, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'max_grad_norm': 1.0, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE', 'R2', 'EVAR'], 'mode': 'single', 'save_modes': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'coordinates': 'coordinate', 'embedding': 'other'}}, 'usr': {'properties': {}}, 'dyna': {'including_types': ['trajectory'], 'trajectory': {'entity_id': 'usr_id', 'traj_id': 'num', 'coordinates': 'coordinate', 'current_dis': 'num', 'speeds': 'other', 'speeds_relevant1': 'other', 'speeds_relevant2': 'other', 'speeds_long': 'other', 'grid_len': 'num', 'holiday': 'num'}}, 'geo_file': 'Beijing_Taxi_Sample_new_longer30', 'usr_file': 'Beijing_Taxi_Sample_new_longer30', 'dyna_file': 'Beijing_Taxi_Sample_new_longer30', 'device': device(type='cuda', index=0), 'exp_id': 12166}
2023-01-06 13:52:01,895 - INFO - Dataset created
2023-01-06 13:52:03,234 - INFO - Loaded file Beijing_Taxi_Sample_new_longer30.dyna, shape=(290813, 14)
2023-01-06 13:52:39,335 - INFO - Saved at ./libcity/cache/dataset_cache/eta_Beijing_Taxi_Sample_new_longer30_DeeptteEncoder.json
2023-01-06 13:52:39,497 - INFO - longi_mean: 116.38775224575281
2023-01-06 13:52:39,497 - INFO - longi_std: 0.07436581782419843
2023-01-06 13:52:39,497 - INFO - lati_mean: 39.92595912826014
2023-01-06 13:52:39,497 - INFO - lati_std: 0.051960612818695746
2023-01-06 13:52:39,498 - INFO - dist_mean: 46.394674660283926
2023-01-06 13:52:39,498 - INFO - dist_std: 24.736874488458618
2023-01-06 13:52:39,498 - INFO - time_mean: 2249.877973358706
2023-01-06 13:52:39,498 - INFO - time_std: 1145.3334062262293
2023-01-06 13:52:39,498 - INFO - dist_gap_mean: 0.9741931585426983
2023-01-06 13:52:39,498 - INFO - dist_gap_std: 4.724302966927774
2023-01-06 13:52:39,498 - INFO - time_gap_mean: 47.24283002847011
2023-01-06 13:52:39,498 - INFO - time_gap_std: 44.31597855637794
2023-01-06 13:52:39,513 - INFO - Number of train data: 4204
2023-01-06 13:52:39,513 - INFO - Number of eval  data: 587
2023-01-06 13:52:39,513 - INFO - Number of test  data: 1171
2023-01-06 13:52:42,317 - INFO - DeepTTE(
  (attr_net): Attr(
    (uid_em): Embedding(69, 16)
    (weekid_em): Embedding(7, 3)
    (timeid_em): Embedding(1440, 8)
  )
  (spatio_temporal): SpatioTemporal(
    (geo_conv): GeoConv(
      (state_em): Embedding(2, 2)
      (process_coords): Linear(in_features=4, out_features=16, bias=True)
      (conv): Conv1d(16, 32, kernel_size=(3,), stride=(1,))
    )
    (rnn): LSTM(61, 128, batch_first=True)
    (attr2atten): Linear(in_features=28, out_features=128, bias=True)
  )
  (entire_estimate): EntireEstimator(
    (input2hid): Linear(in_features=156, out_features=128, bias=True)
    (residuals): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Linear(in_features=128, out_features=128, bias=True)
    )
    (hid2out): Linear(in_features=128, out_features=1, bias=True)
  )
  (local_estimate): LocalEstimator(
    (input2hid): Linear(in_features=128, out_features=64, bias=True)
    (hid2hid): Linear(in_features=64, out_features=32, bias=True)
    (hid2out): Linear(in_features=32, out_features=1, bias=True)
  )
)
2023-01-06 13:52:42,318 - INFO - attr_net.uid_em.weight	torch.Size([69, 16])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - attr_net.weekid_em.weight	torch.Size([7, 3])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - attr_net.timeid_em.weight	torch.Size([1440, 8])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.geo_conv.state_em.weight	torch.Size([2, 2])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.geo_conv.process_coords.weight	torch.Size([16, 4])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.geo_conv.process_coords.bias	torch.Size([16])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.geo_conv.conv.weight	torch.Size([32, 16, 3])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.geo_conv.conv.bias	torch.Size([32])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.rnn.weight_ih_l0	torch.Size([512, 61])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.rnn.weight_hh_l0	torch.Size([512, 128])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.rnn.bias_ih_l0	torch.Size([512])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.rnn.bias_hh_l0	torch.Size([512])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.attr2atten.weight	torch.Size([128, 28])	cuda:0	True
2023-01-06 13:52:42,318 - INFO - spatio_temporal.attr2atten.bias	torch.Size([128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.input2hid.weight	torch.Size([128, 156])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.input2hid.bias	torch.Size([128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.residuals.0.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.residuals.0.bias	torch.Size([128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.residuals.1.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.residuals.1.bias	torch.Size([128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.residuals.2.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.residuals.2.bias	torch.Size([128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.residuals.3.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.residuals.3.bias	torch.Size([128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.hid2out.weight	torch.Size([1, 128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - entire_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - local_estimate.input2hid.weight	torch.Size([64, 128])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - local_estimate.input2hid.bias	torch.Size([64])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - local_estimate.hid2hid.weight	torch.Size([32, 64])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - local_estimate.hid2hid.bias	torch.Size([32])	cuda:0	True
2023-01-06 13:52:42,319 - INFO - local_estimate.hid2out.weight	torch.Size([1, 32])	cuda:0	True
2023-01-06 13:52:42,320 - INFO - local_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-06 13:52:42,320 - INFO - Total parameter numbers: 212443
2023-01-06 13:52:42,320 - INFO - You select `adam` optimizer.
2023-01-06 13:52:42,320 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-01-06 13:52:42,320 - INFO - Start training ...
2023-01-06 13:52:42,321 - INFO - num_batches:66
2023-01-06 13:52:45,104 - INFO - epoch complete!
2023-01-06 13:52:45,105 - INFO - evaluating now!
2023-01-06 13:52:45,275 - INFO - Epoch [0/100] train_loss: 0.5084, val_loss: 0.3693, lr: 0.001000, 2.95s
2023-01-06 13:52:45,284 - INFO - Saved model at 0
2023-01-06 13:52:45,284 - INFO - Val loss decrease from inf to 0.3693, saving to ./libcity/cache/12166/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch0.tar
2023-01-06 13:52:48,062 - INFO - epoch complete!
2023-01-06 13:52:48,062 - INFO - evaluating now!
2023-01-06 13:52:48,235 - INFO - Epoch [1/100] train_loss: 0.3273, val_loss: 0.3013, lr: 0.001000, 2.95s
2023-01-06 13:52:48,243 - INFO - Saved model at 1
2023-01-06 13:52:48,244 - INFO - Val loss decrease from 0.3693 to 0.3013, saving to ./libcity/cache/12166/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch1.tar
2023-01-06 13:52:50,960 - INFO - epoch complete!
2023-01-06 13:52:50,961 - INFO - evaluating now!
2023-01-06 13:52:51,136 - INFO - Epoch [2/100] train_loss: 0.3372, val_loss: 0.2352, lr: 0.001000, 2.89s
2023-01-06 13:52:51,145 - INFO - Saved model at 2
2023-01-06 13:52:51,145 - INFO - Val loss decrease from 0.3013 to 0.2352, saving to ./libcity/cache/12166/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch2.tar
2023-01-06 13:52:53,837 - INFO - epoch complete!
2023-01-06 13:52:53,838 - INFO - evaluating now!
2023-01-06 13:52:54,010 - INFO - Epoch [3/100] train_loss: 0.2929, val_loss: 0.2757, lr: 0.001000, 2.87s
2023-01-06 13:52:56,707 - INFO - epoch complete!
2023-01-06 13:52:56,707 - INFO - evaluating now!
2023-01-06 13:52:56,877 - INFO - Epoch [4/100] train_loss: 0.2663, val_loss: 0.2282, lr: 0.001000, 2.87s
2023-01-06 13:52:56,886 - INFO - Saved model at 4
2023-01-06 13:52:56,886 - INFO - Val loss decrease from 0.2352 to 0.2282, saving to ./libcity/cache/12166/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch4.tar
2023-01-06 13:52:59,599 - INFO - epoch complete!
2023-01-06 13:52:59,599 - INFO - evaluating now!
2023-01-06 13:52:59,769 - INFO - Epoch [5/100] train_loss: 0.2732, val_loss: 0.2311, lr: 0.001000, 2.88s
2023-01-06 13:53:02,507 - INFO - epoch complete!
2023-01-06 13:53:02,507 - INFO - evaluating now!
2023-01-06 13:53:02,676 - INFO - Epoch [6/100] train_loss: 0.2432, val_loss: 0.2084, lr: 0.001000, 2.91s
2023-01-06 13:53:02,685 - INFO - Saved model at 6
2023-01-06 13:53:02,685 - INFO - Val loss decrease from 0.2282 to 0.2084, saving to ./libcity/cache/12166/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch6.tar
2023-01-06 13:53:05,399 - INFO - epoch complete!
2023-01-06 13:53:05,399 - INFO - evaluating now!
2023-01-06 13:53:05,568 - INFO - Epoch [7/100] train_loss: 0.2331, val_loss: 0.2108, lr: 0.001000, 2.88s
2023-01-06 13:53:08,652 - INFO - epoch complete!
2023-01-06 13:53:08,652 - INFO - evaluating now!
2023-01-06 13:53:08,830 - INFO - Epoch [8/100] train_loss: 0.2143, val_loss: 0.2199, lr: 0.001000, 3.26s
2023-01-06 13:53:11,572 - INFO - epoch complete!
2023-01-06 13:53:11,572 - INFO - evaluating now!
2023-01-06 13:53:11,743 - INFO - Epoch [9/100] train_loss: 0.2176, val_loss: 0.2056, lr: 0.001000, 2.91s
2023-01-06 13:53:11,752 - INFO - Saved model at 9
2023-01-06 13:53:11,752 - INFO - Val loss decrease from 0.2084 to 0.2056, saving to ./libcity/cache/12166/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch9.tar
2023-01-06 13:53:14,451 - INFO - epoch complete!
2023-01-06 13:53:14,451 - INFO - evaluating now!
2023-01-06 13:53:14,621 - INFO - Epoch [10/100] train_loss: 0.2180, val_loss: 0.2062, lr: 0.001000, 2.87s
2023-01-06 13:53:17,341 - INFO - epoch complete!
2023-01-06 13:53:17,341 - INFO - evaluating now!
2023-01-06 13:53:17,513 - INFO - Epoch [11/100] train_loss: 0.2282, val_loss: 0.2080, lr: 0.001000, 2.89s
2023-01-06 13:53:20,183 - INFO - epoch complete!
2023-01-06 13:53:20,184 - INFO - evaluating now!
2023-01-06 13:53:20,354 - INFO - Epoch [12/100] train_loss: 0.2108, val_loss: 0.2158, lr: 0.001000, 2.84s
2023-01-06 13:53:23,165 - INFO - epoch complete!
2023-01-06 13:53:23,165 - INFO - evaluating now!
2023-01-06 13:53:23,346 - INFO - Epoch [13/100] train_loss: 0.2270, val_loss: 0.2303, lr: 0.001000, 2.99s
2023-01-06 13:53:26,343 - INFO - epoch complete!
2023-01-06 13:53:26,343 - INFO - evaluating now!
2023-01-06 13:53:26,527 - INFO - Epoch [14/100] train_loss: 0.2185, val_loss: 0.2028, lr: 0.001000, 3.18s
2023-01-06 13:53:26,537 - INFO - Saved model at 14
2023-01-06 13:53:26,537 - INFO - Val loss decrease from 0.2056 to 0.2028, saving to ./libcity/cache/12166/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch14.tar
2023-01-06 13:53:29,361 - INFO - epoch complete!
2023-01-06 13:53:29,362 - INFO - evaluating now!
2023-01-06 13:53:29,536 - INFO - Epoch [15/100] train_loss: 0.1994, val_loss: 0.2060, lr: 0.001000, 3.00s
2023-01-06 13:53:32,302 - INFO - epoch complete!
2023-01-06 13:53:32,302 - INFO - evaluating now!
2023-01-06 13:53:32,477 - INFO - Epoch [16/100] train_loss: 0.2009, val_loss: 0.2057, lr: 0.001000, 2.94s
2023-01-06 13:53:35,357 - INFO - epoch complete!
2023-01-06 13:53:35,357 - INFO - evaluating now!
2023-01-06 13:53:35,531 - INFO - Epoch [17/100] train_loss: 0.2007, val_loss: 0.2104, lr: 0.001000, 3.05s
2023-01-06 13:53:38,415 - INFO - epoch complete!
2023-01-06 13:53:38,416 - INFO - evaluating now!
2023-01-06 13:53:38,587 - INFO - Epoch [18/100] train_loss: 0.1904, val_loss: 0.2113, lr: 0.001000, 3.06s
2023-01-06 13:53:41,325 - INFO - epoch complete!
2023-01-06 13:53:41,325 - INFO - evaluating now!
2023-01-06 13:53:41,500 - INFO - Epoch [19/100] train_loss: 0.1945, val_loss: 0.2202, lr: 0.001000, 2.91s
2023-01-06 13:53:44,265 - INFO - epoch complete!
2023-01-06 13:53:44,265 - INFO - evaluating now!
2023-01-06 13:53:44,438 - INFO - Epoch [20/100] train_loss: 0.1914, val_loss: 0.2412, lr: 0.001000, 2.94s
2023-01-06 13:53:47,348 - INFO - epoch complete!
2023-01-06 13:53:47,348 - INFO - evaluating now!
2023-01-06 13:53:47,519 - INFO - Epoch [21/100] train_loss: 0.1931, val_loss: 0.2910, lr: 0.001000, 3.08s
2023-01-06 13:53:50,319 - INFO - epoch complete!
2023-01-06 13:53:50,320 - INFO - evaluating now!
2023-01-06 13:53:50,491 - INFO - Epoch [22/100] train_loss: 0.2138, val_loss: 0.2832, lr: 0.001000, 2.97s
2023-01-06 13:53:53,304 - INFO - epoch complete!
2023-01-06 13:53:53,304 - INFO - evaluating now!
2023-01-06 13:53:53,475 - INFO - Epoch [23/100] train_loss: 0.1965, val_loss: 0.2210, lr: 0.001000, 2.98s
2023-01-06 13:53:56,267 - INFO - epoch complete!
2023-01-06 13:53:56,267 - INFO - evaluating now!
2023-01-06 13:53:56,441 - INFO - Epoch [24/100] train_loss: 0.1851, val_loss: 0.2398, lr: 0.001000, 2.97s
2023-01-06 13:53:59,177 - INFO - epoch complete!
2023-01-06 13:53:59,177 - INFO - evaluating now!
2023-01-06 13:53:59,355 - INFO - Epoch [25/100] train_loss: 0.1881, val_loss: 0.2356, lr: 0.001000, 2.91s
2023-01-06 13:54:02,119 - INFO - epoch complete!
2023-01-06 13:54:02,119 - INFO - evaluating now!
2023-01-06 13:54:02,294 - INFO - Epoch [26/100] train_loss: 0.1919, val_loss: 0.2232, lr: 0.001000, 2.94s
2023-01-06 13:54:05,020 - INFO - epoch complete!
2023-01-06 13:54:05,020 - INFO - evaluating now!
2023-01-06 13:54:05,191 - INFO - Epoch [27/100] train_loss: 0.1956, val_loss: 0.2055, lr: 0.001000, 2.90s
2023-01-06 13:54:07,918 - INFO - epoch complete!
2023-01-06 13:54:07,919 - INFO - evaluating now!
2023-01-06 13:54:08,094 - INFO - Epoch [28/100] train_loss: 0.2053, val_loss: 0.2097, lr: 0.001000, 2.90s
2023-01-06 13:54:10,843 - INFO - epoch complete!
2023-01-06 13:54:10,844 - INFO - evaluating now!
2023-01-06 13:54:11,017 - INFO - Epoch [29/100] train_loss: 0.1844, val_loss: 0.2309, lr: 0.001000, 2.92s
2023-01-06 13:54:13,823 - INFO - epoch complete!
2023-01-06 13:54:13,824 - INFO - evaluating now!
2023-01-06 13:54:13,997 - INFO - Epoch [30/100] train_loss: 0.1779, val_loss: 0.2660, lr: 0.001000, 2.98s
2023-01-06 13:54:16,772 - INFO - epoch complete!
2023-01-06 13:54:16,772 - INFO - evaluating now!
2023-01-06 13:54:16,942 - INFO - Epoch [31/100] train_loss: 0.1783, val_loss: 0.2694, lr: 0.001000, 2.94s
2023-01-06 13:54:19,651 - INFO - epoch complete!
2023-01-06 13:54:19,652 - INFO - evaluating now!
2023-01-06 13:54:19,820 - INFO - Epoch [32/100] train_loss: 0.1777, val_loss: 0.2584, lr: 0.001000, 2.88s
2023-01-06 13:54:22,712 - INFO - epoch complete!
2023-01-06 13:54:22,713 - INFO - evaluating now!
2023-01-06 13:54:22,888 - INFO - Epoch [33/100] train_loss: 0.1786, val_loss: 0.2906, lr: 0.001000, 3.07s
2023-01-06 13:54:25,634 - INFO - epoch complete!
2023-01-06 13:54:25,634 - INFO - evaluating now!
2023-01-06 13:54:25,805 - INFO - Epoch [34/100] train_loss: 0.1862, val_loss: 0.2541, lr: 0.001000, 2.92s
2023-01-06 13:54:28,566 - INFO - epoch complete!
2023-01-06 13:54:28,567 - INFO - evaluating now!
2023-01-06 13:54:28,736 - INFO - Epoch [35/100] train_loss: 0.1852, val_loss: 0.2238, lr: 0.001000, 2.93s
2023-01-06 13:54:31,508 - INFO - epoch complete!
2023-01-06 13:54:31,508 - INFO - evaluating now!
2023-01-06 13:54:31,682 - INFO - Epoch [36/100] train_loss: 0.1890, val_loss: 0.2052, lr: 0.001000, 2.95s
2023-01-06 13:54:34,457 - INFO - epoch complete!
2023-01-06 13:54:34,457 - INFO - evaluating now!
2023-01-06 13:54:34,627 - INFO - Epoch [37/100] train_loss: 0.1778, val_loss: 0.2524, lr: 0.001000, 2.94s
2023-01-06 13:54:37,341 - INFO - epoch complete!
2023-01-06 13:54:37,341 - INFO - evaluating now!
2023-01-06 13:54:37,509 - INFO - Epoch [38/100] train_loss: 0.1745, val_loss: 0.3000, lr: 0.001000, 2.88s
2023-01-06 13:54:40,227 - INFO - epoch complete!
2023-01-06 13:54:40,228 - INFO - evaluating now!
2023-01-06 13:54:40,398 - INFO - Epoch [39/100] train_loss: 0.1697, val_loss: 0.3043, lr: 0.001000, 2.89s
2023-01-06 13:54:43,104 - INFO - epoch complete!
2023-01-06 13:54:43,105 - INFO - evaluating now!
2023-01-06 13:54:43,274 - INFO - Epoch [40/100] train_loss: 0.1784, val_loss: 0.2745, lr: 0.001000, 2.88s
2023-01-06 13:54:46,093 - INFO - epoch complete!
2023-01-06 13:54:46,094 - INFO - evaluating now!
2023-01-06 13:54:46,276 - INFO - Epoch [41/100] train_loss: 0.1713, val_loss: 0.2309, lr: 0.001000, 3.00s
2023-01-06 13:54:49,193 - INFO - epoch complete!
2023-01-06 13:54:49,194 - INFO - evaluating now!
2023-01-06 13:54:49,368 - INFO - Epoch [42/100] train_loss: 0.1723, val_loss: 0.2328, lr: 0.001000, 3.09s
2023-01-06 13:54:52,176 - INFO - epoch complete!
2023-01-06 13:54:52,177 - INFO - evaluating now!
2023-01-06 13:54:52,362 - INFO - Epoch [43/100] train_loss: 0.1701, val_loss: 0.2881, lr: 0.001000, 2.99s
2023-01-06 13:54:55,179 - INFO - epoch complete!
2023-01-06 13:54:55,180 - INFO - evaluating now!
2023-01-06 13:54:55,359 - INFO - Epoch [44/100] train_loss: 0.1669, val_loss: 0.3208, lr: 0.001000, 3.00s
2023-01-06 13:54:58,328 - INFO - epoch complete!
2023-01-06 13:54:58,329 - INFO - evaluating now!
2023-01-06 13:54:58,501 - INFO - Epoch [45/100] train_loss: 0.1669, val_loss: 0.3146, lr: 0.001000, 3.14s
2023-01-06 13:55:01,369 - INFO - epoch complete!
2023-01-06 13:55:01,370 - INFO - evaluating now!
2023-01-06 13:55:01,551 - INFO - Epoch [46/100] train_loss: 0.1607, val_loss: 0.3003, lr: 0.001000, 3.05s
2023-01-06 13:55:04,385 - INFO - epoch complete!
2023-01-06 13:55:04,385 - INFO - evaluating now!
2023-01-06 13:55:04,563 - INFO - Epoch [47/100] train_loss: 0.1696, val_loss: 0.2697, lr: 0.001000, 3.01s
2023-01-06 13:55:07,387 - INFO - epoch complete!
2023-01-06 13:55:07,387 - INFO - evaluating now!
2023-01-06 13:55:07,566 - INFO - Epoch [48/100] train_loss: 0.1717, val_loss: 0.2178, lr: 0.001000, 3.00s
2023-01-06 13:55:10,372 - INFO - epoch complete!
2023-01-06 13:55:10,373 - INFO - evaluating now!
2023-01-06 13:55:10,548 - INFO - Epoch [49/100] train_loss: 0.1712, val_loss: 0.2309, lr: 0.001000, 2.98s
2023-01-06 13:55:13,341 - INFO - epoch complete!
2023-01-06 13:55:13,342 - INFO - evaluating now!
2023-01-06 13:55:13,515 - INFO - Epoch [50/100] train_loss: 0.1631, val_loss: 0.2519, lr: 0.001000, 2.97s
2023-01-06 13:55:16,291 - INFO - epoch complete!
2023-01-06 13:55:16,292 - INFO - evaluating now!
2023-01-06 13:55:16,462 - INFO - Epoch [51/100] train_loss: 0.1630, val_loss: 0.2831, lr: 0.001000, 2.95s
2023-01-06 13:55:19,249 - INFO - epoch complete!
2023-01-06 13:55:19,250 - INFO - evaluating now!
2023-01-06 13:55:19,421 - INFO - Epoch [52/100] train_loss: 0.1548, val_loss: 0.2692, lr: 0.001000, 2.96s
2023-01-06 13:55:22,154 - INFO - epoch complete!
2023-01-06 13:55:22,154 - INFO - evaluating now!
2023-01-06 13:55:22,322 - INFO - Epoch [53/100] train_loss: 0.1510, val_loss: 0.3003, lr: 0.001000, 2.90s
2023-01-06 13:55:25,023 - INFO - epoch complete!
2023-01-06 13:55:25,024 - INFO - evaluating now!
2023-01-06 13:55:25,190 - INFO - Epoch [54/100] train_loss: 0.1488, val_loss: 0.3062, lr: 0.001000, 2.87s
2023-01-06 13:55:27,967 - INFO - epoch complete!
2023-01-06 13:55:27,967 - INFO - evaluating now!
2023-01-06 13:55:28,153 - INFO - Epoch [55/100] train_loss: 0.1490, val_loss: 0.3277, lr: 0.001000, 2.96s
2023-01-06 13:55:30,943 - INFO - epoch complete!
2023-01-06 13:55:30,943 - INFO - evaluating now!
2023-01-06 13:55:31,117 - INFO - Epoch [56/100] train_loss: 0.1545, val_loss: 0.3009, lr: 0.001000, 2.96s
2023-01-06 13:55:33,935 - INFO - epoch complete!
2023-01-06 13:55:33,936 - INFO - evaluating now!
2023-01-06 13:55:34,118 - INFO - Epoch [57/100] train_loss: 0.1601, val_loss: 0.2508, lr: 0.001000, 3.00s
2023-01-06 13:55:36,935 - INFO - epoch complete!
2023-01-06 13:55:36,935 - INFO - evaluating now!
2023-01-06 13:55:37,108 - INFO - Epoch [58/100] train_loss: 0.1542, val_loss: 0.2396, lr: 0.001000, 2.99s
2023-01-06 13:55:39,774 - INFO - epoch complete!
2023-01-06 13:55:39,775 - INFO - evaluating now!
2023-01-06 13:55:39,945 - INFO - Epoch [59/100] train_loss: 0.1527, val_loss: 0.2380, lr: 0.001000, 2.84s
2023-01-06 13:55:42,698 - INFO - epoch complete!
2023-01-06 13:55:42,698 - INFO - evaluating now!
2023-01-06 13:55:42,867 - INFO - Epoch [60/100] train_loss: 0.1546, val_loss: 0.2200, lr: 0.001000, 2.92s
2023-01-06 13:55:45,535 - INFO - epoch complete!
2023-01-06 13:55:45,535 - INFO - evaluating now!
2023-01-06 13:55:45,769 - INFO - Epoch [61/100] train_loss: 0.1534, val_loss: 0.2406, lr: 0.001000, 2.90s
2023-01-06 13:55:48,497 - INFO - epoch complete!
2023-01-06 13:55:48,497 - INFO - evaluating now!
2023-01-06 13:55:48,664 - INFO - Epoch [62/100] train_loss: 0.1477, val_loss: 0.2456, lr: 0.001000, 2.89s
2023-01-06 13:55:51,374 - INFO - epoch complete!
2023-01-06 13:55:51,374 - INFO - evaluating now!
2023-01-06 13:55:51,540 - INFO - Epoch [63/100] train_loss: 0.1526, val_loss: 0.2219, lr: 0.001000, 2.88s
2023-01-06 13:55:54,198 - INFO - epoch complete!
2023-01-06 13:55:54,199 - INFO - evaluating now!
2023-01-06 13:55:54,371 - INFO - Epoch [64/100] train_loss: 0.1377, val_loss: 0.2292, lr: 0.001000, 2.83s
2023-01-06 13:55:57,203 - INFO - epoch complete!
2023-01-06 13:55:57,203 - INFO - evaluating now!
2023-01-06 13:55:57,368 - INFO - Epoch [65/100] train_loss: 0.1432, val_loss: 0.2258, lr: 0.001000, 3.00s
2023-01-06 13:56:00,205 - INFO - epoch complete!
2023-01-06 13:56:00,205 - INFO - evaluating now!
2023-01-06 13:56:00,372 - INFO - Epoch [66/100] train_loss: 0.1342, val_loss: 0.2140, lr: 0.001000, 3.00s
2023-01-06 13:56:03,071 - INFO - epoch complete!
2023-01-06 13:56:03,071 - INFO - evaluating now!
2023-01-06 13:56:03,237 - INFO - Epoch [67/100] train_loss: 0.1353, val_loss: 0.2106, lr: 0.001000, 2.86s
2023-01-06 13:56:05,925 - INFO - epoch complete!
2023-01-06 13:56:05,926 - INFO - evaluating now!
2023-01-06 13:56:06,097 - INFO - Epoch [68/100] train_loss: 0.1394, val_loss: 0.2097, lr: 0.001000, 2.86s
2023-01-06 13:56:08,871 - INFO - epoch complete!
2023-01-06 13:56:08,871 - INFO - evaluating now!
2023-01-06 13:56:09,054 - INFO - Epoch [69/100] train_loss: 0.1278, val_loss: 0.2179, lr: 0.001000, 2.96s
2023-01-06 13:56:12,093 - INFO - epoch complete!
2023-01-06 13:56:12,094 - INFO - evaluating now!
2023-01-06 13:56:12,273 - INFO - Epoch [70/100] train_loss: 0.1332, val_loss: 0.2116, lr: 0.001000, 3.22s
2023-01-06 13:56:15,245 - INFO - epoch complete!
2023-01-06 13:56:15,245 - INFO - evaluating now!
2023-01-06 13:56:15,425 - INFO - Epoch [71/100] train_loss: 0.1255, val_loss: 0.2308, lr: 0.001000, 3.15s
2023-01-06 13:56:18,237 - INFO - epoch complete!
2023-01-06 13:56:18,238 - INFO - evaluating now!
2023-01-06 13:56:18,408 - INFO - Epoch [72/100] train_loss: 0.1278, val_loss: 0.2299, lr: 0.001000, 2.98s
2023-01-06 13:56:21,144 - INFO - epoch complete!
2023-01-06 13:56:21,144 - INFO - evaluating now!
2023-01-06 13:56:21,313 - INFO - Epoch [73/100] train_loss: 0.1356, val_loss: 0.2447, lr: 0.001000, 2.90s
2023-01-06 13:56:24,155 - INFO - epoch complete!
2023-01-06 13:56:24,155 - INFO - evaluating now!
2023-01-06 13:56:24,334 - INFO - Epoch [74/100] train_loss: 0.1291, val_loss: 0.2287, lr: 0.001000, 3.02s
2023-01-06 13:56:27,236 - INFO - epoch complete!
2023-01-06 13:56:27,236 - INFO - evaluating now!
2023-01-06 13:56:27,410 - INFO - Epoch [75/100] train_loss: 0.1318, val_loss: 0.2242, lr: 0.001000, 3.08s
2023-01-06 13:56:30,266 - INFO - epoch complete!
2023-01-06 13:56:30,267 - INFO - evaluating now!
2023-01-06 13:56:30,443 - INFO - Epoch [76/100] train_loss: 0.1381, val_loss: 0.2141, lr: 0.001000, 3.03s
2023-01-06 13:56:33,355 - INFO - epoch complete!
2023-01-06 13:56:33,356 - INFO - evaluating now!
2023-01-06 13:56:33,528 - INFO - Epoch [77/100] train_loss: 0.1350, val_loss: 0.2162, lr: 0.001000, 3.08s
2023-01-06 13:56:36,315 - INFO - epoch complete!
2023-01-06 13:56:36,316 - INFO - evaluating now!
2023-01-06 13:56:36,486 - INFO - Epoch [78/100] train_loss: 0.1299, val_loss: 0.2206, lr: 0.001000, 2.96s
2023-01-06 13:56:39,207 - INFO - epoch complete!
2023-01-06 13:56:39,207 - INFO - evaluating now!
2023-01-06 13:56:39,378 - INFO - Epoch [79/100] train_loss: 0.1283, val_loss: 0.2297, lr: 0.001000, 2.89s
2023-01-06 13:56:42,147 - INFO - epoch complete!
2023-01-06 13:56:42,147 - INFO - evaluating now!
2023-01-06 13:56:42,315 - INFO - Epoch [80/100] train_loss: 0.1234, val_loss: 0.2469, lr: 0.001000, 2.94s
2023-01-06 13:56:45,040 - INFO - epoch complete!
2023-01-06 13:56:45,040 - INFO - evaluating now!
2023-01-06 13:56:45,220 - INFO - Epoch [81/100] train_loss: 0.1278, val_loss: 0.2564, lr: 0.001000, 2.90s
2023-01-06 13:56:48,328 - INFO - epoch complete!
2023-01-06 13:56:48,329 - INFO - evaluating now!
2023-01-06 13:56:48,496 - INFO - Epoch [82/100] train_loss: 0.1209, val_loss: 0.3086, lr: 0.001000, 3.28s
2023-01-06 13:56:51,172 - INFO - epoch complete!
2023-01-06 13:56:51,173 - INFO - evaluating now!
2023-01-06 13:56:51,339 - INFO - Epoch [83/100] train_loss: 0.1236, val_loss: 0.2898, lr: 0.001000, 2.84s
2023-01-06 13:56:54,339 - INFO - epoch complete!
2023-01-06 13:56:54,340 - INFO - evaluating now!
2023-01-06 13:56:54,527 - INFO - Epoch [84/100] train_loss: 0.1178, val_loss: 0.3048, lr: 0.001000, 3.19s
2023-01-06 13:56:57,173 - INFO - epoch complete!
2023-01-06 13:56:57,173 - INFO - evaluating now!
2023-01-06 13:56:57,347 - INFO - Epoch [85/100] train_loss: 0.1159, val_loss: 0.3054, lr: 0.001000, 2.82s
2023-01-06 13:57:00,010 - INFO - epoch complete!
2023-01-06 13:57:00,010 - INFO - evaluating now!
2023-01-06 13:57:00,176 - INFO - Epoch [86/100] train_loss: 0.1318, val_loss: 0.3106, lr: 0.001000, 2.83s
2023-01-06 13:57:02,829 - INFO - epoch complete!
2023-01-06 13:57:02,830 - INFO - evaluating now!
2023-01-06 13:57:03,006 - INFO - Epoch [87/100] train_loss: 0.1266, val_loss: 0.2572, lr: 0.001000, 2.83s
2023-01-06 13:57:05,807 - INFO - epoch complete!
2023-01-06 13:57:05,808 - INFO - evaluating now!
2023-01-06 13:57:05,980 - INFO - Epoch [88/100] train_loss: 0.1340, val_loss: 0.2440, lr: 0.001000, 2.97s
2023-01-06 13:57:08,932 - INFO - epoch complete!
2023-01-06 13:57:08,933 - INFO - evaluating now!
2023-01-06 13:57:09,108 - INFO - Epoch [89/100] train_loss: 0.1212, val_loss: 0.2297, lr: 0.001000, 3.13s
2023-01-06 13:57:11,950 - INFO - epoch complete!
2023-01-06 13:57:11,951 - INFO - evaluating now!
2023-01-06 13:57:12,122 - INFO - Epoch [90/100] train_loss: 0.1135, val_loss: 0.2326, lr: 0.001000, 3.01s
2023-01-06 13:57:14,915 - INFO - epoch complete!
2023-01-06 13:57:14,916 - INFO - evaluating now!
2023-01-06 13:57:15,091 - INFO - Epoch [91/100] train_loss: 0.1133, val_loss: 0.2244, lr: 0.001000, 2.97s
2023-01-06 13:57:18,032 - INFO - epoch complete!
2023-01-06 13:57:18,032 - INFO - evaluating now!
2023-01-06 13:57:18,217 - INFO - Epoch [92/100] train_loss: 0.1003, val_loss: 0.2208, lr: 0.001000, 3.13s
2023-01-06 13:57:20,911 - INFO - epoch complete!
2023-01-06 13:57:20,911 - INFO - evaluating now!
2023-01-06 13:57:21,082 - INFO - Epoch [93/100] train_loss: 0.0942, val_loss: 0.2274, lr: 0.001000, 2.86s
2023-01-06 13:57:23,820 - INFO - epoch complete!
2023-01-06 13:57:23,820 - INFO - evaluating now!
2023-01-06 13:57:23,992 - INFO - Epoch [94/100] train_loss: 0.0946, val_loss: 0.2283, lr: 0.001000, 2.91s
2023-01-06 13:57:26,786 - INFO - epoch complete!
2023-01-06 13:57:26,786 - INFO - evaluating now!
2023-01-06 13:57:26,960 - INFO - Epoch [95/100] train_loss: 0.0910, val_loss: 0.2278, lr: 0.001000, 2.97s
2023-01-06 13:57:29,711 - INFO - epoch complete!
2023-01-06 13:57:29,712 - INFO - evaluating now!
2023-01-06 13:57:29,879 - INFO - Epoch [96/100] train_loss: 0.0993, val_loss: 0.2249, lr: 0.001000, 2.92s
2023-01-06 13:57:32,535 - INFO - epoch complete!
2023-01-06 13:57:32,535 - INFO - evaluating now!
2023-01-06 13:57:32,775 - INFO - Epoch [97/100] train_loss: 0.0988, val_loss: 0.2262, lr: 0.001000, 2.90s
2023-01-06 13:57:35,443 - INFO - epoch complete!
2023-01-06 13:57:35,444 - INFO - evaluating now!
2023-01-06 13:57:35,615 - INFO - Epoch [98/100] train_loss: 0.1025, val_loss: 0.2319, lr: 0.001000, 2.84s
2023-01-06 13:57:38,311 - INFO - epoch complete!
2023-01-06 13:57:38,312 - INFO - evaluating now!
2023-01-06 13:57:38,477 - INFO - Epoch [99/100] train_loss: 0.1055, val_loss: 0.2396, lr: 0.001000, 2.86s
2023-01-06 13:57:38,478 - INFO - Trained totally 100 epochs, average train time is 2.785s, average eval time is 0.174s
2023-01-06 13:57:38,488 - INFO - Loaded model at 14
2023-01-06 13:57:38,489 - INFO - Saved model at ./libcity/cache/12166/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30.m
2023-01-06 13:57:38,497 - INFO - Start evaluating ...
2023-01-06 13:57:39,113 - INFO - Evaluate result is saved at ./libcity/cache/12166/evaluate_cache/2023_01_06_13_57_39_DeepTTE_Beijing_Taxi_Sample_new_longer30.csv
2023-01-06 13:57:39,124 - INFO - 
          MAE      MAPE           MSE        RMSE  masked_MAE  masked_MAPE    masked_MSE  masked_RMSE       R2      EVAR
1  495.294464  0.216082  474867.65625  689.106445  495.294464     0.216082  474867.65625   689.106445  0.59403  0.671792

2023-01-06 17:33:36,585 - INFO - Log directory: ./libcity/log
2023-01-06 17:33:36,586 - INFO - Begin pipeline, task=eta, model_name=DeepTTE, dataset_name=Beijing_Taxi_Sample_new_longer30, exp_id=5
2023-01-06 17:33:36,586 - INFO - {'task': 'eta', 'model': 'DeepTTE', 'dataset': 'Beijing_Taxi_Sample_new_longer30', 'saved_model': True, 'train': True, 'exp_id': '5', 'seed': 0, 'batch_size': 64, 'dataset_class': 'ETADataset', 'eta_encoder': 'DeeptteEncoder', 'executor': 'ETAExecutor', 'evaluator': 'ETAEvaluator', 'uid_emb_size': 16, 'weekid_emb_size': 3, 'timdid_emb_size': 8, 'kernel_size': 3, 'num_filter': 32, 'pooling_method': 'attention', 'num_final_fcs': 4, 'final_fc_size': 128, 'alpha': 0.1, 'rnn_type': 'LSTM', 'rnn_num_layers': 1, 'hidden_size': 128, 'max_epoch': 100, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': False, 'use_early_stop': False, 'patience': 20, 'num_workers': 0, 'min_session_len': 5, 'max_session_len': 50, 'min_sessions': 0, 'window_size': 1, 'cut_method': 'time_interval', 'pad_with_last_sample': True, 'sort_by_traj_len': True, 'cache_dataset': True, 'train_rate': 0.7, 'eval_rate': 0.1, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'max_grad_norm': 1.0, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE', 'R2', 'EVAR'], 'mode': 'single', 'save_modes': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'coordinates': 'coordinate', 'embedding': 'other'}}, 'usr': {'properties': {}}, 'dyna': {'including_types': ['trajectory'], 'trajectory': {'entity_id': 'usr_id', 'traj_id': 'num', 'coordinates': 'coordinate', 'current_dis': 'num', 'speeds': 'other', 'speeds_relevant1': 'other', 'speeds_relevant2': 'other', 'speeds_long': 'other', 'grid_len': 'num', 'holiday': 'num'}}, 'geo_file': 'Beijing_Taxi_Sample_new_longer30', 'usr_file': 'Beijing_Taxi_Sample_new_longer30', 'dyna_file': 'Beijing_Taxi_Sample_new_longer30', 'device': device(type='cuda', index=0)}
2023-01-06 17:33:36,593 - INFO - Dataset created
2023-01-06 17:33:37,862 - INFO - Loaded file Beijing_Taxi_Sample_new_longer30.dyna, shape=(290813, 14)
2023-01-06 17:34:13,503 - INFO - Saved at ./libcity/cache/dataset_cache/eta_Beijing_Taxi_Sample_new_longer30_DeeptteEncoder.json
2023-01-06 17:34:13,657 - INFO - longi_mean: 116.3877277762501
2023-01-06 17:34:13,657 - INFO - longi_std: 0.07261763696944946
2023-01-06 17:34:13,657 - INFO - lati_mean: 39.92589546626833
2023-01-06 17:34:13,657 - INFO - lati_std: 0.04941081329032585
2023-01-06 17:34:13,657 - INFO - dist_mean: 13.354736771240418
2023-01-06 17:34:13,658 - INFO - dist_std: 5.228743572273171
2023-01-06 17:34:13,658 - INFO - time_mean: 2249.877973358706
2023-01-06 17:34:13,658 - INFO - time_std: 1145.3334062262293
2023-01-06 17:34:13,658 - INFO - dist_gap_mean: 0.2804221237015869
2023-01-06 17:34:13,658 - INFO - dist_gap_std: 0.2571732687726144
2023-01-06 17:34:13,658 - INFO - time_gap_mean: 47.24283002847011
2023-01-06 17:34:13,658 - INFO - time_gap_std: 44.31597855637794
2023-01-06 17:34:13,673 - INFO - Number of train data: 4204
2023-01-06 17:34:13,673 - INFO - Number of eval  data: 587
2023-01-06 17:34:13,673 - INFO - Number of test  data: 1171
2023-01-06 17:34:16,316 - INFO - DeepTTE(
  (attr_net): Attr(
    (uid_em): Embedding(69, 16)
    (weekid_em): Embedding(7, 3)
    (timeid_em): Embedding(1440, 8)
  )
  (spatio_temporal): SpatioTemporal(
    (geo_conv): GeoConv(
      (state_em): Embedding(2, 2)
      (process_coords): Linear(in_features=4, out_features=16, bias=True)
      (conv): Conv1d(16, 32, kernel_size=(3,), stride=(1,))
    )
    (rnn): LSTM(61, 128, batch_first=True)
    (attr2atten): Linear(in_features=28, out_features=128, bias=True)
  )
  (entire_estimate): EntireEstimator(
    (input2hid): Linear(in_features=156, out_features=128, bias=True)
    (residuals): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Linear(in_features=128, out_features=128, bias=True)
    )
    (hid2out): Linear(in_features=128, out_features=1, bias=True)
  )
  (local_estimate): LocalEstimator(
    (input2hid): Linear(in_features=128, out_features=64, bias=True)
    (hid2hid): Linear(in_features=64, out_features=32, bias=True)
    (hid2out): Linear(in_features=32, out_features=1, bias=True)
  )
)
2023-01-06 17:34:16,317 - INFO - attr_net.uid_em.weight	torch.Size([69, 16])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - attr_net.weekid_em.weight	torch.Size([7, 3])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - attr_net.timeid_em.weight	torch.Size([1440, 8])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - spatio_temporal.geo_conv.state_em.weight	torch.Size([2, 2])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - spatio_temporal.geo_conv.process_coords.weight	torch.Size([16, 4])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - spatio_temporal.geo_conv.process_coords.bias	torch.Size([16])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - spatio_temporal.geo_conv.conv.weight	torch.Size([32, 16, 3])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - spatio_temporal.geo_conv.conv.bias	torch.Size([32])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - spatio_temporal.rnn.weight_ih_l0	torch.Size([512, 61])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - spatio_temporal.rnn.weight_hh_l0	torch.Size([512, 128])	cuda:0	True
2023-01-06 17:34:16,317 - INFO - spatio_temporal.rnn.bias_ih_l0	torch.Size([512])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - spatio_temporal.rnn.bias_hh_l0	torch.Size([512])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - spatio_temporal.attr2atten.weight	torch.Size([128, 28])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - spatio_temporal.attr2atten.bias	torch.Size([128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.input2hid.weight	torch.Size([128, 156])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.input2hid.bias	torch.Size([128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.residuals.0.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.residuals.0.bias	torch.Size([128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.residuals.1.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.residuals.1.bias	torch.Size([128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.residuals.2.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.residuals.2.bias	torch.Size([128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.residuals.3.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.residuals.3.bias	torch.Size([128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.hid2out.weight	torch.Size([1, 128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - entire_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - local_estimate.input2hid.weight	torch.Size([64, 128])	cuda:0	True
2023-01-06 17:34:16,318 - INFO - local_estimate.input2hid.bias	torch.Size([64])	cuda:0	True
2023-01-06 17:34:16,319 - INFO - local_estimate.hid2hid.weight	torch.Size([32, 64])	cuda:0	True
2023-01-06 17:34:16,319 - INFO - local_estimate.hid2hid.bias	torch.Size([32])	cuda:0	True
2023-01-06 17:34:16,319 - INFO - local_estimate.hid2out.weight	torch.Size([1, 32])	cuda:0	True
2023-01-06 17:34:16,319 - INFO - local_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-06 17:34:16,319 - INFO - Total parameter numbers: 212443
2023-01-06 17:34:16,319 - INFO - You select `adam` optimizer.
2023-01-06 17:34:16,320 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-01-06 17:34:16,320 - INFO - Start training ...
2023-01-06 17:34:16,320 - INFO - num_batches:66
2023-01-06 17:34:18,902 - INFO - epoch complete!
2023-01-06 17:34:18,902 - INFO - evaluating now!
2023-01-06 17:34:19,060 - INFO - Epoch [0/100] train_loss: 0.4591, val_loss: 0.3040, lr: 0.001000, 2.74s
2023-01-06 17:34:19,069 - INFO - Saved model at 0
2023-01-06 17:34:19,069 - INFO - Val loss decrease from inf to 0.3040, saving to ./libcity/cache/5/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch0.tar
2023-01-06 17:34:21,638 - INFO - epoch complete!
2023-01-06 17:34:21,638 - INFO - evaluating now!
2023-01-06 17:34:21,795 - INFO - Epoch [1/100] train_loss: 0.3441, val_loss: 0.2493, lr: 0.001000, 2.73s
2023-01-06 17:34:21,803 - INFO - Saved model at 1
2023-01-06 17:34:21,804 - INFO - Val loss decrease from 0.3040 to 0.2493, saving to ./libcity/cache/5/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch1.tar
2023-01-06 17:34:24,317 - INFO - epoch complete!
2023-01-06 17:34:24,317 - INFO - evaluating now!
2023-01-06 17:34:24,473 - INFO - Epoch [2/100] train_loss: 0.3422, val_loss: 0.2899, lr: 0.001000, 2.67s
2023-01-06 17:34:26,909 - INFO - epoch complete!
2023-01-06 17:34:26,909 - INFO - evaluating now!
2023-01-06 17:34:27,066 - INFO - Epoch [3/100] train_loss: 0.2860, val_loss: 0.2266, lr: 0.001000, 2.59s
2023-01-06 17:34:27,075 - INFO - Saved model at 3
2023-01-06 17:34:27,075 - INFO - Val loss decrease from 0.2493 to 0.2266, saving to ./libcity/cache/5/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch3.tar
2023-01-06 17:34:29,590 - INFO - epoch complete!
2023-01-06 17:34:29,591 - INFO - evaluating now!
2023-01-06 17:34:29,749 - INFO - Epoch [4/100] train_loss: 0.2582, val_loss: 0.2674, lr: 0.001000, 2.67s
2023-01-06 17:34:32,280 - INFO - epoch complete!
2023-01-06 17:34:32,281 - INFO - evaluating now!
2023-01-06 17:34:32,439 - INFO - Epoch [5/100] train_loss: 0.2459, val_loss: 0.2095, lr: 0.001000, 2.69s
2023-01-06 17:34:32,448 - INFO - Saved model at 5
2023-01-06 17:34:32,448 - INFO - Val loss decrease from 0.2266 to 0.2095, saving to ./libcity/cache/5/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch5.tar
2023-01-06 17:34:35,019 - INFO - epoch complete!
2023-01-06 17:34:35,019 - INFO - evaluating now!
2023-01-06 17:34:35,178 - INFO - Epoch [6/100] train_loss: 0.2164, val_loss: 0.2921, lr: 0.001000, 2.73s
2023-01-06 17:34:37,724 - INFO - epoch complete!
2023-01-06 17:34:37,724 - INFO - evaluating now!
2023-01-06 17:34:37,878 - INFO - Epoch [7/100] train_loss: 0.2404, val_loss: 0.2004, lr: 0.001000, 2.70s
2023-01-06 17:34:37,886 - INFO - Saved model at 7
2023-01-06 17:34:37,886 - INFO - Val loss decrease from 0.2095 to 0.2004, saving to ./libcity/cache/5/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch7.tar
2023-01-06 17:34:40,795 - INFO - epoch complete!
2023-01-06 17:34:40,796 - INFO - evaluating now!
2023-01-06 17:34:40,954 - INFO - Epoch [8/100] train_loss: 0.2057, val_loss: 0.2070, lr: 0.001000, 3.07s
2023-01-06 17:34:43,613 - INFO - epoch complete!
2023-01-06 17:34:43,614 - INFO - evaluating now!
2023-01-06 17:34:43,784 - INFO - Epoch [9/100] train_loss: 0.2160, val_loss: 0.2094, lr: 0.001000, 2.83s
2023-01-06 17:34:46,629 - INFO - epoch complete!
2023-01-06 17:34:46,630 - INFO - evaluating now!
2023-01-06 17:34:46,806 - INFO - Epoch [10/100] train_loss: 0.2210, val_loss: 0.2129, lr: 0.001000, 3.02s
2023-01-06 17:34:49,569 - INFO - epoch complete!
2023-01-06 17:34:49,570 - INFO - evaluating now!
2023-01-06 17:34:49,732 - INFO - Epoch [11/100] train_loss: 0.2155, val_loss: 0.2795, lr: 0.001000, 2.93s
2023-01-06 17:34:52,408 - INFO - epoch complete!
2023-01-06 17:34:52,410 - INFO - evaluating now!
2023-01-06 17:34:52,584 - INFO - Epoch [12/100] train_loss: 0.2444, val_loss: 0.2182, lr: 0.001000, 2.85s
2023-01-06 17:34:55,264 - INFO - epoch complete!
2023-01-06 17:34:55,265 - INFO - evaluating now!
2023-01-06 17:34:55,429 - INFO - Epoch [13/100] train_loss: 0.2229, val_loss: 0.1913, lr: 0.001000, 2.84s
2023-01-06 17:34:55,439 - INFO - Saved model at 13
2023-01-06 17:34:55,439 - INFO - Val loss decrease from 0.2004 to 0.1913, saving to ./libcity/cache/5/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch13.tar
2023-01-06 17:34:58,127 - INFO - epoch complete!
2023-01-06 17:34:58,127 - INFO - evaluating now!
2023-01-06 17:34:58,290 - INFO - Epoch [14/100] train_loss: 0.2007, val_loss: 0.1865, lr: 0.001000, 2.85s
2023-01-06 17:34:58,299 - INFO - Saved model at 14
2023-01-06 17:34:58,300 - INFO - Val loss decrease from 0.1913 to 0.1865, saving to ./libcity/cache/5/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch14.tar
2023-01-06 17:35:00,930 - INFO - epoch complete!
2023-01-06 17:35:00,930 - INFO - evaluating now!
2023-01-06 17:35:01,097 - INFO - Epoch [15/100] train_loss: 0.1895, val_loss: 0.1886, lr: 0.001000, 2.80s
2023-01-06 17:35:03,815 - INFO - epoch complete!
2023-01-06 17:35:03,816 - INFO - evaluating now!
2023-01-06 17:35:03,975 - INFO - Epoch [16/100] train_loss: 0.1891, val_loss: 0.2067, lr: 0.001000, 2.88s
2023-01-06 17:35:06,682 - INFO - epoch complete!
2023-01-06 17:35:06,683 - INFO - evaluating now!
2023-01-06 17:35:06,854 - INFO - Epoch [17/100] train_loss: 0.2100, val_loss: 0.1988, lr: 0.001000, 2.88s
2023-01-06 17:35:09,466 - INFO - epoch complete!
2023-01-06 17:35:09,467 - INFO - evaluating now!
2023-01-06 17:35:09,630 - INFO - Epoch [18/100] train_loss: 0.1938, val_loss: 0.1969, lr: 0.001000, 2.78s
2023-01-06 17:35:12,293 - INFO - epoch complete!
2023-01-06 17:35:12,294 - INFO - evaluating now!
2023-01-06 17:35:12,455 - INFO - Epoch [19/100] train_loss: 0.2056, val_loss: 0.2087, lr: 0.001000, 2.82s
2023-01-06 17:35:15,080 - INFO - epoch complete!
2023-01-06 17:35:15,081 - INFO - evaluating now!
2023-01-06 17:35:15,241 - INFO - Epoch [20/100] train_loss: 0.2062, val_loss: 0.1964, lr: 0.001000, 2.79s
2023-01-06 17:35:18,104 - INFO - epoch complete!
2023-01-06 17:35:18,104 - INFO - evaluating now!
2023-01-06 17:35:18,267 - INFO - Epoch [21/100] train_loss: 0.1883, val_loss: 0.2118, lr: 0.001000, 3.03s
2023-01-06 17:35:21,068 - INFO - epoch complete!
2023-01-06 17:35:21,069 - INFO - evaluating now!
2023-01-06 17:35:21,242 - INFO - Epoch [22/100] train_loss: 0.1939, val_loss: 0.2044, lr: 0.001000, 2.97s
2023-01-06 17:35:24,159 - INFO - epoch complete!
2023-01-06 17:35:24,160 - INFO - evaluating now!
2023-01-06 17:35:24,332 - INFO - Epoch [23/100] train_loss: 0.1864, val_loss: 0.2033, lr: 0.001000, 3.09s
2023-01-06 17:35:27,126 - INFO - epoch complete!
2023-01-06 17:35:27,127 - INFO - evaluating now!
2023-01-06 17:35:27,301 - INFO - Epoch [24/100] train_loss: 0.1917, val_loss: 0.2243, lr: 0.001000, 2.97s
2023-01-06 17:35:29,954 - INFO - epoch complete!
2023-01-06 17:35:29,955 - INFO - evaluating now!
2023-01-06 17:35:30,127 - INFO - Epoch [25/100] train_loss: 0.1897, val_loss: 0.2575, lr: 0.001000, 2.83s
2023-01-06 17:35:32,825 - INFO - epoch complete!
2023-01-06 17:35:32,826 - INFO - evaluating now!
2023-01-06 17:35:32,988 - INFO - Epoch [26/100] train_loss: 0.1992, val_loss: 0.2995, lr: 0.001000, 2.86s
2023-01-06 17:35:35,618 - INFO - epoch complete!
2023-01-06 17:35:35,618 - INFO - evaluating now!
2023-01-06 17:35:35,780 - INFO - Epoch [27/100] train_loss: 0.1972, val_loss: 0.3138, lr: 0.001000, 2.79s
2023-01-06 17:35:38,423 - INFO - epoch complete!
2023-01-06 17:35:38,423 - INFO - evaluating now!
2023-01-06 17:35:38,584 - INFO - Epoch [28/100] train_loss: 0.1934, val_loss: 0.3036, lr: 0.001000, 2.80s
2023-01-06 17:35:41,260 - INFO - epoch complete!
2023-01-06 17:35:41,261 - INFO - evaluating now!
2023-01-06 17:35:41,422 - INFO - Epoch [29/100] train_loss: 0.1977, val_loss: 0.2926, lr: 0.001000, 2.84s
2023-01-06 17:35:44,079 - INFO - epoch complete!
2023-01-06 17:35:44,079 - INFO - evaluating now!
2023-01-06 17:35:44,242 - INFO - Epoch [30/100] train_loss: 0.1870, val_loss: 0.2628, lr: 0.001000, 2.82s
2023-01-06 17:35:46,883 - INFO - epoch complete!
2023-01-06 17:35:46,884 - INFO - evaluating now!
2023-01-06 17:35:47,046 - INFO - Epoch [31/100] train_loss: 0.1732, val_loss: 0.2533, lr: 0.001000, 2.80s
2023-01-06 17:35:49,735 - INFO - epoch complete!
2023-01-06 17:35:49,735 - INFO - evaluating now!
2023-01-06 17:35:49,900 - INFO - Epoch [32/100] train_loss: 0.1687, val_loss: 0.2687, lr: 0.001000, 2.85s
2023-01-06 17:35:52,790 - INFO - epoch complete!
2023-01-06 17:35:52,790 - INFO - evaluating now!
2023-01-06 17:35:52,954 - INFO - Epoch [33/100] train_loss: 0.1692, val_loss: 0.2553, lr: 0.001000, 3.05s
2023-01-06 17:35:55,669 - INFO - epoch complete!
2023-01-06 17:35:55,669 - INFO - evaluating now!
2023-01-06 17:35:55,830 - INFO - Epoch [34/100] train_loss: 0.1641, val_loss: 0.2734, lr: 0.001000, 2.88s
2023-01-06 17:35:58,531 - INFO - epoch complete!
2023-01-06 17:35:58,531 - INFO - evaluating now!
2023-01-06 17:35:58,696 - INFO - Epoch [35/100] train_loss: 0.1650, val_loss: 0.2398, lr: 0.001000, 2.86s
2023-01-06 17:36:01,324 - INFO - epoch complete!
2023-01-06 17:36:01,325 - INFO - evaluating now!
2023-01-06 17:36:01,487 - INFO - Epoch [36/100] train_loss: 0.1612, val_loss: 0.2621, lr: 0.001000, 2.79s
2023-01-06 17:36:04,187 - INFO - epoch complete!
2023-01-06 17:36:04,188 - INFO - evaluating now!
2023-01-06 17:36:04,350 - INFO - Epoch [37/100] train_loss: 0.1602, val_loss: 0.2541, lr: 0.001000, 2.86s
2023-01-06 17:36:07,023 - INFO - epoch complete!
2023-01-06 17:36:07,024 - INFO - evaluating now!
2023-01-06 17:36:07,193 - INFO - Epoch [38/100] train_loss: 0.1643, val_loss: 0.3150, lr: 0.001000, 2.84s
2023-01-06 17:36:09,895 - INFO - epoch complete!
2023-01-06 17:36:09,896 - INFO - evaluating now!
2023-01-06 17:36:10,064 - INFO - Epoch [39/100] train_loss: 0.1719, val_loss: 0.2406, lr: 0.001000, 2.87s
2023-01-06 17:36:12,739 - INFO - epoch complete!
2023-01-06 17:36:12,740 - INFO - evaluating now!
2023-01-06 17:36:12,902 - INFO - Epoch [40/100] train_loss: 0.1585, val_loss: 0.3009, lr: 0.001000, 2.84s
2023-01-06 17:36:15,581 - INFO - epoch complete!
2023-01-06 17:36:15,582 - INFO - evaluating now!
2023-01-06 17:36:15,742 - INFO - Epoch [41/100] train_loss: 0.1646, val_loss: 0.2869, lr: 0.001000, 2.84s
2023-01-06 17:36:18,437 - INFO - epoch complete!
2023-01-06 17:36:18,437 - INFO - evaluating now!
2023-01-06 17:36:18,598 - INFO - Epoch [42/100] train_loss: 0.1776, val_loss: 0.3204, lr: 0.001000, 2.86s
2023-01-06 17:36:21,270 - INFO - epoch complete!
2023-01-06 17:36:21,271 - INFO - evaluating now!
2023-01-06 17:36:21,434 - INFO - Epoch [43/100] train_loss: 0.2114, val_loss: 0.4164, lr: 0.001000, 2.84s
2023-01-06 17:36:24,135 - INFO - epoch complete!
2023-01-06 17:36:24,135 - INFO - evaluating now!
2023-01-06 17:36:24,304 - INFO - Epoch [44/100] train_loss: 0.2164, val_loss: 0.3199, lr: 0.001000, 2.87s
2023-01-06 17:36:27,087 - INFO - epoch complete!
2023-01-06 17:36:27,087 - INFO - evaluating now!
2023-01-06 17:36:27,250 - INFO - Epoch [45/100] train_loss: 0.1859, val_loss: 0.2569, lr: 0.001000, 2.94s
2023-01-06 17:36:29,958 - INFO - epoch complete!
2023-01-06 17:36:29,958 - INFO - evaluating now!
2023-01-06 17:36:30,121 - INFO - Epoch [46/100] train_loss: 0.1798, val_loss: 0.2521, lr: 0.001000, 2.87s
2023-01-06 17:36:32,830 - INFO - epoch complete!
2023-01-06 17:36:32,830 - INFO - evaluating now!
2023-01-06 17:36:32,993 - INFO - Epoch [47/100] train_loss: 0.1735, val_loss: 0.2532, lr: 0.001000, 2.87s
2023-01-06 17:36:35,698 - INFO - epoch complete!
2023-01-06 17:36:35,698 - INFO - evaluating now!
2023-01-06 17:36:35,859 - INFO - Epoch [48/100] train_loss: 0.1804, val_loss: 0.2279, lr: 0.001000, 2.87s
2023-01-06 17:36:38,524 - INFO - epoch complete!
2023-01-06 17:36:38,524 - INFO - evaluating now!
2023-01-06 17:36:38,685 - INFO - Epoch [49/100] train_loss: 0.1584, val_loss: 0.2348, lr: 0.001000, 2.83s
2023-01-06 17:36:41,363 - INFO - epoch complete!
2023-01-06 17:36:41,363 - INFO - evaluating now!
2023-01-06 17:36:41,526 - INFO - Epoch [50/100] train_loss: 0.1531, val_loss: 0.2602, lr: 0.001000, 2.84s
2023-01-06 17:36:44,191 - INFO - epoch complete!
2023-01-06 17:36:44,191 - INFO - evaluating now!
2023-01-06 17:36:44,352 - INFO - Epoch [51/100] train_loss: 0.1652, val_loss: 0.2393, lr: 0.001000, 2.83s
2023-01-06 17:36:46,975 - INFO - epoch complete!
2023-01-06 17:36:46,975 - INFO - evaluating now!
2023-01-06 17:36:47,145 - INFO - Epoch [52/100] train_loss: 0.1740, val_loss: 0.3364, lr: 0.001000, 2.79s
2023-01-06 17:36:49,783 - INFO - epoch complete!
2023-01-06 17:36:49,784 - INFO - evaluating now!
2023-01-06 17:36:49,947 - INFO - Epoch [53/100] train_loss: 0.1725, val_loss: 0.2657, lr: 0.001000, 2.80s
2023-01-06 17:36:52,596 - INFO - epoch complete!
2023-01-06 17:36:52,596 - INFO - evaluating now!
2023-01-06 17:36:52,757 - INFO - Epoch [54/100] train_loss: 0.1601, val_loss: 0.2558, lr: 0.001000, 2.81s
2023-01-06 17:36:55,454 - INFO - epoch complete!
2023-01-06 17:36:55,455 - INFO - evaluating now!
2023-01-06 17:36:55,617 - INFO - Epoch [55/100] train_loss: 0.1627, val_loss: 0.2047, lr: 0.001000, 2.86s
2023-01-06 17:36:58,307 - INFO - epoch complete!
2023-01-06 17:36:58,307 - INFO - evaluating now!
2023-01-06 17:36:58,469 - INFO - Epoch [56/100] train_loss: 0.1623, val_loss: 0.1928, lr: 0.001000, 2.85s
2023-01-06 17:37:01,144 - INFO - epoch complete!
2023-01-06 17:37:01,145 - INFO - evaluating now!
2023-01-06 17:37:01,305 - INFO - Epoch [57/100] train_loss: 0.1548, val_loss: 0.2078, lr: 0.001000, 2.84s
2023-01-06 17:37:04,119 - INFO - epoch complete!
2023-01-06 17:37:04,119 - INFO - evaluating now!
2023-01-06 17:37:04,282 - INFO - Epoch [58/100] train_loss: 0.1443, val_loss: 0.2187, lr: 0.001000, 2.98s
2023-01-06 17:37:06,945 - INFO - epoch complete!
2023-01-06 17:37:06,945 - INFO - evaluating now!
2023-01-06 17:37:07,117 - INFO - Epoch [59/100] train_loss: 0.1382, val_loss: 0.2161, lr: 0.001000, 2.83s
2023-01-06 17:37:09,790 - INFO - epoch complete!
2023-01-06 17:37:09,791 - INFO - evaluating now!
2023-01-06 17:37:09,957 - INFO - Epoch [60/100] train_loss: 0.1377, val_loss: 0.2385, lr: 0.001000, 2.84s
2023-01-06 17:37:12,619 - INFO - epoch complete!
2023-01-06 17:37:12,619 - INFO - evaluating now!
2023-01-06 17:37:12,787 - INFO - Epoch [61/100] train_loss: 0.1419, val_loss: 0.2063, lr: 0.001000, 2.83s
2023-01-06 17:37:15,398 - INFO - epoch complete!
2023-01-06 17:37:15,398 - INFO - evaluating now!
2023-01-06 17:37:15,565 - INFO - Epoch [62/100] train_loss: 0.1573, val_loss: 0.2630, lr: 0.001000, 2.78s
2023-01-06 17:37:18,211 - INFO - epoch complete!
2023-01-06 17:37:18,211 - INFO - evaluating now!
2023-01-06 17:37:18,376 - INFO - Epoch [63/100] train_loss: 0.1441, val_loss: 0.2341, lr: 0.001000, 2.81s
2023-01-06 17:37:21,014 - INFO - epoch complete!
2023-01-06 17:37:21,015 - INFO - evaluating now!
2023-01-06 17:37:21,193 - INFO - Epoch [64/100] train_loss: 0.1378, val_loss: 0.2045, lr: 0.001000, 2.82s
2023-01-06 17:37:23,838 - INFO - epoch complete!
2023-01-06 17:37:23,839 - INFO - evaluating now!
2023-01-06 17:37:24,009 - INFO - Epoch [65/100] train_loss: 0.1365, val_loss: 0.2303, lr: 0.001000, 2.81s
2023-01-06 17:37:26,750 - INFO - epoch complete!
2023-01-06 17:37:26,751 - INFO - evaluating now!
2023-01-06 17:37:26,919 - INFO - Epoch [66/100] train_loss: 0.1553, val_loss: 0.1947, lr: 0.001000, 2.91s
2023-01-06 17:37:29,575 - INFO - epoch complete!
2023-01-06 17:37:29,576 - INFO - evaluating now!
2023-01-06 17:37:29,744 - INFO - Epoch [67/100] train_loss: 0.1380, val_loss: 0.1969, lr: 0.001000, 2.82s
2023-01-06 17:37:32,475 - INFO - epoch complete!
2023-01-06 17:37:32,476 - INFO - evaluating now!
2023-01-06 17:37:32,643 - INFO - Epoch [68/100] train_loss: 0.1321, val_loss: 0.1997, lr: 0.001000, 2.90s
2023-01-06 17:37:35,318 - INFO - epoch complete!
2023-01-06 17:37:35,318 - INFO - evaluating now!
2023-01-06 17:37:35,487 - INFO - Epoch [69/100] train_loss: 0.1212, val_loss: 0.2175, lr: 0.001000, 2.84s
2023-01-06 17:37:38,312 - INFO - epoch complete!
2023-01-06 17:37:38,312 - INFO - evaluating now!
2023-01-06 17:37:38,477 - INFO - Epoch [70/100] train_loss: 0.1283, val_loss: 0.2000, lr: 0.001000, 2.99s
2023-01-06 17:37:41,135 - INFO - epoch complete!
2023-01-06 17:37:41,136 - INFO - evaluating now!
2023-01-06 17:37:41,307 - INFO - Epoch [71/100] train_loss: 0.1360, val_loss: 0.2086, lr: 0.001000, 2.83s
2023-01-06 17:37:44,035 - INFO - epoch complete!
2023-01-06 17:37:44,036 - INFO - evaluating now!
2023-01-06 17:37:44,207 - INFO - Epoch [72/100] train_loss: 0.1381, val_loss: 0.2540, lr: 0.001000, 2.90s
2023-01-06 17:37:46,939 - INFO - epoch complete!
2023-01-06 17:37:46,940 - INFO - evaluating now!
2023-01-06 17:37:47,114 - INFO - Epoch [73/100] train_loss: 0.1421, val_loss: 0.2353, lr: 0.001000, 2.91s
2023-01-06 17:37:49,802 - INFO - epoch complete!
2023-01-06 17:37:49,802 - INFO - evaluating now!
2023-01-06 17:37:49,974 - INFO - Epoch [74/100] train_loss: 0.1350, val_loss: 0.2070, lr: 0.001000, 2.86s
2023-01-06 17:37:52,873 - INFO - epoch complete!
2023-01-06 17:37:52,873 - INFO - evaluating now!
2023-01-06 17:37:53,044 - INFO - Epoch [75/100] train_loss: 0.1459, val_loss: 0.1967, lr: 0.001000, 3.07s
2023-01-06 17:37:55,933 - INFO - epoch complete!
2023-01-06 17:37:55,934 - INFO - evaluating now!
2023-01-06 17:37:56,101 - INFO - Epoch [76/100] train_loss: 0.1279, val_loss: 0.1905, lr: 0.001000, 3.06s
2023-01-06 17:37:58,821 - INFO - epoch complete!
2023-01-06 17:37:58,822 - INFO - evaluating now!
2023-01-06 17:37:58,981 - INFO - Epoch [77/100] train_loss: 0.1255, val_loss: 0.2061, lr: 0.001000, 2.88s
2023-01-06 17:38:01,643 - INFO - epoch complete!
2023-01-06 17:38:01,644 - INFO - evaluating now!
2023-01-06 17:38:01,805 - INFO - Epoch [78/100] train_loss: 0.1217, val_loss: 0.2149, lr: 0.001000, 2.82s
2023-01-06 17:38:04,543 - INFO - epoch complete!
2023-01-06 17:38:04,544 - INFO - evaluating now!
2023-01-06 17:38:04,705 - INFO - Epoch [79/100] train_loss: 0.1246, val_loss: 0.2065, lr: 0.001000, 2.90s
2023-01-06 17:38:07,393 - INFO - epoch complete!
2023-01-06 17:38:07,394 - INFO - evaluating now!
2023-01-06 17:38:07,560 - INFO - Epoch [80/100] train_loss: 0.1311, val_loss: 0.2071, lr: 0.001000, 2.85s
2023-01-06 17:38:10,281 - INFO - epoch complete!
2023-01-06 17:38:10,281 - INFO - evaluating now!
2023-01-06 17:38:10,454 - INFO - Epoch [81/100] train_loss: 0.1312, val_loss: 0.2077, lr: 0.001000, 2.89s
2023-01-06 17:38:13,301 - INFO - epoch complete!
2023-01-06 17:38:13,301 - INFO - evaluating now!
2023-01-06 17:38:13,463 - INFO - Epoch [82/100] train_loss: 0.1162, val_loss: 0.1994, lr: 0.001000, 3.01s
2023-01-06 17:38:16,110 - INFO - epoch complete!
2023-01-06 17:38:16,110 - INFO - evaluating now!
2023-01-06 17:38:16,271 - INFO - Epoch [83/100] train_loss: 0.1174, val_loss: 0.1996, lr: 0.001000, 2.81s
2023-01-06 17:38:18,896 - INFO - epoch complete!
2023-01-06 17:38:18,897 - INFO - evaluating now!
2023-01-06 17:38:19,058 - INFO - Epoch [84/100] train_loss: 0.1247, val_loss: 0.1947, lr: 0.001000, 2.79s
2023-01-06 17:38:21,799 - INFO - epoch complete!
2023-01-06 17:38:21,800 - INFO - evaluating now!
2023-01-06 17:38:21,962 - INFO - Epoch [85/100] train_loss: 0.1146, val_loss: 0.1906, lr: 0.001000, 2.90s
2023-01-06 17:38:24,685 - INFO - epoch complete!
2023-01-06 17:38:24,685 - INFO - evaluating now!
2023-01-06 17:38:24,846 - INFO - Epoch [86/100] train_loss: 0.1076, val_loss: 0.2043, lr: 0.001000, 2.88s
2023-01-06 17:38:27,522 - INFO - epoch complete!
2023-01-06 17:38:27,523 - INFO - evaluating now!
2023-01-06 17:38:27,684 - INFO - Epoch [87/100] train_loss: 0.1195, val_loss: 0.2223, lr: 0.001000, 2.84s
2023-01-06 17:38:30,374 - INFO - epoch complete!
2023-01-06 17:38:30,374 - INFO - evaluating now!
2023-01-06 17:38:30,537 - INFO - Epoch [88/100] train_loss: 0.1261, val_loss: 0.2129, lr: 0.001000, 2.85s
2023-01-06 17:38:33,233 - INFO - epoch complete!
2023-01-06 17:38:33,234 - INFO - evaluating now!
2023-01-06 17:38:33,403 - INFO - Epoch [89/100] train_loss: 0.1052, val_loss: 0.1916, lr: 0.001000, 2.86s
2023-01-06 17:38:36,090 - INFO - epoch complete!
2023-01-06 17:38:36,091 - INFO - evaluating now!
2023-01-06 17:38:36,253 - INFO - Epoch [90/100] train_loss: 0.1169, val_loss: 0.2238, lr: 0.001000, 2.85s
2023-01-06 17:38:38,914 - INFO - epoch complete!
2023-01-06 17:38:38,914 - INFO - evaluating now!
2023-01-06 17:38:39,077 - INFO - Epoch [91/100] train_loss: 0.1262, val_loss: 0.1995, lr: 0.001000, 2.82s
2023-01-06 17:38:41,750 - INFO - epoch complete!
2023-01-06 17:38:41,750 - INFO - evaluating now!
2023-01-06 17:38:41,913 - INFO - Epoch [92/100] train_loss: 0.1096, val_loss: 0.2005, lr: 0.001000, 2.84s
2023-01-06 17:38:44,591 - INFO - epoch complete!
2023-01-06 17:38:44,592 - INFO - evaluating now!
2023-01-06 17:38:44,754 - INFO - Epoch [93/100] train_loss: 0.0973, val_loss: 0.2029, lr: 0.001000, 2.84s
2023-01-06 17:38:47,406 - INFO - epoch complete!
2023-01-06 17:38:47,406 - INFO - evaluating now!
2023-01-06 17:38:47,569 - INFO - Epoch [94/100] train_loss: 0.1008, val_loss: 0.2076, lr: 0.001000, 2.81s
2023-01-06 17:38:50,362 - INFO - epoch complete!
2023-01-06 17:38:50,362 - INFO - evaluating now!
2023-01-06 17:38:50,523 - INFO - Epoch [95/100] train_loss: 0.1022, val_loss: 0.2024, lr: 0.001000, 2.95s
2023-01-06 17:38:53,174 - INFO - epoch complete!
2023-01-06 17:38:53,175 - INFO - evaluating now!
2023-01-06 17:38:53,338 - INFO - Epoch [96/100] train_loss: 0.1004, val_loss: 0.2014, lr: 0.001000, 2.81s
2023-01-06 17:38:56,072 - INFO - epoch complete!
2023-01-06 17:38:56,073 - INFO - evaluating now!
2023-01-06 17:38:56,236 - INFO - Epoch [97/100] train_loss: 0.1102, val_loss: 0.2007, lr: 0.001000, 2.90s
2023-01-06 17:38:58,928 - INFO - epoch complete!
2023-01-06 17:38:58,929 - INFO - evaluating now!
2023-01-06 17:38:59,091 - INFO - Epoch [98/100] train_loss: 0.1148, val_loss: 0.2162, lr: 0.001000, 2.85s
2023-01-06 17:39:01,766 - INFO - epoch complete!
2023-01-06 17:39:01,767 - INFO - evaluating now!
2023-01-06 17:39:01,927 - INFO - Epoch [99/100] train_loss: 0.0982, val_loss: 0.2013, lr: 0.001000, 2.84s
2023-01-06 17:39:01,927 - INFO - Trained totally 100 epochs, average train time is 2.690s, average eval time is 0.164s
2023-01-06 17:39:01,941 - INFO - Loaded model at 14
2023-01-06 17:39:01,941 - INFO - Saved model at ./libcity/cache/5/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30.m
2023-01-06 17:39:01,949 - INFO - Start evaluating ...
2023-01-06 17:39:02,556 - INFO - Evaluate result is saved at ./libcity/cache/5/evaluate_cache/2023_01_06_17_39_02_DeepTTE_Beijing_Taxi_Sample_new_longer30.csv
2023-01-06 17:39:02,567 - INFO - 
          MAE      MAPE           MSE        RMSE  masked_MAE  masked_MAPE    masked_MSE  masked_RMSE        R2      EVAR
1  454.665466  0.201174  394350.96875  627.973694  454.665466     0.201174  394350.96875   627.973694  0.662864  0.718247

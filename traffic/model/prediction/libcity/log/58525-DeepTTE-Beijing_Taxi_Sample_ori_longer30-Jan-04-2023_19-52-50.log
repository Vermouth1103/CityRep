2023-01-04 19:52:50,949 - INFO - Log directory: ./libcity/log
2023-01-04 19:52:50,950 - INFO - Begin pipeline, task=eta, model_name=DeepTTE, dataset_name=Beijing_Taxi_Sample_ori_longer30, exp_id=58525
2023-01-04 19:52:50,950 - INFO - {'task': 'eta', 'model': 'DeepTTE', 'dataset': 'Beijing_Taxi_Sample_ori_longer30', 'saved_model': True, 'train': True, 'seed': 0, 'batch_size': 64, 'dataset_class': 'ETADataset', 'eta_encoder': 'DeeptteEncoder', 'executor': 'ETAExecutor', 'evaluator': 'ETAEvaluator', 'uid_emb_size': 16, 'weekid_emb_size': 3, 'timdid_emb_size': 8, 'kernel_size': 3, 'num_filter': 32, 'pooling_method': 'attention', 'num_final_fcs': 4, 'final_fc_size': 128, 'alpha': 0.1, 'rnn_type': 'LSTM', 'rnn_num_layers': 1, 'hidden_size': 128, 'max_epoch': 100, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': False, 'use_early_stop': False, 'patience': 20, 'num_workers': 0, 'min_session_len': 5, 'max_session_len': 50, 'min_sessions': 0, 'window_size': 1, 'cut_method': 'time_interval', 'pad_with_last_sample': True, 'sort_by_traj_len': True, 'cache_dataset': True, 'train_rate': 0.7, 'eval_rate': 0.1, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'max_grad_norm': 1.0, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE', 'R2', 'EVAR'], 'mode': 'single', 'save_modes': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'coordinates': 'coordinate', 'embedding': 'other'}}, 'usr': {'properties': {}}, 'dyna': {'including_types': ['trajectory'], 'trajectory': {'entity_id': 'usr_id', 'traj_id': 'num', 'coordinates': 'coordinate', 'current_dis': 'num', 'speeds': 'other', 'speeds_relevant1': 'other', 'speeds_relevant2': 'other', 'speeds_long': 'other', 'grid_len': 'num', 'holiday': 'num'}}, 'geo_file': 'Beijing_Taxi_Sample_ori_longer30', 'usr_file': 'Beijing_Taxi_Sample_ori_longer30', 'dyna_file': 'Beijing_Taxi_Sample_ori_longer30', 'device': device(type='cuda', index=0), 'exp_id': 58525}
2023-01-04 19:52:50,957 - INFO - Dataset created
2023-01-04 19:52:53,295 - INFO - Loaded file Beijing_Taxi_Sample_ori_longer30.dyna, shape=(290813, 14)
2023-01-04 19:54:04,585 - INFO - Saved at ./libcity/cache/dataset_cache/eta_Beijing_Taxi_Sample_ori_longer30_DeeptteEncoder.json
2023-01-04 19:54:04,932 - INFO - longi_mean: 116.38772777340102
2023-01-04 19:54:04,933 - INFO - longi_std: 0.07261763841827414
2023-01-04 19:54:04,933 - INFO - lati_mean: 39.92589546753647
2023-01-04 19:54:04,933 - INFO - lati_std: 0.049410813934588
2023-01-04 19:54:04,933 - INFO - dist_mean: 17.511338453736464
2023-01-04 19:54:04,933 - INFO - dist_std: 6.814852670055433
2023-01-04 19:54:04,933 - INFO - time_mean: 2249.877973358706
2023-01-04 19:54:04,933 - INFO - time_std: 1145.3334062262293
2023-01-04 19:54:04,933 - INFO - dist_gap_mean: 0.3677022469382553
2023-01-04 19:54:04,933 - INFO - dist_gap_std: 0.19961494844682895
2023-01-04 19:54:04,933 - INFO - time_gap_mean: 47.24283002847011
2023-01-04 19:54:04,933 - INFO - time_gap_std: 44.31597855637794
2023-01-04 19:54:04,986 - INFO - Number of train data: 4204
2023-01-04 19:54:04,986 - INFO - Number of eval  data: 587
2023-01-04 19:54:04,986 - INFO - Number of test  data: 1171
2023-01-04 19:54:09,459 - INFO - DeepTTE(
  (attr_net): Attr(
    (uid_em): Embedding(69, 16)
    (weekid_em): Embedding(7, 3)
    (timeid_em): Embedding(1440, 8)
  )
  (spatio_temporal): SpatioTemporal(
    (geo_conv): GeoConv(
      (state_em): Embedding(2, 2)
      (process_coords): Linear(in_features=4, out_features=16, bias=True)
      (conv): Conv1d(16, 32, kernel_size=(3,), stride=(1,))
    )
    (rnn): LSTM(61, 128, batch_first=True)
    (attr2atten): Linear(in_features=28, out_features=128, bias=True)
  )
  (entire_estimate): EntireEstimator(
    (input2hid): Linear(in_features=156, out_features=128, bias=True)
    (residuals): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Linear(in_features=128, out_features=128, bias=True)
    )
    (hid2out): Linear(in_features=128, out_features=1, bias=True)
  )
  (local_estimate): LocalEstimator(
    (input2hid): Linear(in_features=128, out_features=64, bias=True)
    (hid2hid): Linear(in_features=64, out_features=32, bias=True)
    (hid2out): Linear(in_features=32, out_features=1, bias=True)
  )
)
2023-01-04 19:54:09,460 - INFO - attr_net.uid_em.weight	torch.Size([69, 16])	cuda:0	True
2023-01-04 19:54:09,460 - INFO - attr_net.weekid_em.weight	torch.Size([7, 3])	cuda:0	True
2023-01-04 19:54:09,461 - INFO - attr_net.timeid_em.weight	torch.Size([1440, 8])	cuda:0	True
2023-01-04 19:54:09,461 - INFO - spatio_temporal.geo_conv.state_em.weight	torch.Size([2, 2])	cuda:0	True
2023-01-04 19:54:09,461 - INFO - spatio_temporal.geo_conv.process_coords.weight	torch.Size([16, 4])	cuda:0	True
2023-01-04 19:54:09,461 - INFO - spatio_temporal.geo_conv.process_coords.bias	torch.Size([16])	cuda:0	True
2023-01-04 19:54:09,462 - INFO - spatio_temporal.geo_conv.conv.weight	torch.Size([32, 16, 3])	cuda:0	True
2023-01-04 19:54:09,462 - INFO - spatio_temporal.geo_conv.conv.bias	torch.Size([32])	cuda:0	True
2023-01-04 19:54:09,462 - INFO - spatio_temporal.rnn.weight_ih_l0	torch.Size([512, 61])	cuda:0	True
2023-01-04 19:54:09,462 - INFO - spatio_temporal.rnn.weight_hh_l0	torch.Size([512, 128])	cuda:0	True
2023-01-04 19:54:09,462 - INFO - spatio_temporal.rnn.bias_ih_l0	torch.Size([512])	cuda:0	True
2023-01-04 19:54:09,463 - INFO - spatio_temporal.rnn.bias_hh_l0	torch.Size([512])	cuda:0	True
2023-01-04 19:54:09,463 - INFO - spatio_temporal.attr2atten.weight	torch.Size([128, 28])	cuda:0	True
2023-01-04 19:54:09,463 - INFO - spatio_temporal.attr2atten.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:54:09,463 - INFO - entire_estimate.input2hid.weight	torch.Size([128, 156])	cuda:0	True
2023-01-04 19:54:09,463 - INFO - entire_estimate.input2hid.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:54:09,464 - INFO - entire_estimate.residuals.0.weight	torch.Size([128, 128])	cuda:0	True
2023-01-04 19:54:09,465 - INFO - entire_estimate.residuals.0.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:54:09,465 - INFO - entire_estimate.residuals.1.weight	torch.Size([128, 128])	cuda:0	True
2023-01-04 19:54:09,466 - INFO - entire_estimate.residuals.1.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:54:09,466 - INFO - entire_estimate.residuals.2.weight	torch.Size([128, 128])	cuda:0	True
2023-01-04 19:54:09,466 - INFO - entire_estimate.residuals.2.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:54:09,466 - INFO - entire_estimate.residuals.3.weight	torch.Size([128, 128])	cuda:0	True
2023-01-04 19:54:09,466 - INFO - entire_estimate.residuals.3.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:54:09,466 - INFO - entire_estimate.hid2out.weight	torch.Size([1, 128])	cuda:0	True
2023-01-04 19:54:09,467 - INFO - entire_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-04 19:54:09,467 - INFO - local_estimate.input2hid.weight	torch.Size([64, 128])	cuda:0	True
2023-01-04 19:54:09,467 - INFO - local_estimate.input2hid.bias	torch.Size([64])	cuda:0	True
2023-01-04 19:54:09,467 - INFO - local_estimate.hid2hid.weight	torch.Size([32, 64])	cuda:0	True
2023-01-04 19:54:09,467 - INFO - local_estimate.hid2hid.bias	torch.Size([32])	cuda:0	True
2023-01-04 19:54:09,468 - INFO - local_estimate.hid2out.weight	torch.Size([1, 32])	cuda:0	True
2023-01-04 19:54:09,468 - INFO - local_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-04 19:54:09,469 - INFO - Total parameter numbers: 212443
2023-01-04 19:54:09,469 - INFO - You select `adam` optimizer.
2023-01-04 19:54:09,470 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-01-04 19:54:09,470 - INFO - Start training ...
2023-01-04 19:54:09,470 - INFO - num_batches:66
2023-01-04 19:54:13,628 - INFO - epoch complete!
2023-01-04 19:54:13,628 - INFO - evaluating now!
2023-01-04 19:54:13,973 - INFO - Epoch [0/100] train_loss: 0.4330, val_loss: 0.2873, lr: 0.001000, 4.50s
2023-01-04 19:54:13,990 - INFO - Saved model at 0
2023-01-04 19:54:13,991 - INFO - Val loss decrease from inf to 0.2873, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch0.tar
2023-01-04 19:54:18,244 - INFO - epoch complete!
2023-01-04 19:54:18,245 - INFO - evaluating now!
2023-01-04 19:54:18,791 - INFO - Epoch [1/100] train_loss: 0.3614, val_loss: 0.3216, lr: 0.001000, 4.80s
2023-01-04 19:54:23,222 - INFO - epoch complete!
2023-01-04 19:54:23,223 - INFO - evaluating now!
2023-01-04 19:54:23,535 - INFO - Epoch [2/100] train_loss: 0.2965, val_loss: 0.2544, lr: 0.001000, 4.74s
2023-01-04 19:54:23,549 - INFO - Saved model at 2
2023-01-04 19:54:23,549 - INFO - Val loss decrease from 0.2873 to 0.2544, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch2.tar
2023-01-04 19:54:27,780 - INFO - epoch complete!
2023-01-04 19:54:27,782 - INFO - evaluating now!
2023-01-04 19:54:28,111 - INFO - Epoch [3/100] train_loss: 0.2750, val_loss: 0.2683, lr: 0.001000, 4.56s
2023-01-04 19:54:32,071 - INFO - epoch complete!
2023-01-04 19:54:32,071 - INFO - evaluating now!
2023-01-04 19:54:32,467 - INFO - Epoch [4/100] train_loss: 0.2677, val_loss: 0.2669, lr: 0.001000, 4.35s
2023-01-04 19:54:36,603 - INFO - epoch complete!
2023-01-04 19:54:36,604 - INFO - evaluating now!
2023-01-04 19:54:36,895 - INFO - Epoch [5/100] train_loss: 0.2603, val_loss: 0.2470, lr: 0.001000, 4.43s
2023-01-04 19:54:36,909 - INFO - Saved model at 5
2023-01-04 19:54:36,909 - INFO - Val loss decrease from 0.2544 to 0.2470, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch5.tar
2023-01-04 19:54:40,977 - INFO - epoch complete!
2023-01-04 19:54:40,980 - INFO - evaluating now!
2023-01-04 19:54:41,295 - INFO - Epoch [6/100] train_loss: 0.2462, val_loss: 0.2149, lr: 0.001000, 4.39s
2023-01-04 19:54:41,309 - INFO - Saved model at 6
2023-01-04 19:54:41,309 - INFO - Val loss decrease from 0.2470 to 0.2149, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch6.tar
2023-01-04 19:54:45,495 - INFO - epoch complete!
2023-01-04 19:54:45,496 - INFO - evaluating now!
2023-01-04 19:54:45,802 - INFO - Epoch [7/100] train_loss: 0.2398, val_loss: 0.2312, lr: 0.001000, 4.49s
2023-01-04 19:54:50,390 - INFO - epoch complete!
2023-01-04 19:54:50,392 - INFO - evaluating now!
2023-01-04 19:54:50,718 - INFO - Epoch [8/100] train_loss: 0.2545, val_loss: 0.2190, lr: 0.001000, 4.92s
2023-01-04 19:54:54,931 - INFO - epoch complete!
2023-01-04 19:54:54,935 - INFO - evaluating now!
2023-01-04 19:54:55,364 - INFO - Epoch [9/100] train_loss: 0.2297, val_loss: 0.2159, lr: 0.001000, 4.65s
2023-01-04 19:54:59,448 - INFO - epoch complete!
2023-01-04 19:54:59,449 - INFO - evaluating now!
2023-01-04 19:54:59,785 - INFO - Epoch [10/100] train_loss: 0.2409, val_loss: 0.2477, lr: 0.001000, 4.42s
2023-01-04 19:55:04,191 - INFO - epoch complete!
2023-01-04 19:55:04,191 - INFO - evaluating now!
2023-01-04 19:55:04,513 - INFO - Epoch [11/100] train_loss: 0.2245, val_loss: 0.2185, lr: 0.001000, 4.73s
2023-01-04 19:55:08,818 - INFO - epoch complete!
2023-01-04 19:55:08,819 - INFO - evaluating now!
2023-01-04 19:55:09,152 - INFO - Epoch [12/100] train_loss: 0.2263, val_loss: 0.2565, lr: 0.001000, 4.64s
2023-01-04 19:55:13,626 - INFO - epoch complete!
2023-01-04 19:55:13,627 - INFO - evaluating now!
2023-01-04 19:55:13,942 - INFO - Epoch [13/100] train_loss: 0.2253, val_loss: 0.2142, lr: 0.001000, 4.79s
2023-01-04 19:55:13,960 - INFO - Saved model at 13
2023-01-04 19:55:13,961 - INFO - Val loss decrease from 0.2149 to 0.2142, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch13.tar
2023-01-04 19:55:18,126 - INFO - epoch complete!
2023-01-04 19:55:18,129 - INFO - evaluating now!
2023-01-04 19:55:18,568 - INFO - Epoch [14/100] train_loss: 0.2222, val_loss: 0.2108, lr: 0.001000, 4.61s
2023-01-04 19:55:18,594 - INFO - Saved model at 14
2023-01-04 19:55:18,594 - INFO - Val loss decrease from 0.2142 to 0.2108, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch14.tar
2023-01-04 19:55:22,747 - INFO - epoch complete!
2023-01-04 19:55:22,749 - INFO - evaluating now!
2023-01-04 19:55:23,119 - INFO - Epoch [15/100] train_loss: 0.2061, val_loss: 0.1954, lr: 0.001000, 4.52s
2023-01-04 19:55:23,138 - INFO - Saved model at 15
2023-01-04 19:55:23,138 - INFO - Val loss decrease from 0.2108 to 0.1954, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch15.tar
2023-01-04 19:55:27,016 - INFO - epoch complete!
2023-01-04 19:55:27,017 - INFO - evaluating now!
2023-01-04 19:55:27,374 - INFO - Epoch [16/100] train_loss: 0.1981, val_loss: 0.1950, lr: 0.001000, 4.24s
2023-01-04 19:55:27,388 - INFO - Saved model at 16
2023-01-04 19:55:27,388 - INFO - Val loss decrease from 0.1954 to 0.1950, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch16.tar
2023-01-04 19:55:31,226 - INFO - epoch complete!
2023-01-04 19:55:31,226 - INFO - evaluating now!
2023-01-04 19:55:31,523 - INFO - Epoch [17/100] train_loss: 0.1891, val_loss: 0.1981, lr: 0.001000, 4.13s
2023-01-04 19:55:35,790 - INFO - epoch complete!
2023-01-04 19:55:35,790 - INFO - evaluating now!
2023-01-04 19:55:36,159 - INFO - Epoch [18/100] train_loss: 0.1906, val_loss: 0.1954, lr: 0.001000, 4.63s
2023-01-04 19:55:40,734 - INFO - epoch complete!
2023-01-04 19:55:40,735 - INFO - evaluating now!
2023-01-04 19:55:41,076 - INFO - Epoch [19/100] train_loss: 0.1842, val_loss: 0.1897, lr: 0.001000, 4.92s
2023-01-04 19:55:41,096 - INFO - Saved model at 19
2023-01-04 19:55:41,096 - INFO - Val loss decrease from 0.1950 to 0.1897, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch19.tar
2023-01-04 19:55:45,169 - INFO - epoch complete!
2023-01-04 19:55:45,170 - INFO - evaluating now!
2023-01-04 19:55:45,495 - INFO - Epoch [20/100] train_loss: 0.1835, val_loss: 0.1915, lr: 0.001000, 4.40s
2023-01-04 19:55:50,210 - INFO - epoch complete!
2023-01-04 19:55:50,214 - INFO - evaluating now!
2023-01-04 19:55:50,572 - INFO - Epoch [21/100] train_loss: 0.1854, val_loss: 0.1993, lr: 0.001000, 5.08s
2023-01-04 19:55:54,840 - INFO - epoch complete!
2023-01-04 19:55:54,841 - INFO - evaluating now!
2023-01-04 19:55:55,153 - INFO - Epoch [22/100] train_loss: 0.1989, val_loss: 0.2246, lr: 0.001000, 4.58s
2023-01-04 19:55:59,535 - INFO - epoch complete!
2023-01-04 19:55:59,535 - INFO - evaluating now!
2023-01-04 19:55:59,854 - INFO - Epoch [23/100] train_loss: 0.2181, val_loss: 0.2476, lr: 0.001000, 4.70s
2023-01-04 19:56:04,280 - INFO - epoch complete!
2023-01-04 19:56:04,280 - INFO - evaluating now!
2023-01-04 19:56:04,598 - INFO - Epoch [24/100] train_loss: 0.1951, val_loss: 0.2289, lr: 0.001000, 4.74s
2023-01-04 19:56:08,579 - INFO - epoch complete!
2023-01-04 19:56:08,585 - INFO - evaluating now!
2023-01-04 19:56:08,916 - INFO - Epoch [25/100] train_loss: 0.1955, val_loss: 0.2504, lr: 0.001000, 4.32s
2023-01-04 19:56:12,777 - INFO - epoch complete!
2023-01-04 19:56:12,782 - INFO - evaluating now!
2023-01-04 19:56:13,152 - INFO - Epoch [26/100] train_loss: 0.1820, val_loss: 0.2515, lr: 0.001000, 4.23s
2023-01-04 19:56:17,118 - INFO - epoch complete!
2023-01-04 19:56:17,120 - INFO - evaluating now!
2023-01-04 19:56:17,478 - INFO - Epoch [27/100] train_loss: 0.1840, val_loss: 0.2453, lr: 0.001000, 4.33s
2023-01-04 19:56:21,831 - INFO - epoch complete!
2023-01-04 19:56:21,833 - INFO - evaluating now!
2023-01-04 19:56:22,160 - INFO - Epoch [28/100] train_loss: 0.1718, val_loss: 0.2485, lr: 0.001000, 4.68s
2023-01-04 19:56:26,520 - INFO - epoch complete!
2023-01-04 19:56:26,521 - INFO - evaluating now!
2023-01-04 19:56:27,019 - INFO - Epoch [29/100] train_loss: 0.1704, val_loss: 0.2625, lr: 0.001000, 4.86s
2023-01-04 19:56:31,384 - INFO - epoch complete!
2023-01-04 19:56:31,385 - INFO - evaluating now!
2023-01-04 19:56:31,698 - INFO - Epoch [30/100] train_loss: 0.1773, val_loss: 0.2851, lr: 0.001000, 4.68s
2023-01-04 19:56:35,763 - INFO - epoch complete!
2023-01-04 19:56:35,764 - INFO - evaluating now!
2023-01-04 19:56:36,074 - INFO - Epoch [31/100] train_loss: 0.1903, val_loss: 0.2555, lr: 0.001000, 4.38s
2023-01-04 19:56:40,073 - INFO - epoch complete!
2023-01-04 19:56:40,074 - INFO - evaluating now!
2023-01-04 19:56:40,439 - INFO - Epoch [32/100] train_loss: 0.1756, val_loss: 0.2427, lr: 0.001000, 4.37s
2023-01-04 19:56:44,839 - INFO - epoch complete!
2023-01-04 19:56:44,841 - INFO - evaluating now!
2023-01-04 19:56:45,201 - INFO - Epoch [33/100] train_loss: 0.1698, val_loss: 0.2663, lr: 0.001000, 4.76s
2023-01-04 19:56:49,092 - INFO - epoch complete!
2023-01-04 19:56:49,093 - INFO - evaluating now!
2023-01-04 19:56:49,389 - INFO - Epoch [34/100] train_loss: 0.1675, val_loss: 0.2549, lr: 0.001000, 4.19s
2023-01-04 19:56:53,417 - INFO - epoch complete!
2023-01-04 19:56:53,418 - INFO - evaluating now!
2023-01-04 19:56:53,712 - INFO - Epoch [35/100] train_loss: 0.1647, val_loss: 0.2604, lr: 0.001000, 4.32s
2023-01-04 19:56:57,590 - INFO - epoch complete!
2023-01-04 19:56:57,591 - INFO - evaluating now!
2023-01-04 19:56:57,908 - INFO - Epoch [36/100] train_loss: 0.1674, val_loss: 0.2684, lr: 0.001000, 4.19s
2023-01-04 19:57:02,040 - INFO - epoch complete!
2023-01-04 19:57:02,042 - INFO - evaluating now!
2023-01-04 19:57:02,391 - INFO - Epoch [37/100] train_loss: 0.1909, val_loss: 0.3706, lr: 0.001000, 4.48s
2023-01-04 19:57:06,630 - INFO - epoch complete!
2023-01-04 19:57:06,631 - INFO - evaluating now!
2023-01-04 19:57:06,988 - INFO - Epoch [38/100] train_loss: 0.2089, val_loss: 0.3137, lr: 0.001000, 4.60s
2023-01-04 19:57:11,063 - INFO - epoch complete!
2023-01-04 19:57:11,065 - INFO - evaluating now!
2023-01-04 19:57:11,438 - INFO - Epoch [39/100] train_loss: 0.1893, val_loss: 0.2611, lr: 0.001000, 4.45s
2023-01-04 19:57:15,464 - INFO - epoch complete!
2023-01-04 19:57:15,467 - INFO - evaluating now!
2023-01-04 19:57:15,774 - INFO - Epoch [40/100] train_loss: 0.1770, val_loss: 0.2745, lr: 0.001000, 4.33s
2023-01-04 19:57:19,903 - INFO - epoch complete!
2023-01-04 19:57:19,905 - INFO - evaluating now!
2023-01-04 19:57:20,352 - INFO - Epoch [41/100] train_loss: 0.1713, val_loss: 0.2705, lr: 0.001000, 4.58s
2023-01-04 19:57:24,343 - INFO - epoch complete!
2023-01-04 19:57:24,350 - INFO - evaluating now!
2023-01-04 19:57:24,756 - INFO - Epoch [42/100] train_loss: 0.1726, val_loss: 0.2594, lr: 0.001000, 4.40s
2023-01-04 19:57:28,562 - INFO - epoch complete!
2023-01-04 19:57:28,564 - INFO - evaluating now!
2023-01-04 19:57:28,967 - INFO - Epoch [43/100] train_loss: 0.1648, val_loss: 0.2744, lr: 0.001000, 4.21s
2023-01-04 19:57:32,714 - INFO - epoch complete!
2023-01-04 19:57:32,715 - INFO - evaluating now!
2023-01-04 19:57:33,020 - INFO - Epoch [44/100] train_loss: 0.1572, val_loss: 0.2760, lr: 0.001000, 4.05s
2023-01-04 19:57:37,028 - INFO - epoch complete!
2023-01-04 19:57:37,028 - INFO - evaluating now!
2023-01-04 19:57:37,352 - INFO - Epoch [45/100] train_loss: 0.1598, val_loss: 0.2742, lr: 0.001000, 4.33s
2023-01-04 19:57:41,210 - INFO - epoch complete!
2023-01-04 19:57:41,211 - INFO - evaluating now!
2023-01-04 19:57:41,808 - INFO - Epoch [46/100] train_loss: 0.1609, val_loss: 0.2977, lr: 0.001000, 4.46s
2023-01-04 19:57:46,112 - INFO - epoch complete!
2023-01-04 19:57:46,114 - INFO - evaluating now!
2023-01-04 19:57:46,458 - INFO - Epoch [47/100] train_loss: 0.1654, val_loss: 0.3090, lr: 0.001000, 4.65s
2023-01-04 19:57:50,700 - INFO - epoch complete!
2023-01-04 19:57:50,703 - INFO - evaluating now!
2023-01-04 19:57:51,126 - INFO - Epoch [48/100] train_loss: 0.1694, val_loss: 0.3067, lr: 0.001000, 4.67s
2023-01-04 19:57:55,299 - INFO - epoch complete!
2023-01-04 19:57:55,300 - INFO - evaluating now!
2023-01-04 19:57:55,687 - INFO - Epoch [49/100] train_loss: 0.1786, val_loss: 0.2261, lr: 0.001000, 4.56s
2023-01-04 19:57:59,636 - INFO - epoch complete!
2023-01-04 19:57:59,638 - INFO - evaluating now!
2023-01-04 19:57:59,944 - INFO - Epoch [50/100] train_loss: 0.1732, val_loss: 0.2212, lr: 0.001000, 4.26s
2023-01-04 19:58:03,992 - INFO - epoch complete!
2023-01-04 19:58:03,992 - INFO - evaluating now!
2023-01-04 19:58:04,416 - INFO - Epoch [51/100] train_loss: 0.1651, val_loss: 0.2208, lr: 0.001000, 4.47s
2023-01-04 19:58:08,401 - INFO - epoch complete!
2023-01-04 19:58:08,404 - INFO - evaluating now!
2023-01-04 19:58:08,735 - INFO - Epoch [52/100] train_loss: 0.1551, val_loss: 0.2132, lr: 0.001000, 4.32s
2023-01-04 19:58:12,780 - INFO - epoch complete!
2023-01-04 19:58:12,781 - INFO - evaluating now!
2023-01-04 19:58:13,094 - INFO - Epoch [53/100] train_loss: 0.1551, val_loss: 0.1928, lr: 0.001000, 4.36s
2023-01-04 19:58:16,876 - INFO - epoch complete!
2023-01-04 19:58:16,879 - INFO - evaluating now!
2023-01-04 19:58:17,239 - INFO - Epoch [54/100] train_loss: 0.1623, val_loss: 0.1855, lr: 0.001000, 4.14s
2023-01-04 19:58:17,251 - INFO - Saved model at 54
2023-01-04 19:58:17,252 - INFO - Val loss decrease from 0.1897 to 0.1855, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch54.tar
2023-01-04 19:58:21,502 - INFO - epoch complete!
2023-01-04 19:58:21,503 - INFO - evaluating now!
2023-01-04 19:58:21,871 - INFO - Epoch [55/100] train_loss: 0.1479, val_loss: 0.1906, lr: 0.001000, 4.62s
2023-01-04 19:58:25,944 - INFO - epoch complete!
2023-01-04 19:58:25,946 - INFO - evaluating now!
2023-01-04 19:58:26,264 - INFO - Epoch [56/100] train_loss: 0.1439, val_loss: 0.1912, lr: 0.001000, 4.39s
2023-01-04 19:58:30,805 - INFO - epoch complete!
2023-01-04 19:58:30,805 - INFO - evaluating now!
2023-01-04 19:58:31,257 - INFO - Epoch [57/100] train_loss: 0.1476, val_loss: 0.1881, lr: 0.001000, 4.99s
2023-01-04 19:58:35,327 - INFO - epoch complete!
2023-01-04 19:58:35,329 - INFO - evaluating now!
2023-01-04 19:58:35,763 - INFO - Epoch [58/100] train_loss: 0.1419, val_loss: 0.2068, lr: 0.001000, 4.51s
2023-01-04 19:58:40,050 - INFO - epoch complete!
2023-01-04 19:58:40,055 - INFO - evaluating now!
2023-01-04 19:58:40,384 - INFO - Epoch [59/100] train_loss: 0.1454, val_loss: 0.2093, lr: 0.001000, 4.62s
2023-01-04 19:58:44,460 - INFO - epoch complete!
2023-01-04 19:58:44,461 - INFO - evaluating now!
2023-01-04 19:58:44,775 - INFO - Epoch [60/100] train_loss: 0.1354, val_loss: 0.2288, lr: 0.001000, 4.39s
2023-01-04 19:58:49,104 - INFO - epoch complete!
2023-01-04 19:58:49,106 - INFO - evaluating now!
2023-01-04 19:58:49,481 - INFO - Epoch [61/100] train_loss: 0.1402, val_loss: 0.2063, lr: 0.001000, 4.70s
2023-01-04 19:58:53,310 - INFO - epoch complete!
2023-01-04 19:58:53,311 - INFO - evaluating now!
2023-01-04 19:58:53,626 - INFO - Epoch [62/100] train_loss: 0.1517, val_loss: 0.1838, lr: 0.001000, 4.14s
2023-01-04 19:58:53,642 - INFO - Saved model at 62
2023-01-04 19:58:53,642 - INFO - Val loss decrease from 0.1855 to 0.1838, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch62.tar
2023-01-04 19:58:57,494 - INFO - epoch complete!
2023-01-04 19:58:57,494 - INFO - evaluating now!
2023-01-04 19:58:57,811 - INFO - Epoch [63/100] train_loss: 0.1440, val_loss: 0.1842, lr: 0.001000, 4.17s
2023-01-04 19:59:01,675 - INFO - epoch complete!
2023-01-04 19:59:01,676 - INFO - evaluating now!
2023-01-04 19:59:02,018 - INFO - Epoch [64/100] train_loss: 0.1329, val_loss: 0.1778, lr: 0.001000, 4.21s
2023-01-04 19:59:02,033 - INFO - Saved model at 64
2023-01-04 19:59:02,033 - INFO - Val loss decrease from 0.1838 to 0.1778, saving to ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30_epoch64.tar
2023-01-04 19:59:06,203 - INFO - epoch complete!
2023-01-04 19:59:06,204 - INFO - evaluating now!
2023-01-04 19:59:06,517 - INFO - Epoch [65/100] train_loss: 0.1292, val_loss: 0.1810, lr: 0.001000, 4.48s
2023-01-04 19:59:10,625 - INFO - epoch complete!
2023-01-04 19:59:10,625 - INFO - evaluating now!
2023-01-04 19:59:10,950 - INFO - Epoch [66/100] train_loss: 0.1272, val_loss: 0.1842, lr: 0.001000, 4.43s
2023-01-04 19:59:15,247 - INFO - epoch complete!
2023-01-04 19:59:15,248 - INFO - evaluating now!
2023-01-04 19:59:15,559 - INFO - Epoch [67/100] train_loss: 0.1274, val_loss: 0.1893, lr: 0.001000, 4.61s
2023-01-04 19:59:19,912 - INFO - epoch complete!
2023-01-04 19:59:19,913 - INFO - evaluating now!
2023-01-04 19:59:20,244 - INFO - Epoch [68/100] train_loss: 0.1284, val_loss: 0.1928, lr: 0.001000, 4.68s
2023-01-04 19:59:24,271 - INFO - epoch complete!
2023-01-04 19:59:24,272 - INFO - evaluating now!
2023-01-04 19:59:24,572 - INFO - Epoch [69/100] train_loss: 0.1354, val_loss: 0.2065, lr: 0.001000, 4.33s
2023-01-04 19:59:28,936 - INFO - epoch complete!
2023-01-04 19:59:28,937 - INFO - evaluating now!
2023-01-04 19:59:29,258 - INFO - Epoch [70/100] train_loss: 0.1280, val_loss: 0.1906, lr: 0.001000, 4.69s
2023-01-04 19:59:33,405 - INFO - epoch complete!
2023-01-04 19:59:33,406 - INFO - evaluating now!
2023-01-04 19:59:33,749 - INFO - Epoch [71/100] train_loss: 0.1277, val_loss: 0.1920, lr: 0.001000, 4.49s
2023-01-04 19:59:37,874 - INFO - epoch complete!
2023-01-04 19:59:37,875 - INFO - evaluating now!
2023-01-04 19:59:38,219 - INFO - Epoch [72/100] train_loss: 0.1405, val_loss: 0.1820, lr: 0.001000, 4.47s
2023-01-04 19:59:41,879 - INFO - epoch complete!
2023-01-04 19:59:41,879 - INFO - evaluating now!
2023-01-04 19:59:42,180 - INFO - Epoch [73/100] train_loss: 0.1282, val_loss: 0.1790, lr: 0.001000, 3.96s
2023-01-04 19:59:46,491 - INFO - epoch complete!
2023-01-04 19:59:46,492 - INFO - evaluating now!
2023-01-04 19:59:46,937 - INFO - Epoch [74/100] train_loss: 0.1193, val_loss: 0.1884, lr: 0.001000, 4.76s
2023-01-04 19:59:51,082 - INFO - epoch complete!
2023-01-04 19:59:51,084 - INFO - evaluating now!
2023-01-04 19:59:51,409 - INFO - Epoch [75/100] train_loss: 0.1260, val_loss: 0.1804, lr: 0.001000, 4.47s
2023-01-04 19:59:55,664 - INFO - epoch complete!
2023-01-04 19:59:55,665 - INFO - evaluating now!
2023-01-04 19:59:55,983 - INFO - Epoch [76/100] train_loss: 0.1238, val_loss: 0.1817, lr: 0.001000, 4.57s
2023-01-04 20:00:00,173 - INFO - epoch complete!
2023-01-04 20:00:00,174 - INFO - evaluating now!
2023-01-04 20:00:00,500 - INFO - Epoch [77/100] train_loss: 0.1249, val_loss: 0.1890, lr: 0.001000, 4.52s
2023-01-04 20:00:04,629 - INFO - epoch complete!
2023-01-04 20:00:04,630 - INFO - evaluating now!
2023-01-04 20:00:05,013 - INFO - Epoch [78/100] train_loss: 0.1159, val_loss: 0.1904, lr: 0.001000, 4.51s
2023-01-04 20:00:09,160 - INFO - epoch complete!
2023-01-04 20:00:09,161 - INFO - evaluating now!
2023-01-04 20:00:09,487 - INFO - Epoch [79/100] train_loss: 0.1215, val_loss: 0.2055, lr: 0.001000, 4.47s
2023-01-04 20:00:13,787 - INFO - epoch complete!
2023-01-04 20:00:13,788 - INFO - evaluating now!
2023-01-04 20:00:14,086 - INFO - Epoch [80/100] train_loss: 0.1195, val_loss: 0.1984, lr: 0.001000, 4.60s
2023-01-04 20:00:17,859 - INFO - epoch complete!
2023-01-04 20:00:17,860 - INFO - evaluating now!
2023-01-04 20:00:18,170 - INFO - Epoch [81/100] train_loss: 0.1170, val_loss: 0.1874, lr: 0.001000, 4.08s
2023-01-04 20:00:22,369 - INFO - epoch complete!
2023-01-04 20:00:22,371 - INFO - evaluating now!
2023-01-04 20:00:22,656 - INFO - Epoch [82/100] train_loss: 0.1180, val_loss: 0.1837, lr: 0.001000, 4.48s
2023-01-04 20:00:26,826 - INFO - epoch complete!
2023-01-04 20:00:26,829 - INFO - evaluating now!
2023-01-04 20:00:27,129 - INFO - Epoch [83/100] train_loss: 0.1192, val_loss: 0.1928, lr: 0.001000, 4.47s
2023-01-04 20:00:31,188 - INFO - epoch complete!
2023-01-04 20:00:31,189 - INFO - evaluating now!
2023-01-04 20:00:31,502 - INFO - Epoch [84/100] train_loss: 0.1155, val_loss: 0.1911, lr: 0.001000, 4.37s
2023-01-04 20:00:35,619 - INFO - epoch complete!
2023-01-04 20:00:35,619 - INFO - evaluating now!
2023-01-04 20:00:36,028 - INFO - Epoch [85/100] train_loss: 0.1052, val_loss: 0.1857, lr: 0.001000, 4.53s
2023-01-04 20:00:40,279 - INFO - epoch complete!
2023-01-04 20:00:40,280 - INFO - evaluating now!
2023-01-04 20:00:40,599 - INFO - Epoch [86/100] train_loss: 0.1193, val_loss: 0.2047, lr: 0.001000, 4.57s
2023-01-04 20:00:44,787 - INFO - epoch complete!
2023-01-04 20:00:44,788 - INFO - evaluating now!
2023-01-04 20:00:45,116 - INFO - Epoch [87/100] train_loss: 0.1144, val_loss: 0.2027, lr: 0.001000, 4.52s
2023-01-04 20:00:49,294 - INFO - epoch complete!
2023-01-04 20:00:49,296 - INFO - evaluating now!
2023-01-04 20:00:49,605 - INFO - Epoch [88/100] train_loss: 0.1181, val_loss: 0.2250, lr: 0.001000, 4.49s
2023-01-04 20:00:53,843 - INFO - epoch complete!
2023-01-04 20:00:53,844 - INFO - evaluating now!
2023-01-04 20:00:54,228 - INFO - Epoch [89/100] train_loss: 0.1245, val_loss: 0.2105, lr: 0.001000, 4.62s
2023-01-04 20:00:57,982 - INFO - epoch complete!
2023-01-04 20:00:57,983 - INFO - evaluating now!
2023-01-04 20:00:58,337 - INFO - Epoch [90/100] train_loss: 0.1168, val_loss: 0.2019, lr: 0.001000, 4.11s
2023-01-04 20:01:02,006 - INFO - epoch complete!
2023-01-04 20:01:02,007 - INFO - evaluating now!
2023-01-04 20:01:02,309 - INFO - Epoch [91/100] train_loss: 0.1132, val_loss: 0.1925, lr: 0.001000, 3.97s
2023-01-04 20:01:06,397 - INFO - epoch complete!
2023-01-04 20:01:06,398 - INFO - evaluating now!
2023-01-04 20:01:06,707 - INFO - Epoch [92/100] train_loss: 0.1278, val_loss: 0.1911, lr: 0.001000, 4.40s
2023-01-04 20:01:10,630 - INFO - epoch complete!
2023-01-04 20:01:10,631 - INFO - evaluating now!
2023-01-04 20:01:10,959 - INFO - Epoch [93/100] train_loss: 0.1081, val_loss: 0.1833, lr: 0.001000, 4.25s
2023-01-04 20:01:15,312 - INFO - epoch complete!
2023-01-04 20:01:15,316 - INFO - evaluating now!
2023-01-04 20:01:15,802 - INFO - Epoch [94/100] train_loss: 0.0932, val_loss: 0.1895, lr: 0.001000, 4.84s
2023-01-04 20:01:19,930 - INFO - epoch complete!
2023-01-04 20:01:19,932 - INFO - evaluating now!
2023-01-04 20:01:20,274 - INFO - Epoch [95/100] train_loss: 0.0894, val_loss: 0.1943, lr: 0.001000, 4.47s
2023-01-04 20:01:24,168 - INFO - epoch complete!
2023-01-04 20:01:24,169 - INFO - evaluating now!
2023-01-04 20:01:24,476 - INFO - Epoch [96/100] train_loss: 0.0988, val_loss: 0.1877, lr: 0.001000, 4.20s
2023-01-04 20:01:28,345 - INFO - epoch complete!
2023-01-04 20:01:28,348 - INFO - evaluating now!
2023-01-04 20:01:28,660 - INFO - Epoch [97/100] train_loss: 0.1013, val_loss: 0.1853, lr: 0.001000, 4.18s
2023-01-04 20:01:33,094 - INFO - epoch complete!
2023-01-04 20:01:33,094 - INFO - evaluating now!
2023-01-04 20:01:33,417 - INFO - Epoch [98/100] train_loss: 0.1093, val_loss: 0.1961, lr: 0.001000, 4.76s
2023-01-04 20:01:37,737 - INFO - epoch complete!
2023-01-04 20:01:37,737 - INFO - evaluating now!
2023-01-04 20:01:38,047 - INFO - Epoch [99/100] train_loss: 0.1103, val_loss: 0.1983, lr: 0.001000, 4.63s
2023-01-04 20:01:38,048 - INFO - Trained totally 100 epochs, average train time is 4.133s, average eval time is 0.348s
2023-01-04 20:01:38,068 - INFO - Loaded model at 64
2023-01-04 20:01:38,068 - INFO - Saved model at ./libcity/cache/58525/model_cache/DeepTTE_Beijing_Taxi_Sample_ori_longer30.m
2023-01-04 20:01:38,081 - INFO - Start evaluating ...
2023-01-04 20:01:39,238 - INFO - Evaluate result is saved at ./libcity/cache/58525/evaluate_cache/2023_01_04_20_01_39_DeepTTE_Beijing_Taxi_Sample_ori_longer30.csv
2023-01-04 20:01:39,255 - INFO - 
          MAE      MAPE       MSE        RMSE  masked_MAE  masked_MAPE  masked_MSE  masked_RMSE        R2      EVAR
1  430.995087  0.198509  341127.0  584.060791  430.995087     0.198509    341127.0   584.060791  0.708366  0.749432

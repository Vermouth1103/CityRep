2023-01-04 19:16:09,242 - INFO - Log directory: ./libcity/log
2023-01-04 19:16:09,242 - INFO - Begin pipeline, task=eta, model_name=DeepTTE, dataset_name=Beijing_Taxi_Sample, exp_id=75149
2023-01-04 19:16:09,242 - INFO - {'task': 'eta', 'model': 'DeepTTE', 'dataset': 'Beijing_Taxi_Sample', 'saved_model': True, 'train': True, 'seed': 0, 'batch_size': 64, 'dataset_class': 'ETADataset', 'eta_encoder': 'DeeptteEncoder', 'executor': 'ETAExecutor', 'evaluator': 'ETAEvaluator', 'uid_emb_size': 16, 'weekid_emb_size': 3, 'timdid_emb_size': 8, 'kernel_size': 3, 'num_filter': 32, 'pooling_method': 'attention', 'num_final_fcs': 4, 'final_fc_size': 128, 'alpha': 0.1, 'rnn_type': 'LSTM', 'rnn_num_layers': 1, 'hidden_size': 128, 'max_epoch': 100, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': False, 'use_early_stop': False, 'patience': 20, 'num_workers': 0, 'min_session_len': 5, 'max_session_len': 50, 'min_sessions': 0, 'window_size': 1, 'cut_method': 'time_interval', 'pad_with_last_sample': True, 'sort_by_traj_len': True, 'cache_dataset': True, 'train_rate': 0.7, 'eval_rate': 0.1, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'max_grad_norm': 1.0, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE', 'R2', 'EVAR'], 'mode': 'single', 'save_modes': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'coordinates': 'coordinate', 'embedding': 'other'}}, 'usr': {'properties': {}}, 'dyna': {'including_types': ['trajectory'], 'trajectory': {'entity_id': 'usr_id', 'traj_id': 'num', 'coordinates': 'coordinate', 'current_dis': 'num', 'speeds': 'other', 'speeds_relevant1': 'other', 'speeds_relevant2': 'other', 'speeds_long': 'other', 'grid_len': 'num', 'holiday': 'num'}}, 'geo_file': 'Beijing_Taxi_Sample_ori_longer30', 'usr_file': 'Beijing_Taxi_Sample_ori_longer30', 'dyna_file': 'Beijing_Taxi_Sample_ori_longer30', 'device': device(type='cuda', index=0), 'exp_id': 75149}
2023-01-04 19:16:10,529 - INFO - Loading file ./libcity/cache/dataset_cache/eta_Beijing_Taxi_Sample_DeeptteEncoder.json
2023-01-04 19:16:11,038 - INFO - longi_mean: 116.38167145561566
2023-01-04 19:16:11,039 - INFO - longi_std: 0.07416283805701834
2023-01-04 19:16:11,039 - INFO - lati_mean: 39.92553892677388
2023-01-04 19:16:11,039 - INFO - lati_std: 0.049981772271572
2023-01-04 19:16:11,039 - INFO - dist_mean: 8.814695989840317
2023-01-04 19:16:11,040 - INFO - dist_std: 7.073727325486173
2023-01-04 19:16:11,040 - INFO - time_mean: 1166.8956065318819
2023-01-04 19:16:11,040 - INFO - time_std: 973.9645727168944
2023-01-04 19:16:11,040 - INFO - dist_gap_mean: 0.3897806752549975
2023-01-04 19:16:11,040 - INFO - dist_gap_std: 0.22667483085183782
2023-01-04 19:16:11,040 - INFO - time_gap_mean: 51.59943780140807
2023-01-04 19:16:11,040 - INFO - time_gap_std: 44.83032982709822
2023-01-04 19:16:11,124 - INFO - Number of train data: 15432
2023-01-04 19:16:11,124 - INFO - Number of eval  data: 2200
2023-01-04 19:16:11,124 - INFO - Number of test  data: 4368
2023-01-04 19:16:14,708 - INFO - DeepTTE(
  (attr_net): Attr(
    (uid_em): Embedding(4968, 16)
    (weekid_em): Embedding(7, 3)
    (timeid_em): Embedding(1440, 8)
  )
  (spatio_temporal): SpatioTemporal(
    (geo_conv): GeoConv(
      (state_em): Embedding(2, 2)
      (process_coords): Linear(in_features=4, out_features=16, bias=True)
      (conv): Conv1d(16, 32, kernel_size=(3,), stride=(1,))
    )
    (rnn): LSTM(61, 128, batch_first=True)
    (attr2atten): Linear(in_features=28, out_features=128, bias=True)
  )
  (entire_estimate): EntireEstimator(
    (input2hid): Linear(in_features=156, out_features=128, bias=True)
    (residuals): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Linear(in_features=128, out_features=128, bias=True)
    )
    (hid2out): Linear(in_features=128, out_features=1, bias=True)
  )
  (local_estimate): LocalEstimator(
    (input2hid): Linear(in_features=128, out_features=64, bias=True)
    (hid2hid): Linear(in_features=64, out_features=32, bias=True)
    (hid2out): Linear(in_features=32, out_features=1, bias=True)
  )
)
2023-01-04 19:16:14,709 - INFO - attr_net.uid_em.weight	torch.Size([4968, 16])	cuda:0	True
2023-01-04 19:16:14,709 - INFO - attr_net.weekid_em.weight	torch.Size([7, 3])	cuda:0	True
2023-01-04 19:16:14,709 - INFO - attr_net.timeid_em.weight	torch.Size([1440, 8])	cuda:0	True
2023-01-04 19:16:14,709 - INFO - spatio_temporal.geo_conv.state_em.weight	torch.Size([2, 2])	cuda:0	True
2023-01-04 19:16:14,709 - INFO - spatio_temporal.geo_conv.process_coords.weight	torch.Size([16, 4])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.geo_conv.process_coords.bias	torch.Size([16])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.geo_conv.conv.weight	torch.Size([32, 16, 3])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.geo_conv.conv.bias	torch.Size([32])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.rnn.weight_ih_l0	torch.Size([512, 61])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.rnn.weight_hh_l0	torch.Size([512, 128])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.rnn.bias_ih_l0	torch.Size([512])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.rnn.bias_hh_l0	torch.Size([512])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.attr2atten.weight	torch.Size([128, 28])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - spatio_temporal.attr2atten.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - entire_estimate.input2hid.weight	torch.Size([128, 156])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - entire_estimate.input2hid.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:16:14,710 - INFO - entire_estimate.residuals.0.weight	torch.Size([128, 128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.residuals.0.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.residuals.1.weight	torch.Size([128, 128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.residuals.1.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.residuals.2.weight	torch.Size([128, 128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.residuals.2.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.residuals.3.weight	torch.Size([128, 128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.residuals.3.bias	torch.Size([128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.hid2out.weight	torch.Size([1, 128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - entire_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - local_estimate.input2hid.weight	torch.Size([64, 128])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - local_estimate.input2hid.bias	torch.Size([64])	cuda:0	True
2023-01-04 19:16:14,711 - INFO - local_estimate.hid2hid.weight	torch.Size([32, 64])	cuda:0	True
2023-01-04 19:16:14,712 - INFO - local_estimate.hid2hid.bias	torch.Size([32])	cuda:0	True
2023-01-04 19:16:14,712 - INFO - local_estimate.hid2out.weight	torch.Size([1, 32])	cuda:0	True
2023-01-04 19:16:14,712 - INFO - local_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-04 19:16:14,712 - INFO - Total parameter numbers: 290827
2023-01-04 19:16:14,712 - INFO - You select `adam` optimizer.
2023-01-04 19:16:14,713 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-01-04 19:16:14,713 - INFO - Start training ...
2023-01-04 19:16:14,713 - INFO - num_batches:242
2023-01-04 19:16:25,020 - INFO - epoch complete!
2023-01-04 19:16:25,020 - INFO - evaluating now!
2023-01-04 19:16:25,767 - INFO - Epoch [0/100] train_loss: 0.4573, val_loss: 0.2972, lr: 0.001000, 11.05s
2023-01-04 19:16:25,781 - INFO - Saved model at 0
2023-01-04 19:16:25,781 - INFO - Val loss decrease from inf to 0.2972, saving to ./libcity/cache/75149/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch0.tar
2023-01-04 19:16:35,768 - INFO - epoch complete!
2023-01-04 19:16:35,768 - INFO - evaluating now!
2023-01-04 19:16:36,486 - INFO - Epoch [1/100] train_loss: 0.2870, val_loss: 0.2561, lr: 0.001000, 10.70s
2023-01-04 19:16:36,499 - INFO - Saved model at 1
2023-01-04 19:16:36,499 - INFO - Val loss decrease from 0.2972 to 0.2561, saving to ./libcity/cache/75149/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch1.tar
2023-01-04 19:16:46,055 - INFO - epoch complete!
2023-01-04 19:16:46,056 - INFO - evaluating now!
2023-01-04 19:16:46,889 - INFO - Epoch [2/100] train_loss: 0.2787, val_loss: 0.2282, lr: 0.001000, 10.39s
2023-01-04 19:16:46,905 - INFO - Saved model at 2
2023-01-04 19:16:46,905 - INFO - Val loss decrease from 0.2561 to 0.2282, saving to ./libcity/cache/75149/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch2.tar
2023-01-04 19:16:56,704 - INFO - epoch complete!
2023-01-04 19:16:56,705 - INFO - evaluating now!
2023-01-04 19:16:57,438 - INFO - Epoch [3/100] train_loss: 0.2472, val_loss: 0.2497, lr: 0.001000, 10.53s
2023-01-04 19:17:07,198 - INFO - epoch complete!
2023-01-04 19:17:07,199 - INFO - evaluating now!
2023-01-04 19:17:07,880 - INFO - Epoch [4/100] train_loss: 0.2216, val_loss: 0.2367, lr: 0.001000, 10.44s
2023-01-04 19:17:18,002 - INFO - epoch complete!
2023-01-04 19:17:18,003 - INFO - evaluating now!
2023-01-04 19:17:18,724 - INFO - Epoch [5/100] train_loss: 0.2210, val_loss: 0.2330, lr: 0.001000, 10.84s
2023-01-04 19:17:28,480 - INFO - epoch complete!
2023-01-04 19:17:28,481 - INFO - evaluating now!
2023-01-04 19:17:29,294 - INFO - Epoch [6/100] train_loss: 0.2158, val_loss: 0.2314, lr: 0.001000, 10.57s
2023-01-04 19:17:39,284 - INFO - epoch complete!
2023-01-04 19:17:39,284 - INFO - evaluating now!
2023-01-04 19:17:40,061 - INFO - Epoch [7/100] train_loss: 0.2223, val_loss: 0.2122, lr: 0.001000, 10.77s
2023-01-04 19:17:40,074 - INFO - Saved model at 7
2023-01-04 19:17:40,075 - INFO - Val loss decrease from 0.2282 to 0.2122, saving to ./libcity/cache/75149/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch7.tar
2023-01-04 19:17:50,030 - INFO - epoch complete!
2023-01-04 19:17:50,031 - INFO - evaluating now!
2023-01-04 19:17:50,801 - INFO - Epoch [8/100] train_loss: 0.2133, val_loss: 0.2177, lr: 0.001000, 10.73s
2023-01-04 19:18:00,764 - INFO - epoch complete!
2023-01-04 19:18:00,764 - INFO - evaluating now!
2023-01-04 19:18:01,739 - INFO - Epoch [9/100] train_loss: 0.2102, val_loss: 0.2131, lr: 0.001000, 10.94s
2023-01-04 19:18:11,711 - INFO - epoch complete!
2023-01-04 19:18:11,711 - INFO - evaluating now!
2023-01-04 19:18:12,371 - INFO - Epoch [10/100] train_loss: 0.2002, val_loss: 0.2073, lr: 0.001000, 10.63s
2023-01-04 19:18:12,385 - INFO - Saved model at 10
2023-01-04 19:18:12,386 - INFO - Val loss decrease from 0.2122 to 0.2073, saving to ./libcity/cache/75149/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch10.tar
2023-01-04 19:18:22,124 - INFO - epoch complete!
2023-01-04 19:18:22,125 - INFO - evaluating now!
2023-01-04 19:18:22,913 - INFO - Epoch [11/100] train_loss: 0.2076, val_loss: 0.2115, lr: 0.001000, 10.53s
2023-01-04 19:18:33,025 - INFO - epoch complete!
2023-01-04 19:18:33,026 - INFO - evaluating now!
2023-01-04 19:18:33,693 - INFO - Epoch [12/100] train_loss: 0.2053, val_loss: 0.2223, lr: 0.001000, 10.78s
2023-01-04 19:18:44,121 - INFO - epoch complete!
2023-01-04 19:18:44,122 - INFO - evaluating now!
2023-01-04 19:18:45,079 - INFO - Epoch [13/100] train_loss: 0.1960, val_loss: 0.2008, lr: 0.001000, 11.39s
2023-01-04 19:18:45,092 - INFO - Saved model at 13
2023-01-04 19:18:45,092 - INFO - Val loss decrease from 0.2073 to 0.2008, saving to ./libcity/cache/75149/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch13.tar
2023-01-04 19:18:54,776 - INFO - epoch complete!
2023-01-04 19:18:54,777 - INFO - evaluating now!
2023-01-04 19:18:55,526 - INFO - Epoch [14/100] train_loss: 0.1928, val_loss: 0.2008, lr: 0.001000, 10.43s
2023-01-04 19:19:05,265 - INFO - epoch complete!
2023-01-04 19:19:05,266 - INFO - evaluating now!
2023-01-04 19:19:05,983 - INFO - Epoch [15/100] train_loss: 0.1965, val_loss: 0.2067, lr: 0.001000, 10.46s
2023-01-04 19:19:15,974 - INFO - epoch complete!
2023-01-04 19:19:15,975 - INFO - evaluating now!
2023-01-04 19:19:16,787 - INFO - Epoch [16/100] train_loss: 0.1952, val_loss: 0.2098, lr: 0.001000, 10.80s
2023-01-04 19:19:26,792 - INFO - epoch complete!
2023-01-04 19:19:26,800 - INFO - evaluating now!
2023-01-04 19:19:27,538 - INFO - Epoch [17/100] train_loss: 0.1944, val_loss: 0.2034, lr: 0.001000, 10.75s
2023-01-04 19:19:37,469 - INFO - epoch complete!
2023-01-04 19:19:37,470 - INFO - evaluating now!
2023-01-04 19:19:38,130 - INFO - Epoch [18/100] train_loss: 0.1906, val_loss: 0.1994, lr: 0.001000, 10.59s
2023-01-04 19:19:38,143 - INFO - Saved model at 18
2023-01-04 19:19:38,144 - INFO - Val loss decrease from 0.2008 to 0.1994, saving to ./libcity/cache/75149/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch18.tar
2023-01-04 19:19:48,147 - INFO - epoch complete!
2023-01-04 19:19:48,147 - INFO - evaluating now!
2023-01-04 19:19:48,847 - INFO - Epoch [19/100] train_loss: 0.1912, val_loss: 0.2030, lr: 0.001000, 10.70s
2023-01-04 19:19:58,675 - INFO - epoch complete!
2023-01-04 19:19:58,680 - INFO - evaluating now!
2023-01-04 19:19:59,487 - INFO - Epoch [20/100] train_loss: 0.1883, val_loss: 0.2056, lr: 0.001000, 10.64s
2023-01-04 19:20:09,347 - INFO - epoch complete!
2023-01-04 19:20:09,348 - INFO - evaluating now!
2023-01-04 19:20:10,130 - INFO - Epoch [21/100] train_loss: 0.1914, val_loss: 0.2080, lr: 0.001000, 10.64s
2023-01-04 19:20:19,983 - INFO - epoch complete!
2023-01-04 19:20:19,985 - INFO - evaluating now!
2023-01-04 19:20:20,712 - INFO - Epoch [22/100] train_loss: 0.1880, val_loss: 0.2090, lr: 0.001000, 10.58s
2023-01-04 19:20:30,628 - INFO - epoch complete!
2023-01-04 19:20:30,629 - INFO - evaluating now!
2023-01-04 19:20:31,306 - INFO - Epoch [23/100] train_loss: 0.1872, val_loss: 0.2053, lr: 0.001000, 10.59s
2023-01-04 19:20:41,035 - INFO - epoch complete!
2023-01-04 19:20:41,035 - INFO - evaluating now!
2023-01-04 19:20:41,685 - INFO - Epoch [24/100] train_loss: 0.1864, val_loss: 0.2090, lr: 0.001000, 10.38s
2023-01-04 19:20:51,432 - INFO - epoch complete!
2023-01-04 19:20:51,433 - INFO - evaluating now!
2023-01-04 19:20:52,332 - INFO - Epoch [25/100] train_loss: 0.1915, val_loss: 0.2508, lr: 0.001000, 10.65s
2023-01-04 19:21:02,133 - INFO - epoch complete!
2023-01-04 19:21:02,134 - INFO - evaluating now!
2023-01-04 19:21:02,989 - INFO - Epoch [26/100] train_loss: 0.1916, val_loss: 0.2327, lr: 0.001000, 10.66s
2023-01-04 19:21:13,012 - INFO - epoch complete!
2023-01-04 19:21:13,013 - INFO - evaluating now!
2023-01-04 19:21:13,820 - INFO - Epoch [27/100] train_loss: 0.1891, val_loss: 0.2270, lr: 0.001000, 10.83s
2023-01-04 19:21:23,291 - INFO - epoch complete!
2023-01-04 19:21:23,292 - INFO - evaluating now!
2023-01-04 19:21:24,059 - INFO - Epoch [28/100] train_loss: 0.1859, val_loss: 0.2195, lr: 0.001000, 10.24s
2023-01-04 19:21:34,377 - INFO - epoch complete!
2023-01-04 19:21:34,377 - INFO - evaluating now!
2023-01-04 19:21:35,084 - INFO - Epoch [29/100] train_loss: 0.1839, val_loss: 0.2237, lr: 0.001000, 11.02s
2023-01-04 19:21:44,806 - INFO - epoch complete!
2023-01-04 19:21:44,807 - INFO - evaluating now!
2023-01-04 19:21:45,656 - INFO - Epoch [30/100] train_loss: 0.1847, val_loss: 0.2173, lr: 0.001000, 10.57s
2023-01-04 19:21:55,176 - INFO - epoch complete!
2023-01-04 19:21:55,177 - INFO - evaluating now!
2023-01-04 19:21:55,784 - INFO - Epoch [31/100] train_loss: 0.1831, val_loss: 0.2125, lr: 0.001000, 10.13s
2023-01-04 19:22:06,055 - INFO - epoch complete!
2023-01-04 19:22:06,056 - INFO - evaluating now!
2023-01-04 19:22:06,724 - INFO - Epoch [32/100] train_loss: 0.1813, val_loss: 0.2115, lr: 0.001000, 10.94s
2023-01-04 19:22:16,588 - INFO - epoch complete!
2023-01-04 19:22:16,589 - INFO - evaluating now!
2023-01-04 19:22:17,356 - INFO - Epoch [33/100] train_loss: 0.1803, val_loss: 0.2205, lr: 0.001000, 10.63s
2023-01-04 19:22:27,166 - INFO - epoch complete!
2023-01-04 19:22:27,166 - INFO - evaluating now!
2023-01-04 19:22:27,951 - INFO - Epoch [34/100] train_loss: 0.1806, val_loss: 0.2155, lr: 0.001000, 10.60s
2023-01-04 19:22:37,703 - INFO - epoch complete!
2023-01-04 19:22:37,704 - INFO - evaluating now!
2023-01-04 19:22:38,503 - INFO - Epoch [35/100] train_loss: 0.1814, val_loss: 0.2196, lr: 0.001000, 10.55s
2023-01-04 19:22:48,474 - INFO - epoch complete!
2023-01-04 19:22:48,474 - INFO - evaluating now!
2023-01-04 19:22:49,318 - INFO - Epoch [36/100] train_loss: 0.1778, val_loss: 0.2156, lr: 0.001000, 10.81s
2023-01-04 19:22:59,290 - INFO - epoch complete!
2023-01-04 19:22:59,292 - INFO - evaluating now!
2023-01-04 19:23:00,037 - INFO - Epoch [37/100] train_loss: 0.1767, val_loss: 0.2186, lr: 0.001000, 10.72s
2023-01-04 19:23:10,203 - INFO - epoch complete!
2023-01-04 19:23:10,203 - INFO - evaluating now!
2023-01-04 19:23:11,141 - INFO - Epoch [38/100] train_loss: 0.1765, val_loss: 0.2066, lr: 0.001000, 11.10s
2023-01-04 19:23:21,170 - INFO - epoch complete!
2023-01-04 19:23:21,171 - INFO - evaluating now!
2023-01-04 19:23:21,947 - INFO - Epoch [39/100] train_loss: 0.1767, val_loss: 0.2104, lr: 0.001000, 10.81s
2023-01-04 19:23:31,927 - INFO - epoch complete!
2023-01-04 19:23:31,928 - INFO - evaluating now!
2023-01-04 19:23:32,728 - INFO - Epoch [40/100] train_loss: 0.1750, val_loss: 0.2211, lr: 0.001000, 10.78s
2023-01-04 19:23:42,574 - INFO - epoch complete!
2023-01-04 19:23:42,575 - INFO - evaluating now!
2023-01-04 19:23:43,309 - INFO - Epoch [41/100] train_loss: 0.1775, val_loss: 0.2154, lr: 0.001000, 10.58s
2023-01-04 19:23:53,023 - INFO - epoch complete!
2023-01-04 19:23:53,023 - INFO - evaluating now!
2023-01-04 19:23:53,788 - INFO - Epoch [42/100] train_loss: 0.1811, val_loss: 0.2391, lr: 0.001000, 10.48s
2023-01-04 19:24:03,447 - INFO - epoch complete!
2023-01-04 19:24:03,448 - INFO - evaluating now!
2023-01-04 19:24:04,134 - INFO - Epoch [43/100] train_loss: 0.1825, val_loss: 0.2092, lr: 0.001000, 10.35s
2023-01-04 19:24:13,824 - INFO - epoch complete!
2023-01-04 19:24:13,825 - INFO - evaluating now!
2023-01-04 19:24:14,502 - INFO - Epoch [44/100] train_loss: 0.1790, val_loss: 0.2073, lr: 0.001000, 10.37s
2023-01-04 19:24:24,369 - INFO - epoch complete!
2023-01-04 19:24:24,369 - INFO - evaluating now!
2023-01-04 19:24:25,166 - INFO - Epoch [45/100] train_loss: 0.1748, val_loss: 0.2019, lr: 0.001000, 10.66s
2023-01-04 19:24:34,843 - INFO - epoch complete!
2023-01-04 19:24:34,844 - INFO - evaluating now!
2023-01-04 19:24:35,661 - INFO - Epoch [46/100] train_loss: 0.1719, val_loss: 0.2069, lr: 0.001000, 10.49s
2023-01-04 19:24:45,230 - INFO - epoch complete!
2023-01-04 19:24:45,231 - INFO - evaluating now!
2023-01-04 19:24:45,905 - INFO - Epoch [47/100] train_loss: 0.1690, val_loss: 0.2117, lr: 0.001000, 10.24s
2023-01-04 19:24:55,642 - INFO - epoch complete!
2023-01-04 19:24:55,643 - INFO - evaluating now!
2023-01-04 19:24:56,497 - INFO - Epoch [48/100] train_loss: 0.1691, val_loss: 0.2116, lr: 0.001000, 10.59s
2023-01-04 19:25:06,455 - INFO - epoch complete!
2023-01-04 19:25:06,456 - INFO - evaluating now!
2023-01-04 19:25:07,069 - INFO - Epoch [49/100] train_loss: 0.1646, val_loss: 0.2077, lr: 0.001000, 10.57s
2023-01-04 19:25:16,917 - INFO - epoch complete!
2023-01-04 19:25:16,918 - INFO - evaluating now!
2023-01-04 19:25:17,810 - INFO - Epoch [50/100] train_loss: 0.1636, val_loss: 0.2084, lr: 0.001000, 10.74s
2023-01-04 19:25:27,706 - INFO - epoch complete!
2023-01-04 19:25:27,707 - INFO - evaluating now!
2023-01-04 19:25:28,600 - INFO - Epoch [51/100] train_loss: 0.1621, val_loss: 0.2055, lr: 0.001000, 10.79s
2023-01-04 19:25:38,667 - INFO - epoch complete!
2023-01-04 19:25:38,668 - INFO - evaluating now!
2023-01-04 19:25:39,516 - INFO - Epoch [52/100] train_loss: 0.1590, val_loss: 0.2080, lr: 0.001000, 10.92s
2023-01-04 19:25:50,582 - INFO - epoch complete!
2023-01-04 19:25:50,584 - INFO - evaluating now!
2023-01-04 19:25:51,475 - INFO - Epoch [53/100] train_loss: 0.1593, val_loss: 0.2163, lr: 0.001000, 11.96s
2023-01-04 19:26:03,484 - INFO - epoch complete!
2023-01-04 19:26:03,485 - INFO - evaluating now!
2023-01-04 19:26:04,303 - INFO - Epoch [54/100] train_loss: 0.1565, val_loss: 0.2124, lr: 0.001000, 12.83s
2023-01-04 19:26:16,138 - INFO - epoch complete!
2023-01-04 19:26:16,139 - INFO - evaluating now!
2023-01-04 19:26:17,171 - INFO - Epoch [55/100] train_loss: 0.1545, val_loss: 0.2197, lr: 0.001000, 12.87s
2023-01-04 19:26:28,387 - INFO - epoch complete!
2023-01-04 19:26:28,388 - INFO - evaluating now!
2023-01-04 19:26:29,317 - INFO - Epoch [56/100] train_loss: 0.1542, val_loss: 0.2133, lr: 0.001000, 12.15s
2023-01-04 19:26:41,705 - INFO - epoch complete!
2023-01-04 19:26:41,707 - INFO - evaluating now!
2023-01-04 19:26:42,606 - INFO - Epoch [57/100] train_loss: 0.1537, val_loss: 0.2148, lr: 0.001000, 13.29s
2023-01-04 19:26:54,725 - INFO - epoch complete!
2023-01-04 19:26:54,726 - INFO - evaluating now!
2023-01-04 19:26:55,702 - INFO - Epoch [58/100] train_loss: 0.1531, val_loss: 0.2466, lr: 0.001000, 13.10s
2023-01-04 19:27:07,528 - INFO - epoch complete!
2023-01-04 19:27:07,530 - INFO - evaluating now!
2023-01-04 19:27:08,311 - INFO - Epoch [59/100] train_loss: 0.1542, val_loss: 0.2507, lr: 0.001000, 12.61s
2023-01-04 19:27:20,436 - INFO - epoch complete!
2023-01-04 19:27:20,437 - INFO - evaluating now!
2023-01-04 19:27:21,421 - INFO - Epoch [60/100] train_loss: 0.1538, val_loss: 0.2121, lr: 0.001000, 13.11s
2023-01-04 19:27:33,378 - INFO - epoch complete!
2023-01-04 19:27:33,378 - INFO - evaluating now!
2023-01-04 19:27:34,492 - INFO - Epoch [61/100] train_loss: 0.1521, val_loss: 0.2176, lr: 0.001000, 13.07s
2023-01-04 19:27:46,311 - INFO - epoch complete!
2023-01-04 19:27:46,312 - INFO - evaluating now!
2023-01-04 19:27:47,264 - INFO - Epoch [62/100] train_loss: 0.1491, val_loss: 0.2159, lr: 0.001000, 12.77s
2023-01-04 19:27:59,517 - INFO - epoch complete!
2023-01-04 19:27:59,518 - INFO - evaluating now!
2023-01-04 19:28:00,372 - INFO - Epoch [63/100] train_loss: 0.1469, val_loss: 0.2212, lr: 0.001000, 13.11s
2023-01-04 19:28:12,723 - INFO - epoch complete!
2023-01-04 19:28:12,723 - INFO - evaluating now!
2023-01-04 19:28:13,804 - INFO - Epoch [64/100] train_loss: 0.1467, val_loss: 0.2229, lr: 0.001000, 13.43s
2023-01-04 19:28:25,927 - INFO - epoch complete!
2023-01-04 19:28:25,929 - INFO - evaluating now!
2023-01-04 19:28:26,738 - INFO - Epoch [65/100] train_loss: 0.1482, val_loss: 0.2302, lr: 0.001000, 12.93s
2023-01-04 19:28:38,659 - INFO - epoch complete!
2023-01-04 19:28:38,662 - INFO - evaluating now!
2023-01-04 19:28:39,515 - INFO - Epoch [66/100] train_loss: 0.1468, val_loss: 0.2274, lr: 0.001000, 12.78s
2023-01-04 19:28:51,549 - INFO - epoch complete!
2023-01-04 19:28:51,550 - INFO - evaluating now!
2023-01-04 19:28:52,487 - INFO - Epoch [67/100] train_loss: 0.1443, val_loss: 0.2409, lr: 0.001000, 12.97s
2023-01-04 19:29:04,258 - INFO - epoch complete!
2023-01-04 19:29:04,260 - INFO - evaluating now!
2023-01-04 19:29:05,040 - INFO - Epoch [68/100] train_loss: 0.1410, val_loss: 0.2251, lr: 0.001000, 12.55s
2023-01-04 19:29:16,684 - INFO - epoch complete!
2023-01-04 19:29:16,687 - INFO - evaluating now!
2023-01-04 19:29:17,672 - INFO - Epoch [69/100] train_loss: 0.1409, val_loss: 0.2338, lr: 0.001000, 12.63s
2023-01-04 19:29:30,030 - INFO - epoch complete!
2023-01-04 19:29:30,031 - INFO - evaluating now!
2023-01-04 19:29:30,847 - INFO - Epoch [70/100] train_loss: 0.1425, val_loss: 0.2304, lr: 0.001000, 13.17s
2023-01-04 19:29:43,669 - INFO - epoch complete!
2023-01-04 19:29:43,670 - INFO - evaluating now!
2023-01-04 19:29:44,568 - INFO - Epoch [71/100] train_loss: 0.1404, val_loss: 0.2365, lr: 0.001000, 13.72s
2023-01-04 19:29:56,008 - INFO - epoch complete!
2023-01-04 19:29:56,009 - INFO - evaluating now!
2023-01-04 19:29:56,941 - INFO - Epoch [72/100] train_loss: 0.1419, val_loss: 0.2277, lr: 0.001000, 12.37s
2023-01-04 19:30:09,516 - INFO - epoch complete!
2023-01-04 19:30:09,517 - INFO - evaluating now!
2023-01-04 19:30:10,401 - INFO - Epoch [73/100] train_loss: 0.1422, val_loss: 0.2448, lr: 0.001000, 13.46s
2023-01-04 19:30:23,030 - INFO - epoch complete!
2023-01-04 19:30:23,031 - INFO - evaluating now!
2023-01-04 19:30:24,053 - INFO - Epoch [74/100] train_loss: 0.1388, val_loss: 0.2439, lr: 0.001000, 13.65s
2023-01-04 19:30:35,527 - INFO - epoch complete!
2023-01-04 19:30:35,528 - INFO - evaluating now!
2023-01-04 19:30:36,456 - INFO - Epoch [75/100] train_loss: 0.1408, val_loss: 0.2336, lr: 0.001000, 12.40s
2023-01-04 19:30:48,682 - INFO - epoch complete!
2023-01-04 19:30:48,683 - INFO - evaluating now!
2023-01-04 19:30:49,563 - INFO - Epoch [76/100] train_loss: 0.1422, val_loss: 0.2232, lr: 0.001000, 13.11s
2023-01-04 19:31:01,373 - INFO - epoch complete!
2023-01-04 19:31:01,374 - INFO - evaluating now!
2023-01-04 19:31:02,314 - INFO - Epoch [77/100] train_loss: 0.1432, val_loss: 0.2323, lr: 0.001000, 12.75s
2023-01-04 19:31:13,794 - INFO - epoch complete!
2023-01-04 19:31:13,795 - INFO - evaluating now!
2023-01-04 19:31:14,696 - INFO - Epoch [78/100] train_loss: 0.1389, val_loss: 0.2230, lr: 0.001000, 12.38s
2023-01-04 19:31:26,278 - INFO - epoch complete!
2023-01-04 19:31:26,280 - INFO - evaluating now!
2023-01-04 19:31:27,178 - INFO - Epoch [79/100] train_loss: 0.1379, val_loss: 0.2237, lr: 0.001000, 12.48s
2023-01-04 19:31:39,212 - INFO - epoch complete!
2023-01-04 19:31:39,214 - INFO - evaluating now!
2023-01-04 19:31:40,238 - INFO - Epoch [80/100] train_loss: 0.1354, val_loss: 0.2347, lr: 0.001000, 13.06s
2023-01-04 19:31:52,409 - INFO - epoch complete!
2023-01-04 19:31:52,409 - INFO - evaluating now!
2023-01-04 19:31:53,257 - INFO - Epoch [81/100] train_loss: 0.1392, val_loss: 0.2367, lr: 0.001000, 13.02s
2023-01-04 19:32:05,070 - INFO - epoch complete!
2023-01-04 19:32:05,072 - INFO - evaluating now!
2023-01-04 19:32:05,906 - INFO - Epoch [82/100] train_loss: 0.1416, val_loss: 0.2414, lr: 0.001000, 12.65s
2023-01-04 19:32:17,888 - INFO - epoch complete!
2023-01-04 19:32:17,889 - INFO - evaluating now!
2023-01-04 19:32:18,804 - INFO - Epoch [83/100] train_loss: 0.1423, val_loss: 0.2447, lr: 0.001000, 12.90s
2023-01-04 19:32:30,643 - INFO - epoch complete!
2023-01-04 19:32:30,644 - INFO - evaluating now!
2023-01-04 19:32:31,439 - INFO - Epoch [84/100] train_loss: 0.1418, val_loss: 0.2455, lr: 0.001000, 12.63s
2023-01-04 19:32:43,296 - INFO - epoch complete!
2023-01-04 19:32:43,297 - INFO - evaluating now!
2023-01-04 19:32:44,179 - INFO - Epoch [85/100] train_loss: 0.1377, val_loss: 0.2309, lr: 0.001000, 12.74s
2023-01-04 19:32:56,277 - INFO - epoch complete!
2023-01-04 19:32:56,288 - INFO - evaluating now!
2023-01-04 19:32:57,281 - INFO - Epoch [86/100] train_loss: 0.1325, val_loss: 0.2227, lr: 0.001000, 13.10s
2023-01-04 19:33:09,246 - INFO - epoch complete!
2023-01-04 19:33:09,246 - INFO - evaluating now!
2023-01-04 19:33:10,111 - INFO - Epoch [87/100] train_loss: 0.1285, val_loss: 0.2194, lr: 0.001000, 12.83s
2023-01-04 19:33:21,818 - INFO - epoch complete!
2023-01-04 19:33:21,823 - INFO - evaluating now!
2023-01-04 19:33:22,719 - INFO - Epoch [88/100] train_loss: 0.1231, val_loss: 0.2218, lr: 0.001000, 12.61s
2023-01-04 19:33:35,216 - INFO - epoch complete!
2023-01-04 19:33:35,216 - INFO - evaluating now!
2023-01-04 19:33:36,023 - INFO - Epoch [89/100] train_loss: 0.1212, val_loss: 0.2205, lr: 0.001000, 13.30s
2023-01-04 19:33:47,840 - INFO - epoch complete!
2023-01-04 19:33:47,841 - INFO - evaluating now!
2023-01-04 19:33:48,670 - INFO - Epoch [90/100] train_loss: 0.1212, val_loss: 0.2259, lr: 0.001000, 12.64s
2023-01-04 19:34:00,214 - INFO - epoch complete!
2023-01-04 19:34:00,215 - INFO - evaluating now!
2023-01-04 19:34:01,159 - INFO - Epoch [91/100] train_loss: 0.1192, val_loss: 0.2187, lr: 0.001000, 12.49s
2023-01-04 19:34:13,428 - INFO - epoch complete!
2023-01-04 19:34:13,429 - INFO - evaluating now!
2023-01-04 19:34:14,464 - INFO - Epoch [92/100] train_loss: 0.1161, val_loss: 0.2216, lr: 0.001000, 13.30s
2023-01-04 19:34:26,386 - INFO - epoch complete!
2023-01-04 19:34:26,387 - INFO - evaluating now!
2023-01-04 19:34:27,285 - INFO - Epoch [93/100] train_loss: 0.1139, val_loss: 0.2208, lr: 0.001000, 12.82s
2023-01-04 19:34:38,861 - INFO - epoch complete!
2023-01-04 19:34:38,862 - INFO - evaluating now!
2023-01-04 19:34:39,777 - INFO - Epoch [94/100] train_loss: 0.1147, val_loss: 0.2202, lr: 0.001000, 12.49s
2023-01-04 19:34:51,914 - INFO - epoch complete!
2023-01-04 19:34:51,916 - INFO - evaluating now!
2023-01-04 19:34:53,012 - INFO - Epoch [95/100] train_loss: 0.1134, val_loss: 0.2217, lr: 0.001000, 13.23s
2023-01-04 19:35:05,376 - INFO - epoch complete!
2023-01-04 19:35:05,377 - INFO - evaluating now!
2023-01-04 19:35:06,352 - INFO - Epoch [96/100] train_loss: 0.1126, val_loss: 0.2325, lr: 0.001000, 13.34s
2023-01-04 19:35:18,044 - INFO - epoch complete!
2023-01-04 19:35:18,045 - INFO - evaluating now!
2023-01-04 19:35:18,934 - INFO - Epoch [97/100] train_loss: 0.1173, val_loss: 0.2297, lr: 0.001000, 12.58s
2023-01-04 19:35:30,624 - INFO - epoch complete!
2023-01-04 19:35:30,626 - INFO - evaluating now!
2023-01-04 19:35:31,642 - INFO - Epoch [98/100] train_loss: 0.1162, val_loss: 0.2257, lr: 0.001000, 12.71s
2023-01-04 19:35:43,615 - INFO - epoch complete!
2023-01-04 19:35:43,616 - INFO - evaluating now!
2023-01-04 19:35:44,467 - INFO - Epoch [99/100] train_loss: 0.1157, val_loss: 0.2267, lr: 0.001000, 12.82s
2023-01-04 19:35:44,469 - INFO - Trained totally 100 epochs, average train time is 10.855s, average eval time is 0.838s
2023-01-04 19:35:44,496 - INFO - Loaded model at 18
2023-01-04 19:35:44,497 - INFO - Saved model at ./libcity/cache/75149/model_cache/DeepTTE_Beijing_Taxi_Sample.m
2023-01-04 19:35:44,529 - INFO - Start evaluating ...
2023-01-04 19:35:48,035 - INFO - Evaluate result is saved at ./libcity/cache/75149/evaluate_cache/2023_01_04_19_35_48_DeepTTE_Beijing_Taxi_Sample.csv
2023-01-04 19:35:48,055 - INFO - 
          MAE      MAPE            MSE        RMSE  masked_MAE  masked_MAPE     masked_MSE  masked_RMSE      R2      EVAR
1  210.398178  0.199078  107169.226562  327.367126  210.398178     0.199078  107169.226562   327.367126  0.8722  0.877532

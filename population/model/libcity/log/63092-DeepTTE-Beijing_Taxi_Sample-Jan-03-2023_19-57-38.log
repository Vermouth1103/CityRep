2023-01-03 19:57:38,716 - INFO - Log directory: ./libcity/log
2023-01-03 19:57:38,716 - INFO - Begin pipeline, task=eta, model_name=DeepTTE, dataset_name=Beijing_Taxi_Sample, exp_id=63092
2023-01-03 19:57:38,716 - INFO - {'task': 'eta', 'model': 'DeepTTE', 'dataset': 'Beijing_Taxi_Sample', 'saved_model': True, 'train': True, 'seed': 0, 'batch_size': 64, 'dataset_class': 'ETADataset', 'eta_encoder': 'DeeptteEncoder', 'executor': 'ETAExecutor', 'evaluator': 'ETAEvaluator', 'uid_emb_size': 16, 'weekid_emb_size': 3, 'timdid_emb_size': 8, 'kernel_size': 3, 'num_filter': 32, 'pooling_method': 'attention', 'num_final_fcs': 4, 'final_fc_size': 128, 'alpha': 0.1, 'rnn_type': 'LSTM', 'rnn_num_layers': 1, 'hidden_size': 128, 'max_epoch': 100, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': False, 'use_early_stop': False, 'patience': 20, 'num_workers': 0, 'min_session_len': 5, 'max_session_len': 50, 'min_sessions': 0, 'window_size': 1, 'cut_method': 'time_interval', 'pad_with_last_sample': True, 'sort_by_traj_len': True, 'cache_dataset': True, 'train_rate': 0.7, 'eval_rate': 0.1, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'max_grad_norm': 1.0, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE', 'R2', 'EVAR'], 'mode': 'single', 'save_modes': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'coordinates': 'coordinate', 'embedding': 'other'}}, 'usr': {'properties': {}}, 'dyna': {'including_types': ['trajectory'], 'trajectory': {'entity_id': 'usr_id', 'traj_id': 'num', 'coordinates': 'coordinate', 'current_dis': 'num', 'speeds': 'other', 'speeds_relevant1': 'other', 'speeds_relevant2': 'other', 'speeds_long': 'other', 'grid_len': 'num', 'holiday': 'num'}}, 'geo_file': 'Beijing_Taxi_Sample', 'usr_file': 'Beijing_Taxi_Sample', 'dyna_file': 'Beijing_Taxi_Sample', 'device': device(type='cuda', index=0), 'exp_id': 63092}
2023-01-03 19:57:39,659 - INFO - Loading file ./libcity/cache/dataset_cache/eta_Beijing_Taxi_Sample_DeeptteEncoder.json
2023-01-03 19:57:40,138 - INFO - longi_mean: 116.38167145561566
2023-01-03 19:57:40,138 - INFO - longi_std: 0.07416283805701834
2023-01-03 19:57:40,138 - INFO - lati_mean: 39.92553892677388
2023-01-03 19:57:40,138 - INFO - lati_std: 0.049981772271572
2023-01-03 19:57:40,138 - INFO - dist_mean: 8.814695989840317
2023-01-03 19:57:40,138 - INFO - dist_std: 7.073727325486173
2023-01-03 19:57:40,138 - INFO - time_mean: 1166.8956065318819
2023-01-03 19:57:40,138 - INFO - time_std: 973.9645727168944
2023-01-03 19:57:40,138 - INFO - dist_gap_mean: 0.3897806752549975
2023-01-03 19:57:40,138 - INFO - dist_gap_std: 0.22667483085183782
2023-01-03 19:57:40,138 - INFO - time_gap_mean: 51.59943780140807
2023-01-03 19:57:40,138 - INFO - time_gap_std: 44.83032982709822
2023-01-03 19:57:40,181 - INFO - Number of train data: 15432
2023-01-03 19:57:40,182 - INFO - Number of eval  data: 2200
2023-01-03 19:57:40,182 - INFO - Number of test  data: 4368
2023-01-03 19:57:43,121 - INFO - DeepTTE(
  (attr_net): Attr(
    (uid_em): Embedding(4968, 16)
    (weekid_em): Embedding(7, 3)
    (timeid_em): Embedding(1440, 8)
  )
  (spatio_temporal): SpatioTemporal(
    (geo_conv): GeoConv(
      (state_em): Embedding(2, 2)
      (process_coords): Linear(in_features=4, out_features=16, bias=True)
      (conv): Conv1d(16, 32, kernel_size=(3,), stride=(1,))
    )
    (rnn): LSTM(61, 128, batch_first=True)
    (attr2atten): Linear(in_features=28, out_features=128, bias=True)
  )
  (entire_estimate): EntireEstimator(
    (input2hid): Linear(in_features=156, out_features=128, bias=True)
    (residuals): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Linear(in_features=128, out_features=128, bias=True)
    )
    (hid2out): Linear(in_features=128, out_features=1, bias=True)
  )
  (local_estimate): LocalEstimator(
    (input2hid): Linear(in_features=128, out_features=64, bias=True)
    (hid2hid): Linear(in_features=64, out_features=32, bias=True)
    (hid2out): Linear(in_features=32, out_features=1, bias=True)
  )
)
2023-01-03 19:57:43,122 - INFO - attr_net.uid_em.weight	torch.Size([4968, 16])	cuda:0	True
2023-01-03 19:57:43,122 - INFO - attr_net.weekid_em.weight	torch.Size([7, 3])	cuda:0	True
2023-01-03 19:57:43,122 - INFO - attr_net.timeid_em.weight	torch.Size([1440, 8])	cuda:0	True
2023-01-03 19:57:43,122 - INFO - spatio_temporal.geo_conv.state_em.weight	torch.Size([2, 2])	cuda:0	True
2023-01-03 19:57:43,122 - INFO - spatio_temporal.geo_conv.process_coords.weight	torch.Size([16, 4])	cuda:0	True
2023-01-03 19:57:43,122 - INFO - spatio_temporal.geo_conv.process_coords.bias	torch.Size([16])	cuda:0	True
2023-01-03 19:57:43,122 - INFO - spatio_temporal.geo_conv.conv.weight	torch.Size([32, 16, 3])	cuda:0	True
2023-01-03 19:57:43,122 - INFO - spatio_temporal.geo_conv.conv.bias	torch.Size([32])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - spatio_temporal.rnn.weight_ih_l0	torch.Size([512, 61])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - spatio_temporal.rnn.weight_hh_l0	torch.Size([512, 128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - spatio_temporal.rnn.bias_ih_l0	torch.Size([512])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - spatio_temporal.rnn.bias_hh_l0	torch.Size([512])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - spatio_temporal.attr2atten.weight	torch.Size([128, 28])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - spatio_temporal.attr2atten.bias	torch.Size([128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.input2hid.weight	torch.Size([128, 156])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.input2hid.bias	torch.Size([128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.residuals.0.weight	torch.Size([128, 128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.residuals.0.bias	torch.Size([128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.residuals.1.weight	torch.Size([128, 128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.residuals.1.bias	torch.Size([128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.residuals.2.weight	torch.Size([128, 128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.residuals.2.bias	torch.Size([128])	cuda:0	True
2023-01-03 19:57:43,123 - INFO - entire_estimate.residuals.3.weight	torch.Size([128, 128])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - entire_estimate.residuals.3.bias	torch.Size([128])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - entire_estimate.hid2out.weight	torch.Size([1, 128])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - entire_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - local_estimate.input2hid.weight	torch.Size([64, 128])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - local_estimate.input2hid.bias	torch.Size([64])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - local_estimate.hid2hid.weight	torch.Size([32, 64])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - local_estimate.hid2hid.bias	torch.Size([32])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - local_estimate.hid2out.weight	torch.Size([1, 32])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - local_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-03 19:57:43,124 - INFO - Total parameter numbers: 290827
2023-01-03 19:57:43,125 - INFO - You select `adam` optimizer.
2023-01-03 19:57:43,125 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-01-03 19:57:43,125 - INFO - Start training ...
2023-01-03 19:57:43,125 - INFO - num_batches:242
2023-01-03 19:57:52,423 - INFO - epoch complete!
2023-01-03 19:57:52,423 - INFO - evaluating now!
2023-01-03 19:57:53,169 - INFO - Epoch [0/100] train_loss: 0.4573, val_loss: 0.2972, lr: 0.001000, 10.04s
2023-01-03 19:57:53,186 - INFO - Saved model at 0
2023-01-03 19:57:53,186 - INFO - Val loss decrease from inf to 0.2972, saving to ./libcity/cache/63092/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch0.tar
2023-01-03 19:58:02,167 - INFO - epoch complete!
2023-01-03 19:58:02,168 - INFO - evaluating now!
2023-01-03 19:58:02,834 - INFO - Epoch [1/100] train_loss: 0.2870, val_loss: 0.2561, lr: 0.001000, 9.65s
2023-01-03 19:58:02,847 - INFO - Saved model at 1
2023-01-03 19:58:02,847 - INFO - Val loss decrease from 0.2972 to 0.2561, saving to ./libcity/cache/63092/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch1.tar
2023-01-03 19:58:12,181 - INFO - epoch complete!
2023-01-03 19:58:12,182 - INFO - evaluating now!
2023-01-03 19:58:12,708 - INFO - Epoch [2/100] train_loss: 0.2787, val_loss: 0.2282, lr: 0.001000, 9.86s
2023-01-03 19:58:12,720 - INFO - Saved model at 2
2023-01-03 19:58:12,721 - INFO - Val loss decrease from 0.2561 to 0.2282, saving to ./libcity/cache/63092/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch2.tar
2023-01-03 19:58:21,680 - INFO - epoch complete!
2023-01-03 19:58:21,681 - INFO - evaluating now!
2023-01-03 19:58:22,171 - INFO - Epoch [3/100] train_loss: 0.2472, val_loss: 0.2497, lr: 0.001000, 9.45s
2023-01-03 19:58:30,991 - INFO - epoch complete!
2023-01-03 19:58:30,992 - INFO - evaluating now!
2023-01-03 19:58:31,538 - INFO - Epoch [4/100] train_loss: 0.2216, val_loss: 0.2367, lr: 0.001000, 9.37s
2023-01-03 19:58:40,700 - INFO - epoch complete!
2023-01-03 19:58:40,701 - INFO - evaluating now!
2023-01-03 19:58:41,236 - INFO - Epoch [5/100] train_loss: 0.2210, val_loss: 0.2330, lr: 0.001000, 9.70s
2023-01-03 19:58:50,526 - INFO - epoch complete!
2023-01-03 19:58:50,527 - INFO - evaluating now!
2023-01-03 19:58:51,141 - INFO - Epoch [6/100] train_loss: 0.2158, val_loss: 0.2314, lr: 0.001000, 9.90s
2023-01-03 19:59:00,657 - INFO - epoch complete!
2023-01-03 19:59:00,659 - INFO - evaluating now!
2023-01-03 19:59:01,232 - INFO - Epoch [7/100] train_loss: 0.2223, val_loss: 0.2122, lr: 0.001000, 10.09s
2023-01-03 19:59:01,243 - INFO - Saved model at 7
2023-01-03 19:59:01,244 - INFO - Val loss decrease from 0.2282 to 0.2122, saving to ./libcity/cache/63092/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch7.tar
2023-01-03 19:59:10,336 - INFO - epoch complete!
2023-01-03 19:59:10,336 - INFO - evaluating now!
2023-01-03 19:59:10,936 - INFO - Epoch [8/100] train_loss: 0.2133, val_loss: 0.2177, lr: 0.001000, 9.69s
2023-01-03 19:59:19,629 - INFO - epoch complete!
2023-01-03 19:59:19,630 - INFO - evaluating now!
2023-01-03 19:59:20,181 - INFO - Epoch [9/100] train_loss: 0.2102, val_loss: 0.2131, lr: 0.001000, 9.24s
2023-01-03 19:59:29,137 - INFO - epoch complete!
2023-01-03 19:59:29,137 - INFO - evaluating now!
2023-01-03 19:59:29,652 - INFO - Epoch [10/100] train_loss: 0.2002, val_loss: 0.2073, lr: 0.001000, 9.47s
2023-01-03 19:59:29,663 - INFO - Saved model at 10
2023-01-03 19:59:29,664 - INFO - Val loss decrease from 0.2122 to 0.2073, saving to ./libcity/cache/63092/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch10.tar
2023-01-03 19:59:38,691 - INFO - epoch complete!
2023-01-03 19:59:38,692 - INFO - evaluating now!
2023-01-03 19:59:39,216 - INFO - Epoch [11/100] train_loss: 0.2076, val_loss: 0.2115, lr: 0.001000, 9.55s
2023-01-03 19:59:48,505 - INFO - epoch complete!
2023-01-03 19:59:48,505 - INFO - evaluating now!
2023-01-03 19:59:49,072 - INFO - Epoch [12/100] train_loss: 0.2053, val_loss: 0.2223, lr: 0.001000, 9.86s
2023-01-03 19:59:58,432 - INFO - epoch complete!
2023-01-03 19:59:58,433 - INFO - evaluating now!
2023-01-03 19:59:58,997 - INFO - Epoch [13/100] train_loss: 0.1960, val_loss: 0.2008, lr: 0.001000, 9.92s
2023-01-03 19:59:59,012 - INFO - Saved model at 13
2023-01-03 19:59:59,012 - INFO - Val loss decrease from 0.2073 to 0.2008, saving to ./libcity/cache/63092/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch13.tar
2023-01-03 20:00:07,914 - INFO - epoch complete!
2023-01-03 20:00:07,915 - INFO - evaluating now!
2023-01-03 20:00:08,511 - INFO - Epoch [14/100] train_loss: 0.1928, val_loss: 0.2008, lr: 0.001000, 9.50s
2023-01-03 20:00:17,836 - INFO - epoch complete!
2023-01-03 20:00:17,837 - INFO - evaluating now!
2023-01-03 20:00:18,442 - INFO - Epoch [15/100] train_loss: 0.1965, val_loss: 0.2067, lr: 0.001000, 9.93s
2023-01-03 20:00:26,849 - INFO - epoch complete!
2023-01-03 20:00:26,850 - INFO - evaluating now!
2023-01-03 20:00:27,411 - INFO - Epoch [16/100] train_loss: 0.1952, val_loss: 0.2098, lr: 0.001000, 8.97s
2023-01-03 20:00:36,098 - INFO - epoch complete!
2023-01-03 20:00:36,099 - INFO - evaluating now!
2023-01-03 20:00:36,751 - INFO - Epoch [17/100] train_loss: 0.1944, val_loss: 0.2034, lr: 0.001000, 9.34s
2023-01-03 20:00:45,847 - INFO - epoch complete!
2023-01-03 20:00:45,848 - INFO - evaluating now!
2023-01-03 20:00:46,412 - INFO - Epoch [18/100] train_loss: 0.1906, val_loss: 0.1994, lr: 0.001000, 9.66s
2023-01-03 20:00:46,423 - INFO - Saved model at 18
2023-01-03 20:00:46,424 - INFO - Val loss decrease from 0.2008 to 0.1994, saving to ./libcity/cache/63092/model_cache/DeepTTE_Beijing_Taxi_Sample_epoch18.tar
2023-01-03 20:00:55,677 - INFO - epoch complete!
2023-01-03 20:00:55,678 - INFO - evaluating now!
2023-01-03 20:00:56,169 - INFO - Epoch [19/100] train_loss: 0.1912, val_loss: 0.2030, lr: 0.001000, 9.75s
2023-01-03 20:01:05,187 - INFO - epoch complete!
2023-01-03 20:01:05,188 - INFO - evaluating now!
2023-01-03 20:01:05,747 - INFO - Epoch [20/100] train_loss: 0.1883, val_loss: 0.2056, lr: 0.001000, 9.58s
2023-01-03 20:01:14,931 - INFO - epoch complete!
2023-01-03 20:01:14,932 - INFO - evaluating now!
2023-01-03 20:01:15,551 - INFO - Epoch [21/100] train_loss: 0.1914, val_loss: 0.2080, lr: 0.001000, 9.80s
2023-01-03 20:01:24,618 - INFO - epoch complete!
2023-01-03 20:01:24,618 - INFO - evaluating now!
2023-01-03 20:01:25,281 - INFO - Epoch [22/100] train_loss: 0.1880, val_loss: 0.2090, lr: 0.001000, 9.73s
2023-01-03 20:01:34,521 - INFO - epoch complete!
2023-01-03 20:01:34,522 - INFO - evaluating now!
2023-01-03 20:01:35,066 - INFO - Epoch [23/100] train_loss: 0.1872, val_loss: 0.2053, lr: 0.001000, 9.78s
2023-01-03 20:01:44,108 - INFO - epoch complete!
2023-01-03 20:01:44,108 - INFO - evaluating now!
2023-01-03 20:01:44,689 - INFO - Epoch [24/100] train_loss: 0.1864, val_loss: 0.2090, lr: 0.001000, 9.62s
2023-01-03 20:01:53,584 - INFO - epoch complete!
2023-01-03 20:01:53,585 - INFO - evaluating now!
2023-01-03 20:01:54,098 - INFO - Epoch [25/100] train_loss: 0.1915, val_loss: 0.2508, lr: 0.001000, 9.41s
2023-01-03 20:02:03,332 - INFO - epoch complete!
2023-01-03 20:02:03,332 - INFO - evaluating now!
2023-01-03 20:02:03,817 - INFO - Epoch [26/100] train_loss: 0.1916, val_loss: 0.2327, lr: 0.001000, 9.72s
2023-01-03 20:02:12,805 - INFO - epoch complete!
2023-01-03 20:02:12,806 - INFO - evaluating now!
2023-01-03 20:02:13,428 - INFO - Epoch [27/100] train_loss: 0.1891, val_loss: 0.2270, lr: 0.001000, 9.61s
2023-01-03 20:02:22,509 - INFO - epoch complete!
2023-01-03 20:02:22,510 - INFO - evaluating now!
2023-01-03 20:02:23,097 - INFO - Epoch [28/100] train_loss: 0.1859, val_loss: 0.2195, lr: 0.001000, 9.67s
2023-01-03 20:02:31,805 - INFO - epoch complete!
2023-01-03 20:02:31,806 - INFO - evaluating now!
2023-01-03 20:02:32,322 - INFO - Epoch [29/100] train_loss: 0.1839, val_loss: 0.2237, lr: 0.001000, 9.22s
2023-01-03 20:02:41,344 - INFO - epoch complete!
2023-01-03 20:02:41,344 - INFO - evaluating now!
2023-01-03 20:02:41,931 - INFO - Epoch [30/100] train_loss: 0.1847, val_loss: 0.2173, lr: 0.001000, 9.61s
2023-01-03 20:02:50,397 - INFO - epoch complete!
2023-01-03 20:02:50,398 - INFO - evaluating now!
2023-01-03 20:02:50,945 - INFO - Epoch [31/100] train_loss: 0.1831, val_loss: 0.2125, lr: 0.001000, 9.01s
2023-01-03 20:02:59,774 - INFO - epoch complete!
2023-01-03 20:02:59,775 - INFO - evaluating now!
2023-01-03 20:03:00,334 - INFO - Epoch [32/100] train_loss: 0.1813, val_loss: 0.2115, lr: 0.001000, 9.39s
2023-01-03 20:03:09,102 - INFO - epoch complete!
2023-01-03 20:03:09,102 - INFO - evaluating now!
2023-01-03 20:03:09,740 - INFO - Epoch [33/100] train_loss: 0.1803, val_loss: 0.2205, lr: 0.001000, 9.41s
2023-01-03 20:03:18,064 - INFO - epoch complete!
2023-01-03 20:03:18,065 - INFO - evaluating now!
2023-01-03 20:03:18,673 - INFO - Epoch [34/100] train_loss: 0.1806, val_loss: 0.2155, lr: 0.001000, 8.93s
2023-01-03 20:03:27,371 - INFO - epoch complete!
2023-01-03 20:03:27,371 - INFO - evaluating now!
2023-01-03 20:03:27,870 - INFO - Epoch [35/100] train_loss: 0.1814, val_loss: 0.2196, lr: 0.001000, 9.20s
2023-01-03 20:03:36,646 - INFO - epoch complete!
2023-01-03 20:03:36,647 - INFO - evaluating now!
2023-01-03 20:03:37,201 - INFO - Epoch [36/100] train_loss: 0.1778, val_loss: 0.2156, lr: 0.001000, 9.33s
2023-01-03 20:03:45,916 - INFO - epoch complete!
2023-01-03 20:03:45,917 - INFO - evaluating now!
2023-01-03 20:03:46,429 - INFO - Epoch [37/100] train_loss: 0.1767, val_loss: 0.2186, lr: 0.001000, 9.23s
2023-01-03 20:03:55,398 - INFO - epoch complete!
2023-01-03 20:03:55,398 - INFO - evaluating now!
2023-01-03 20:03:55,886 - INFO - Epoch [38/100] train_loss: 0.1765, val_loss: 0.2066, lr: 0.001000, 9.46s
2023-01-03 20:04:04,913 - INFO - epoch complete!
2023-01-03 20:04:04,913 - INFO - evaluating now!
2023-01-03 20:04:05,426 - INFO - Epoch [39/100] train_loss: 0.1767, val_loss: 0.2104, lr: 0.001000, 9.54s
2023-01-03 20:04:14,120 - INFO - epoch complete!
2023-01-03 20:04:14,120 - INFO - evaluating now!
2023-01-03 20:04:14,736 - INFO - Epoch [40/100] train_loss: 0.1750, val_loss: 0.2211, lr: 0.001000, 9.31s
2023-01-03 20:04:23,813 - INFO - epoch complete!
2023-01-03 20:04:23,814 - INFO - evaluating now!
2023-01-03 20:04:24,301 - INFO - Epoch [41/100] train_loss: 0.1775, val_loss: 0.2154, lr: 0.001000, 9.57s
2023-01-03 20:04:32,746 - INFO - epoch complete!
2023-01-03 20:04:32,747 - INFO - evaluating now!
2023-01-03 20:04:33,393 - INFO - Epoch [42/100] train_loss: 0.1811, val_loss: 0.2391, lr: 0.001000, 9.09s
2023-01-03 20:04:42,052 - INFO - epoch complete!
2023-01-03 20:04:42,053 - INFO - evaluating now!
2023-01-03 20:04:42,625 - INFO - Epoch [43/100] train_loss: 0.1825, val_loss: 0.2092, lr: 0.001000, 9.23s
2023-01-03 20:04:51,383 - INFO - epoch complete!
2023-01-03 20:04:51,384 - INFO - evaluating now!
2023-01-03 20:04:51,969 - INFO - Epoch [44/100] train_loss: 0.1790, val_loss: 0.2073, lr: 0.001000, 9.34s
2023-01-03 20:05:00,692 - INFO - epoch complete!
2023-01-03 20:05:00,692 - INFO - evaluating now!
2023-01-03 20:05:01,343 - INFO - Epoch [45/100] train_loss: 0.1748, val_loss: 0.2019, lr: 0.001000, 9.37s
2023-01-03 20:05:09,909 - INFO - epoch complete!
2023-01-03 20:05:09,910 - INFO - evaluating now!
2023-01-03 20:05:10,512 - INFO - Epoch [46/100] train_loss: 0.1719, val_loss: 0.2069, lr: 0.001000, 9.17s
2023-01-03 20:05:19,660 - INFO - epoch complete!
2023-01-03 20:05:19,661 - INFO - evaluating now!
2023-01-03 20:05:20,221 - INFO - Epoch [47/100] train_loss: 0.1690, val_loss: 0.2117, lr: 0.001000, 9.71s
2023-01-03 20:05:29,075 - INFO - epoch complete!
2023-01-03 20:05:29,076 - INFO - evaluating now!
2023-01-03 20:05:29,693 - INFO - Epoch [48/100] train_loss: 0.1691, val_loss: 0.2116, lr: 0.001000, 9.47s
2023-01-03 20:05:38,533 - INFO - epoch complete!
2023-01-03 20:05:38,534 - INFO - evaluating now!
2023-01-03 20:05:39,121 - INFO - Epoch [49/100] train_loss: 0.1646, val_loss: 0.2077, lr: 0.001000, 9.43s
2023-01-03 20:05:48,070 - INFO - epoch complete!
2023-01-03 20:05:48,071 - INFO - evaluating now!
2023-01-03 20:05:48,634 - INFO - Epoch [50/100] train_loss: 0.1636, val_loss: 0.2084, lr: 0.001000, 9.51s
2023-01-03 20:05:57,525 - INFO - epoch complete!
2023-01-03 20:05:57,526 - INFO - evaluating now!
2023-01-03 20:05:58,074 - INFO - Epoch [51/100] train_loss: 0.1621, val_loss: 0.2055, lr: 0.001000, 9.44s
2023-01-03 20:06:07,151 - INFO - epoch complete!
2023-01-03 20:06:07,152 - INFO - evaluating now!
2023-01-03 20:06:07,774 - INFO - Epoch [52/100] train_loss: 0.1590, val_loss: 0.2080, lr: 0.001000, 9.70s
2023-01-03 20:06:16,507 - INFO - epoch complete!
2023-01-03 20:06:16,508 - INFO - evaluating now!
2023-01-03 20:06:17,057 - INFO - Epoch [53/100] train_loss: 0.1593, val_loss: 0.2163, lr: 0.001000, 9.28s
2023-01-03 20:06:25,735 - INFO - epoch complete!
2023-01-03 20:06:25,736 - INFO - evaluating now!
2023-01-03 20:06:26,297 - INFO - Epoch [54/100] train_loss: 0.1565, val_loss: 0.2124, lr: 0.001000, 9.24s
2023-01-03 20:06:35,154 - INFO - epoch complete!
2023-01-03 20:06:35,155 - INFO - evaluating now!
2023-01-03 20:06:35,665 - INFO - Epoch [55/100] train_loss: 0.1545, val_loss: 0.2197, lr: 0.001000, 9.37s
2023-01-03 20:06:44,653 - INFO - epoch complete!
2023-01-03 20:06:44,654 - INFO - evaluating now!
2023-01-03 20:06:45,170 - INFO - Epoch [56/100] train_loss: 0.1542, val_loss: 0.2133, lr: 0.001000, 9.50s
2023-01-03 20:06:54,535 - INFO - epoch complete!
2023-01-03 20:06:54,535 - INFO - evaluating now!
2023-01-03 20:06:55,122 - INFO - Epoch [57/100] train_loss: 0.1537, val_loss: 0.2148, lr: 0.001000, 9.95s
2023-01-03 20:07:03,926 - INFO - epoch complete!
2023-01-03 20:07:03,926 - INFO - evaluating now!
2023-01-03 20:07:04,546 - INFO - Epoch [58/100] train_loss: 0.1531, val_loss: 0.2466, lr: 0.001000, 9.42s
2023-01-03 20:07:13,272 - INFO - epoch complete!
2023-01-03 20:07:13,273 - INFO - evaluating now!
2023-01-03 20:07:13,830 - INFO - Epoch [59/100] train_loss: 0.1542, val_loss: 0.2507, lr: 0.001000, 9.28s
2023-01-03 20:07:22,585 - INFO - epoch complete!
2023-01-03 20:07:22,586 - INFO - evaluating now!
2023-01-03 20:07:23,096 - INFO - Epoch [60/100] train_loss: 0.1538, val_loss: 0.2121, lr: 0.001000, 9.27s
2023-01-03 20:07:32,003 - INFO - epoch complete!
2023-01-03 20:07:32,003 - INFO - evaluating now!
2023-01-03 20:07:32,585 - INFO - Epoch [61/100] train_loss: 0.1521, val_loss: 0.2176, lr: 0.001000, 9.49s
2023-01-03 20:07:41,482 - INFO - epoch complete!
2023-01-03 20:07:41,483 - INFO - evaluating now!
2023-01-03 20:07:41,998 - INFO - Epoch [62/100] train_loss: 0.1491, val_loss: 0.2159, lr: 0.001000, 9.41s
2023-01-03 20:07:50,904 - INFO - epoch complete!
2023-01-03 20:07:50,905 - INFO - evaluating now!
2023-01-03 20:07:51,402 - INFO - Epoch [63/100] train_loss: 0.1469, val_loss: 0.2212, lr: 0.001000, 9.40s
2023-01-03 20:08:00,462 - INFO - epoch complete!
2023-01-03 20:08:00,462 - INFO - evaluating now!
2023-01-03 20:08:00,965 - INFO - Epoch [64/100] train_loss: 0.1467, val_loss: 0.2229, lr: 0.001000, 9.56s
2023-01-03 20:08:09,748 - INFO - epoch complete!
2023-01-03 20:08:09,748 - INFO - evaluating now!
2023-01-03 20:08:10,337 - INFO - Epoch [65/100] train_loss: 0.1482, val_loss: 0.2302, lr: 0.001000, 9.37s
2023-01-03 20:08:18,867 - INFO - epoch complete!
2023-01-03 20:08:18,867 - INFO - evaluating now!
2023-01-03 20:08:19,407 - INFO - Epoch [66/100] train_loss: 0.1468, val_loss: 0.2274, lr: 0.001000, 9.07s
2023-01-03 20:08:28,470 - INFO - epoch complete!
2023-01-03 20:08:28,471 - INFO - evaluating now!
2023-01-03 20:08:28,954 - INFO - Epoch [67/100] train_loss: 0.1443, val_loss: 0.2409, lr: 0.001000, 9.55s
2023-01-03 20:08:37,655 - INFO - epoch complete!
2023-01-03 20:08:37,656 - INFO - evaluating now!
2023-01-03 20:08:38,229 - INFO - Epoch [68/100] train_loss: 0.1410, val_loss: 0.2251, lr: 0.001000, 9.27s
2023-01-03 20:08:47,209 - INFO - epoch complete!
2023-01-03 20:08:47,210 - INFO - evaluating now!
2023-01-03 20:08:47,750 - INFO - Epoch [69/100] train_loss: 0.1409, val_loss: 0.2338, lr: 0.001000, 9.52s
2023-01-03 20:08:56,791 - INFO - epoch complete!
2023-01-03 20:08:56,792 - INFO - evaluating now!
2023-01-03 20:08:57,283 - INFO - Epoch [70/100] train_loss: 0.1425, val_loss: 0.2304, lr: 0.001000, 9.53s
2023-01-03 20:09:05,961 - INFO - epoch complete!
2023-01-03 20:09:05,962 - INFO - evaluating now!
2023-01-03 20:09:06,515 - INFO - Epoch [71/100] train_loss: 0.1404, val_loss: 0.2365, lr: 0.001000, 9.23s
2023-01-03 20:09:15,319 - INFO - epoch complete!
2023-01-03 20:09:15,320 - INFO - evaluating now!
2023-01-03 20:09:15,893 - INFO - Epoch [72/100] train_loss: 0.1419, val_loss: 0.2277, lr: 0.001000, 9.38s
2023-01-03 20:09:24,692 - INFO - epoch complete!
2023-01-03 20:09:24,693 - INFO - evaluating now!
2023-01-03 20:09:25,264 - INFO - Epoch [73/100] train_loss: 0.1422, val_loss: 0.2448, lr: 0.001000, 9.37s
2023-01-03 20:09:34,184 - INFO - epoch complete!
2023-01-03 20:09:34,185 - INFO - evaluating now!
2023-01-03 20:09:34,751 - INFO - Epoch [74/100] train_loss: 0.1388, val_loss: 0.2439, lr: 0.001000, 9.49s
2023-01-03 20:09:43,622 - INFO - epoch complete!
2023-01-03 20:09:43,623 - INFO - evaluating now!
2023-01-03 20:09:44,191 - INFO - Epoch [75/100] train_loss: 0.1408, val_loss: 0.2336, lr: 0.001000, 9.44s
2023-01-03 20:09:52,901 - INFO - epoch complete!
2023-01-03 20:09:52,902 - INFO - evaluating now!
2023-01-03 20:09:53,382 - INFO - Epoch [76/100] train_loss: 0.1422, val_loss: 0.2232, lr: 0.001000, 9.19s
2023-01-03 20:10:02,197 - INFO - epoch complete!
2023-01-03 20:10:02,197 - INFO - evaluating now!
2023-01-03 20:10:02,772 - INFO - Epoch [77/100] train_loss: 0.1432, val_loss: 0.2323, lr: 0.001000, 9.39s
2023-01-03 20:10:11,660 - INFO - epoch complete!
2023-01-03 20:10:11,660 - INFO - evaluating now!
2023-01-03 20:10:12,226 - INFO - Epoch [78/100] train_loss: 0.1389, val_loss: 0.2230, lr: 0.001000, 9.45s
2023-01-03 20:10:20,816 - INFO - epoch complete!
2023-01-03 20:10:20,817 - INFO - evaluating now!
2023-01-03 20:10:21,559 - INFO - Epoch [79/100] train_loss: 0.1379, val_loss: 0.2237, lr: 0.001000, 9.33s
2023-01-03 20:10:30,243 - INFO - epoch complete!
2023-01-03 20:10:30,243 - INFO - evaluating now!
2023-01-03 20:10:30,823 - INFO - Epoch [80/100] train_loss: 0.1354, val_loss: 0.2347, lr: 0.001000, 9.26s
2023-01-03 20:10:39,561 - INFO - epoch complete!
2023-01-03 20:10:39,562 - INFO - evaluating now!
2023-01-03 20:10:40,214 - INFO - Epoch [81/100] train_loss: 0.1392, val_loss: 0.2367, lr: 0.001000, 9.39s
2023-01-03 20:10:48,886 - INFO - epoch complete!
2023-01-03 20:10:48,887 - INFO - evaluating now!
2023-01-03 20:10:49,397 - INFO - Epoch [82/100] train_loss: 0.1416, val_loss: 0.2414, lr: 0.001000, 9.18s
2023-01-03 20:10:58,440 - INFO - epoch complete!
2023-01-03 20:10:58,441 - INFO - evaluating now!
2023-01-03 20:10:59,026 - INFO - Epoch [83/100] train_loss: 0.1423, val_loss: 0.2447, lr: 0.001000, 9.63s
2023-01-03 20:11:07,642 - INFO - epoch complete!
2023-01-03 20:11:07,643 - INFO - evaluating now!
2023-01-03 20:11:08,197 - INFO - Epoch [84/100] train_loss: 0.1418, val_loss: 0.2455, lr: 0.001000, 9.17s
2023-01-03 20:11:16,997 - INFO - epoch complete!
2023-01-03 20:11:16,997 - INFO - evaluating now!
2023-01-03 20:11:17,534 - INFO - Epoch [85/100] train_loss: 0.1377, val_loss: 0.2309, lr: 0.001000, 9.34s
2023-01-03 20:11:26,444 - INFO - epoch complete!
2023-01-03 20:11:26,445 - INFO - evaluating now!
2023-01-03 20:11:27,029 - INFO - Epoch [86/100] train_loss: 0.1325, val_loss: 0.2227, lr: 0.001000, 9.49s
2023-01-03 20:11:35,728 - INFO - epoch complete!
2023-01-03 20:11:35,729 - INFO - evaluating now!
2023-01-03 20:11:36,358 - INFO - Epoch [87/100] train_loss: 0.1285, val_loss: 0.2194, lr: 0.001000, 9.33s
2023-01-03 20:11:45,318 - INFO - epoch complete!
2023-01-03 20:11:45,319 - INFO - evaluating now!
2023-01-03 20:11:45,958 - INFO - Epoch [88/100] train_loss: 0.1231, val_loss: 0.2218, lr: 0.001000, 9.60s
2023-01-03 20:11:54,927 - INFO - epoch complete!
2023-01-03 20:11:54,928 - INFO - evaluating now!
2023-01-03 20:11:55,453 - INFO - Epoch [89/100] train_loss: 0.1212, val_loss: 0.2205, lr: 0.001000, 9.49s
2023-01-03 20:12:04,400 - INFO - epoch complete!
2023-01-03 20:12:04,400 - INFO - evaluating now!
2023-01-03 20:12:04,995 - INFO - Epoch [90/100] train_loss: 0.1212, val_loss: 0.2259, lr: 0.001000, 9.54s
2023-01-03 20:12:14,061 - INFO - epoch complete!
2023-01-03 20:12:14,062 - INFO - evaluating now!
2023-01-03 20:12:14,638 - INFO - Epoch [91/100] train_loss: 0.1192, val_loss: 0.2187, lr: 0.001000, 9.64s
2023-01-03 20:12:23,620 - INFO - epoch complete!
2023-01-03 20:12:23,620 - INFO - evaluating now!
2023-01-03 20:12:24,223 - INFO - Epoch [92/100] train_loss: 0.1161, val_loss: 0.2216, lr: 0.001000, 9.58s
2023-01-03 20:12:33,015 - INFO - epoch complete!
2023-01-03 20:12:33,016 - INFO - evaluating now!
2023-01-03 20:12:33,618 - INFO - Epoch [93/100] train_loss: 0.1139, val_loss: 0.2208, lr: 0.001000, 9.39s
2023-01-03 20:12:42,375 - INFO - epoch complete!
2023-01-03 20:12:42,376 - INFO - evaluating now!
2023-01-03 20:12:43,010 - INFO - Epoch [94/100] train_loss: 0.1147, val_loss: 0.2202, lr: 0.001000, 9.39s
2023-01-03 20:12:51,977 - INFO - epoch complete!
2023-01-03 20:12:51,978 - INFO - evaluating now!
2023-01-03 20:12:52,553 - INFO - Epoch [95/100] train_loss: 0.1134, val_loss: 0.2217, lr: 0.001000, 9.54s
2023-01-03 20:13:01,144 - INFO - epoch complete!
2023-01-03 20:13:01,145 - INFO - evaluating now!
2023-01-03 20:13:01,695 - INFO - Epoch [96/100] train_loss: 0.1126, val_loss: 0.2325, lr: 0.001000, 9.14s
2023-01-03 20:13:10,617 - INFO - epoch complete!
2023-01-03 20:13:10,618 - INFO - evaluating now!
2023-01-03 20:13:11,169 - INFO - Epoch [97/100] train_loss: 0.1173, val_loss: 0.2297, lr: 0.001000, 9.47s
2023-01-03 20:13:19,791 - INFO - epoch complete!
2023-01-03 20:13:19,792 - INFO - evaluating now!
2023-01-03 20:13:20,445 - INFO - Epoch [98/100] train_loss: 0.1162, val_loss: 0.2257, lr: 0.001000, 9.28s
2023-01-03 20:13:29,278 - INFO - epoch complete!
2023-01-03 20:13:29,279 - INFO - evaluating now!
2023-01-03 20:13:29,978 - INFO - Epoch [99/100] train_loss: 0.1157, val_loss: 0.2267, lr: 0.001000, 9.53s
2023-01-03 20:13:29,978 - INFO - Trained totally 100 epochs, average train time is 8.895s, average eval time is 0.571s
2023-01-03 20:13:29,995 - INFO - Loaded model at 18
2023-01-03 20:13:29,995 - INFO - Saved model at ./libcity/cache/63092/model_cache/DeepTTE_Beijing_Taxi_Sample.m
2023-01-03 20:13:30,008 - INFO - Start evaluating ...
2023-01-03 20:13:32,570 - INFO - Evaluate result is saved at ./libcity/cache/63092/evaluate_cache/2023_01_03_20_13_32_DeepTTE_Beijing_Taxi_Sample.csv
2023-01-03 20:13:32,585 - INFO - 
          MAE      MAPE            MSE        RMSE  masked_MAE  masked_MAPE     masked_MSE  masked_RMSE      R2      EVAR
1  210.398178  0.199078  107169.226562  327.367126  210.398178     0.199078  107169.226562   327.367126  0.8722  0.877532

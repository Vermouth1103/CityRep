2023-01-06 15:06:12,047 - INFO - Log directory: ./libcity/log
2023-01-06 15:06:12,047 - INFO - Begin pipeline, task=eta, model_name=DeepTTE, dataset_name=Beijing_Taxi_Sample_new_longer30, exp_id=2
2023-01-06 15:06:12,048 - INFO - {'task': 'eta', 'model': 'DeepTTE', 'dataset': 'Beijing_Taxi_Sample_new_longer30', 'saved_model': True, 'train': True, 'exp_id': '2', 'seed': 0, 'batch_size': 64, 'dataset_class': 'ETADataset', 'eta_encoder': 'DeeptteEncoder', 'executor': 'ETAExecutor', 'evaluator': 'ETAEvaluator', 'uid_emb_size': 16, 'weekid_emb_size': 3, 'timdid_emb_size': 8, 'kernel_size': 3, 'num_filter': 32, 'pooling_method': 'attention', 'num_final_fcs': 4, 'final_fc_size': 128, 'alpha': 0.1, 'rnn_type': 'LSTM', 'rnn_num_layers': 1, 'hidden_size': 128, 'max_epoch': 100, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': False, 'use_early_stop': False, 'patience': 20, 'num_workers': 0, 'min_session_len': 5, 'max_session_len': 50, 'min_sessions': 0, 'window_size': 1, 'cut_method': 'time_interval', 'pad_with_last_sample': True, 'sort_by_traj_len': True, 'cache_dataset': True, 'train_rate': 0.7, 'eval_rate': 0.1, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'max_grad_norm': 1.0, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE', 'R2', 'EVAR'], 'mode': 'single', 'save_modes': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'coordinates': 'coordinate', 'embedding': 'other'}}, 'usr': {'properties': {}}, 'dyna': {'including_types': ['trajectory'], 'trajectory': {'entity_id': 'usr_id', 'traj_id': 'num', 'coordinates': 'coordinate', 'current_dis': 'num', 'speeds': 'other', 'speeds_relevant1': 'other', 'speeds_relevant2': 'other', 'speeds_long': 'other', 'grid_len': 'num', 'holiday': 'num'}}, 'geo_file': 'Beijing_Taxi_Sample_new_longer30', 'usr_file': 'Beijing_Taxi_Sample_new_longer30', 'dyna_file': 'Beijing_Taxi_Sample_new_longer30', 'device': device(type='cuda', index=0)}
2023-01-06 15:06:12,051 - INFO - Dataset created
2023-01-06 15:06:13,384 - INFO - Loaded file Beijing_Taxi_Sample_new_longer30.dyna, shape=(290813, 14)
2023-01-06 15:06:49,748 - INFO - Saved at ./libcity/cache/dataset_cache/eta_Beijing_Taxi_Sample_new_longer30_DeeptteEncoder.json
2023-01-06 15:06:49,914 - INFO - longi_mean: 116.38775224575281
2023-01-06 15:06:49,914 - INFO - longi_std: 0.07436581782419843
2023-01-06 15:06:49,914 - INFO - lati_mean: 39.92595912826014
2023-01-06 15:06:49,914 - INFO - lati_std: 0.051960612818695746
2023-01-06 15:06:49,914 - INFO - dist_mean: 46.394674660283926
2023-01-06 15:06:49,914 - INFO - dist_std: 24.736874488458618
2023-01-06 15:06:49,914 - INFO - time_mean: 2249.877973358706
2023-01-06 15:06:49,914 - INFO - time_std: 1145.3334062262293
2023-01-06 15:06:49,915 - INFO - dist_gap_mean: 0.9741931585426983
2023-01-06 15:06:49,915 - INFO - dist_gap_std: 4.724302966927774
2023-01-06 15:06:49,915 - INFO - time_gap_mean: 47.24283002847011
2023-01-06 15:06:49,915 - INFO - time_gap_std: 44.31597855637794
2023-01-06 15:06:49,929 - INFO - Number of train data: 4204
2023-01-06 15:06:49,930 - INFO - Number of eval  data: 587
2023-01-06 15:06:49,930 - INFO - Number of test  data: 1171
2023-01-06 15:06:52,656 - INFO - DeepTTE(
  (attr_net): Attr(
    (uid_em): Embedding(69, 16)
    (weekid_em): Embedding(7, 3)
    (timeid_em): Embedding(1440, 8)
  )
  (spatio_temporal): SpatioTemporal(
    (geo_conv): GeoConv(
      (state_em): Embedding(2, 2)
      (process_coords): Linear(in_features=4, out_features=16, bias=True)
      (conv): Conv1d(16, 32, kernel_size=(3,), stride=(1,))
    )
    (rnn): LSTM(61, 128, batch_first=True)
    (attr2atten): Linear(in_features=28, out_features=128, bias=True)
  )
  (entire_estimate): EntireEstimator(
    (input2hid): Linear(in_features=156, out_features=128, bias=True)
    (residuals): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Linear(in_features=128, out_features=128, bias=True)
    )
    (hid2out): Linear(in_features=128, out_features=1, bias=True)
  )
  (local_estimate): LocalEstimator(
    (input2hid): Linear(in_features=128, out_features=64, bias=True)
    (hid2hid): Linear(in_features=64, out_features=32, bias=True)
    (hid2out): Linear(in_features=32, out_features=1, bias=True)
  )
)
2023-01-06 15:06:52,656 - INFO - attr_net.uid_em.weight	torch.Size([69, 16])	cuda:0	True
2023-01-06 15:06:52,656 - INFO - attr_net.weekid_em.weight	torch.Size([7, 3])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - attr_net.timeid_em.weight	torch.Size([1440, 8])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.geo_conv.state_em.weight	torch.Size([2, 2])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.geo_conv.process_coords.weight	torch.Size([16, 4])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.geo_conv.process_coords.bias	torch.Size([16])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.geo_conv.conv.weight	torch.Size([32, 16, 3])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.geo_conv.conv.bias	torch.Size([32])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.rnn.weight_ih_l0	torch.Size([512, 61])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.rnn.weight_hh_l0	torch.Size([512, 128])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.rnn.bias_ih_l0	torch.Size([512])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.rnn.bias_hh_l0	torch.Size([512])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.attr2atten.weight	torch.Size([128, 28])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - spatio_temporal.attr2atten.bias	torch.Size([128])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - entire_estimate.input2hid.weight	torch.Size([128, 156])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - entire_estimate.input2hid.bias	torch.Size([128])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - entire_estimate.residuals.0.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 15:06:52,657 - INFO - entire_estimate.residuals.0.bias	torch.Size([128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - entire_estimate.residuals.1.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - entire_estimate.residuals.1.bias	torch.Size([128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - entire_estimate.residuals.2.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - entire_estimate.residuals.2.bias	torch.Size([128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - entire_estimate.residuals.3.weight	torch.Size([128, 128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - entire_estimate.residuals.3.bias	torch.Size([128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - entire_estimate.hid2out.weight	torch.Size([1, 128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - entire_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - local_estimate.input2hid.weight	torch.Size([64, 128])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - local_estimate.input2hid.bias	torch.Size([64])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - local_estimate.hid2hid.weight	torch.Size([32, 64])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - local_estimate.hid2hid.bias	torch.Size([32])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - local_estimate.hid2out.weight	torch.Size([1, 32])	cuda:0	True
2023-01-06 15:06:52,658 - INFO - local_estimate.hid2out.bias	torch.Size([1])	cuda:0	True
2023-01-06 15:06:52,659 - INFO - Total parameter numbers: 212443
2023-01-06 15:06:52,659 - INFO - You select `adam` optimizer.
2023-01-06 15:06:52,659 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-01-06 15:06:52,659 - INFO - Start training ...
2023-01-06 15:06:52,659 - INFO - num_batches:66
2023-01-06 15:06:55,567 - INFO - epoch complete!
2023-01-06 15:06:55,568 - INFO - evaluating now!
2023-01-06 15:06:55,757 - INFO - Epoch [0/100] train_loss: 0.5084, val_loss: 0.3693, lr: 0.001000, 3.10s
2023-01-06 15:06:55,766 - INFO - Saved model at 0
2023-01-06 15:06:55,766 - INFO - Val loss decrease from inf to 0.3693, saving to ./libcity/cache/2/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch0.tar
2023-01-06 15:06:58,639 - INFO - epoch complete!
2023-01-06 15:06:58,640 - INFO - evaluating now!
2023-01-06 15:06:58,825 - INFO - Epoch [1/100] train_loss: 0.3273, val_loss: 0.3013, lr: 0.001000, 3.06s
2023-01-06 15:06:58,834 - INFO - Saved model at 1
2023-01-06 15:06:58,834 - INFO - Val loss decrease from 0.3693 to 0.3013, saving to ./libcity/cache/2/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch1.tar
2023-01-06 15:07:01,711 - INFO - epoch complete!
2023-01-06 15:07:01,711 - INFO - evaluating now!
2023-01-06 15:07:01,898 - INFO - Epoch [2/100] train_loss: 0.3372, val_loss: 0.2352, lr: 0.001000, 3.06s
2023-01-06 15:07:01,906 - INFO - Saved model at 2
2023-01-06 15:07:01,907 - INFO - Val loss decrease from 0.3013 to 0.2352, saving to ./libcity/cache/2/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch2.tar
2023-01-06 15:07:04,832 - INFO - epoch complete!
2023-01-06 15:07:04,832 - INFO - evaluating now!
2023-01-06 15:07:05,016 - INFO - Epoch [3/100] train_loss: 0.2929, val_loss: 0.2757, lr: 0.001000, 3.11s
2023-01-06 15:07:07,933 - INFO - epoch complete!
2023-01-06 15:07:07,934 - INFO - evaluating now!
2023-01-06 15:07:08,115 - INFO - Epoch [4/100] train_loss: 0.2663, val_loss: 0.2282, lr: 0.001000, 3.10s
2023-01-06 15:07:08,123 - INFO - Saved model at 4
2023-01-06 15:07:08,124 - INFO - Val loss decrease from 0.2352 to 0.2282, saving to ./libcity/cache/2/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch4.tar
2023-01-06 15:07:11,021 - INFO - epoch complete!
2023-01-06 15:07:11,021 - INFO - evaluating now!
2023-01-06 15:07:11,205 - INFO - Epoch [5/100] train_loss: 0.2732, val_loss: 0.2311, lr: 0.001000, 3.08s
2023-01-06 15:07:14,081 - INFO - epoch complete!
2023-01-06 15:07:14,082 - INFO - evaluating now!
2023-01-06 15:07:14,270 - INFO - Epoch [6/100] train_loss: 0.2432, val_loss: 0.2084, lr: 0.001000, 3.06s
2023-01-06 15:07:14,279 - INFO - Saved model at 6
2023-01-06 15:07:14,279 - INFO - Val loss decrease from 0.2282 to 0.2084, saving to ./libcity/cache/2/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch6.tar
2023-01-06 15:07:17,176 - INFO - epoch complete!
2023-01-06 15:07:17,177 - INFO - evaluating now!
2023-01-06 15:07:17,357 - INFO - Epoch [7/100] train_loss: 0.2331, val_loss: 0.2108, lr: 0.001000, 3.08s
2023-01-06 15:07:20,594 - INFO - epoch complete!
2023-01-06 15:07:20,594 - INFO - evaluating now!
2023-01-06 15:07:20,778 - INFO - Epoch [8/100] train_loss: 0.2143, val_loss: 0.2199, lr: 0.001000, 3.42s
2023-01-06 15:07:23,644 - INFO - epoch complete!
2023-01-06 15:07:23,644 - INFO - evaluating now!
2023-01-06 15:07:23,828 - INFO - Epoch [9/100] train_loss: 0.2176, val_loss: 0.2056, lr: 0.001000, 3.05s
2023-01-06 15:07:23,837 - INFO - Saved model at 9
2023-01-06 15:07:23,837 - INFO - Val loss decrease from 0.2084 to 0.2056, saving to ./libcity/cache/2/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch9.tar
2023-01-06 15:07:26,724 - INFO - epoch complete!
2023-01-06 15:07:26,724 - INFO - evaluating now!
2023-01-06 15:07:26,908 - INFO - Epoch [10/100] train_loss: 0.2180, val_loss: 0.2062, lr: 0.001000, 3.07s
2023-01-06 15:07:29,807 - INFO - epoch complete!
2023-01-06 15:07:29,807 - INFO - evaluating now!
2023-01-06 15:07:29,988 - INFO - Epoch [11/100] train_loss: 0.2282, val_loss: 0.2080, lr: 0.001000, 3.08s
2023-01-06 15:07:32,868 - INFO - epoch complete!
2023-01-06 15:07:32,869 - INFO - evaluating now!
2023-01-06 15:07:33,056 - INFO - Epoch [12/100] train_loss: 0.2108, val_loss: 0.2158, lr: 0.001000, 3.07s
2023-01-06 15:07:35,961 - INFO - epoch complete!
2023-01-06 15:07:35,962 - INFO - evaluating now!
2023-01-06 15:07:36,146 - INFO - Epoch [13/100] train_loss: 0.2270, val_loss: 0.2303, lr: 0.001000, 3.09s
2023-01-06 15:07:39,010 - INFO - epoch complete!
2023-01-06 15:07:39,011 - INFO - evaluating now!
2023-01-06 15:07:39,196 - INFO - Epoch [14/100] train_loss: 0.2185, val_loss: 0.2028, lr: 0.001000, 3.05s
2023-01-06 15:07:39,205 - INFO - Saved model at 14
2023-01-06 15:07:39,205 - INFO - Val loss decrease from 0.2056 to 0.2028, saving to ./libcity/cache/2/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30_epoch14.tar
2023-01-06 15:07:42,108 - INFO - epoch complete!
2023-01-06 15:07:42,109 - INFO - evaluating now!
2023-01-06 15:07:42,293 - INFO - Epoch [15/100] train_loss: 0.1994, val_loss: 0.2060, lr: 0.001000, 3.09s
2023-01-06 15:07:45,142 - INFO - epoch complete!
2023-01-06 15:07:45,143 - INFO - evaluating now!
2023-01-06 15:07:45,328 - INFO - Epoch [16/100] train_loss: 0.2009, val_loss: 0.2057, lr: 0.001000, 3.03s
2023-01-06 15:07:48,191 - INFO - epoch complete!
2023-01-06 15:07:48,192 - INFO - evaluating now!
2023-01-06 15:07:48,377 - INFO - Epoch [17/100] train_loss: 0.2007, val_loss: 0.2104, lr: 0.001000, 3.05s
2023-01-06 15:07:51,270 - INFO - epoch complete!
2023-01-06 15:07:51,271 - INFO - evaluating now!
2023-01-06 15:07:51,455 - INFO - Epoch [18/100] train_loss: 0.1904, val_loss: 0.2113, lr: 0.001000, 3.08s
2023-01-06 15:07:54,364 - INFO - epoch complete!
2023-01-06 15:07:54,365 - INFO - evaluating now!
2023-01-06 15:07:54,548 - INFO - Epoch [19/100] train_loss: 0.1945, val_loss: 0.2202, lr: 0.001000, 3.09s
2023-01-06 15:07:57,448 - INFO - epoch complete!
2023-01-06 15:07:57,449 - INFO - evaluating now!
2023-01-06 15:07:57,632 - INFO - Epoch [20/100] train_loss: 0.1914, val_loss: 0.2412, lr: 0.001000, 3.08s
2023-01-06 15:08:00,668 - INFO - epoch complete!
2023-01-06 15:08:00,668 - INFO - evaluating now!
2023-01-06 15:08:00,852 - INFO - Epoch [21/100] train_loss: 0.1931, val_loss: 0.2910, lr: 0.001000, 3.22s
2023-01-06 15:08:03,742 - INFO - epoch complete!
2023-01-06 15:08:03,742 - INFO - evaluating now!
2023-01-06 15:08:03,925 - INFO - Epoch [22/100] train_loss: 0.2138, val_loss: 0.2832, lr: 0.001000, 3.07s
2023-01-06 15:08:06,822 - INFO - epoch complete!
2023-01-06 15:08:06,822 - INFO - evaluating now!
2023-01-06 15:08:07,006 - INFO - Epoch [23/100] train_loss: 0.1965, val_loss: 0.2210, lr: 0.001000, 3.08s
2023-01-06 15:08:09,909 - INFO - epoch complete!
2023-01-06 15:08:09,909 - INFO - evaluating now!
2023-01-06 15:08:10,091 - INFO - Epoch [24/100] train_loss: 0.1851, val_loss: 0.2398, lr: 0.001000, 3.08s
2023-01-06 15:08:12,982 - INFO - epoch complete!
2023-01-06 15:08:12,982 - INFO - evaluating now!
2023-01-06 15:08:13,166 - INFO - Epoch [25/100] train_loss: 0.1881, val_loss: 0.2356, lr: 0.001000, 3.07s
2023-01-06 15:08:16,034 - INFO - epoch complete!
2023-01-06 15:08:16,035 - INFO - evaluating now!
2023-01-06 15:08:16,220 - INFO - Epoch [26/100] train_loss: 0.1919, val_loss: 0.2232, lr: 0.001000, 3.05s
2023-01-06 15:08:19,073 - INFO - epoch complete!
2023-01-06 15:08:19,074 - INFO - evaluating now!
2023-01-06 15:08:19,257 - INFO - Epoch [27/100] train_loss: 0.1956, val_loss: 0.2055, lr: 0.001000, 3.04s
2023-01-06 15:08:22,133 - INFO - epoch complete!
2023-01-06 15:08:22,133 - INFO - evaluating now!
2023-01-06 15:08:22,316 - INFO - Epoch [28/100] train_loss: 0.2053, val_loss: 0.2097, lr: 0.001000, 3.06s
2023-01-06 15:08:25,223 - INFO - epoch complete!
2023-01-06 15:08:25,223 - INFO - evaluating now!
2023-01-06 15:08:25,405 - INFO - Epoch [29/100] train_loss: 0.1844, val_loss: 0.2309, lr: 0.001000, 3.09s
2023-01-06 15:08:28,267 - INFO - epoch complete!
2023-01-06 15:08:28,268 - INFO - evaluating now!
2023-01-06 15:08:28,451 - INFO - Epoch [30/100] train_loss: 0.1779, val_loss: 0.2660, lr: 0.001000, 3.05s
2023-01-06 15:08:31,335 - INFO - epoch complete!
2023-01-06 15:08:31,335 - INFO - evaluating now!
2023-01-06 15:08:31,518 - INFO - Epoch [31/100] train_loss: 0.1783, val_loss: 0.2694, lr: 0.001000, 3.07s
2023-01-06 15:08:34,428 - INFO - epoch complete!
2023-01-06 15:08:34,429 - INFO - evaluating now!
2023-01-06 15:08:34,616 - INFO - Epoch [32/100] train_loss: 0.1777, val_loss: 0.2584, lr: 0.001000, 3.10s
2023-01-06 15:08:37,635 - INFO - epoch complete!
2023-01-06 15:08:37,635 - INFO - evaluating now!
2023-01-06 15:08:37,818 - INFO - Epoch [33/100] train_loss: 0.1786, val_loss: 0.2906, lr: 0.001000, 3.20s
2023-01-06 15:08:40,716 - INFO - epoch complete!
2023-01-06 15:08:40,716 - INFO - evaluating now!
2023-01-06 15:08:40,897 - INFO - Epoch [34/100] train_loss: 0.1862, val_loss: 0.2541, lr: 0.001000, 3.08s
2023-01-06 15:08:43,766 - INFO - epoch complete!
2023-01-06 15:08:43,767 - INFO - evaluating now!
2023-01-06 15:08:43,952 - INFO - Epoch [35/100] train_loss: 0.1852, val_loss: 0.2238, lr: 0.001000, 3.05s
2023-01-06 15:08:46,835 - INFO - epoch complete!
2023-01-06 15:08:46,835 - INFO - evaluating now!
2023-01-06 15:08:47,017 - INFO - Epoch [36/100] train_loss: 0.1890, val_loss: 0.2052, lr: 0.001000, 3.07s
2023-01-06 15:08:49,939 - INFO - epoch complete!
2023-01-06 15:08:49,940 - INFO - evaluating now!
2023-01-06 15:08:50,122 - INFO - Epoch [37/100] train_loss: 0.1778, val_loss: 0.2524, lr: 0.001000, 3.10s
2023-01-06 15:08:53,047 - INFO - epoch complete!
2023-01-06 15:08:53,048 - INFO - evaluating now!
2023-01-06 15:08:53,228 - INFO - Epoch [38/100] train_loss: 0.1745, val_loss: 0.3000, lr: 0.001000, 3.11s
2023-01-06 15:08:56,103 - INFO - epoch complete!
2023-01-06 15:08:56,103 - INFO - evaluating now!
2023-01-06 15:08:56,287 - INFO - Epoch [39/100] train_loss: 0.1697, val_loss: 0.3043, lr: 0.001000, 3.06s
2023-01-06 15:08:59,067 - INFO - epoch complete!
2023-01-06 15:08:59,067 - INFO - evaluating now!
2023-01-06 15:08:59,251 - INFO - Epoch [40/100] train_loss: 0.1784, val_loss: 0.2745, lr: 0.001000, 2.96s
2023-01-06 15:09:02,127 - INFO - epoch complete!
2023-01-06 15:09:02,128 - INFO - evaluating now!
2023-01-06 15:09:02,309 - INFO - Epoch [41/100] train_loss: 0.1713, val_loss: 0.2309, lr: 0.001000, 3.06s
2023-01-06 15:09:05,187 - INFO - epoch complete!
2023-01-06 15:09:05,187 - INFO - evaluating now!
2023-01-06 15:09:05,374 - INFO - Epoch [42/100] train_loss: 0.1723, val_loss: 0.2328, lr: 0.001000, 3.06s
2023-01-06 15:09:08,206 - INFO - epoch complete!
2023-01-06 15:09:08,207 - INFO - evaluating now!
2023-01-06 15:09:08,391 - INFO - Epoch [43/100] train_loss: 0.1701, val_loss: 0.2881, lr: 0.001000, 3.02s
2023-01-06 15:09:11,217 - INFO - epoch complete!
2023-01-06 15:09:11,218 - INFO - evaluating now!
2023-01-06 15:09:11,403 - INFO - Epoch [44/100] train_loss: 0.1669, val_loss: 0.3208, lr: 0.001000, 3.01s
2023-01-06 15:09:14,405 - INFO - epoch complete!
2023-01-06 15:09:14,405 - INFO - evaluating now!
2023-01-06 15:09:14,590 - INFO - Epoch [45/100] train_loss: 0.1669, val_loss: 0.3146, lr: 0.001000, 3.19s
2023-01-06 15:09:17,508 - INFO - epoch complete!
2023-01-06 15:09:17,509 - INFO - evaluating now!
2023-01-06 15:09:17,690 - INFO - Epoch [46/100] train_loss: 0.1607, val_loss: 0.3003, lr: 0.001000, 3.10s
2023-01-06 15:09:20,634 - INFO - epoch complete!
2023-01-06 15:09:20,635 - INFO - evaluating now!
2023-01-06 15:09:20,825 - INFO - Epoch [47/100] train_loss: 0.1696, val_loss: 0.2697, lr: 0.001000, 3.13s
2023-01-06 15:09:23,712 - INFO - epoch complete!
2023-01-06 15:09:23,713 - INFO - evaluating now!
2023-01-06 15:09:23,898 - INFO - Epoch [48/100] train_loss: 0.1717, val_loss: 0.2178, lr: 0.001000, 3.07s
2023-01-06 15:09:26,787 - INFO - epoch complete!
2023-01-06 15:09:26,788 - INFO - evaluating now!
2023-01-06 15:09:27,007 - INFO - Epoch [49/100] train_loss: 0.1712, val_loss: 0.2309, lr: 0.001000, 3.11s
2023-01-06 15:09:30,025 - INFO - epoch complete!
2023-01-06 15:09:30,025 - INFO - evaluating now!
2023-01-06 15:09:30,208 - INFO - Epoch [50/100] train_loss: 0.1631, val_loss: 0.2519, lr: 0.001000, 3.20s
2023-01-06 15:09:33,120 - INFO - epoch complete!
2023-01-06 15:09:33,120 - INFO - evaluating now!
2023-01-06 15:09:33,303 - INFO - Epoch [51/100] train_loss: 0.1630, val_loss: 0.2831, lr: 0.001000, 3.09s
2023-01-06 15:09:36,211 - INFO - epoch complete!
2023-01-06 15:09:36,211 - INFO - evaluating now!
2023-01-06 15:09:36,396 - INFO - Epoch [52/100] train_loss: 0.1548, val_loss: 0.2692, lr: 0.001000, 3.09s
2023-01-06 15:09:39,271 - INFO - epoch complete!
2023-01-06 15:09:39,272 - INFO - evaluating now!
2023-01-06 15:09:39,453 - INFO - Epoch [53/100] train_loss: 0.1510, val_loss: 0.3003, lr: 0.001000, 3.06s
2023-01-06 15:09:42,305 - INFO - epoch complete!
2023-01-06 15:09:42,305 - INFO - evaluating now!
2023-01-06 15:09:42,488 - INFO - Epoch [54/100] train_loss: 0.1488, val_loss: 0.3062, lr: 0.001000, 3.03s
2023-01-06 15:09:45,365 - INFO - epoch complete!
2023-01-06 15:09:45,365 - INFO - evaluating now!
2023-01-06 15:09:45,549 - INFO - Epoch [55/100] train_loss: 0.1490, val_loss: 0.3277, lr: 0.001000, 3.06s
2023-01-06 15:09:48,407 - INFO - epoch complete!
2023-01-06 15:09:48,407 - INFO - evaluating now!
2023-01-06 15:09:48,593 - INFO - Epoch [56/100] train_loss: 0.1545, val_loss: 0.3009, lr: 0.001000, 3.04s
2023-01-06 15:09:51,440 - INFO - epoch complete!
2023-01-06 15:09:51,440 - INFO - evaluating now!
2023-01-06 15:09:51,625 - INFO - Epoch [57/100] train_loss: 0.1601, val_loss: 0.2508, lr: 0.001000, 3.03s
2023-01-06 15:09:54,658 - INFO - epoch complete!
2023-01-06 15:09:54,658 - INFO - evaluating now!
2023-01-06 15:09:54,844 - INFO - Epoch [58/100] train_loss: 0.1542, val_loss: 0.2396, lr: 0.001000, 3.22s
2023-01-06 15:09:57,720 - INFO - epoch complete!
2023-01-06 15:09:57,720 - INFO - evaluating now!
2023-01-06 15:09:57,904 - INFO - Epoch [59/100] train_loss: 0.1527, val_loss: 0.2380, lr: 0.001000, 3.06s
2023-01-06 15:10:00,791 - INFO - epoch complete!
2023-01-06 15:10:00,791 - INFO - evaluating now!
2023-01-06 15:10:00,976 - INFO - Epoch [60/100] train_loss: 0.1546, val_loss: 0.2200, lr: 0.001000, 3.07s
2023-01-06 15:10:03,884 - INFO - epoch complete!
2023-01-06 15:10:03,884 - INFO - evaluating now!
2023-01-06 15:10:04,069 - INFO - Epoch [61/100] train_loss: 0.1534, val_loss: 0.2406, lr: 0.001000, 3.09s
2023-01-06 15:10:07,009 - INFO - epoch complete!
2023-01-06 15:10:07,010 - INFO - evaluating now!
2023-01-06 15:10:07,203 - INFO - Epoch [62/100] train_loss: 0.1477, val_loss: 0.2456, lr: 0.001000, 3.13s
2023-01-06 15:10:10,122 - INFO - epoch complete!
2023-01-06 15:10:10,123 - INFO - evaluating now!
2023-01-06 15:10:10,308 - INFO - Epoch [63/100] train_loss: 0.1526, val_loss: 0.2219, lr: 0.001000, 3.10s
2023-01-06 15:10:13,231 - INFO - epoch complete!
2023-01-06 15:10:13,232 - INFO - evaluating now!
2023-01-06 15:10:13,416 - INFO - Epoch [64/100] train_loss: 0.1377, val_loss: 0.2292, lr: 0.001000, 3.11s
2023-01-06 15:10:16,310 - INFO - epoch complete!
2023-01-06 15:10:16,310 - INFO - evaluating now!
2023-01-06 15:10:16,494 - INFO - Epoch [65/100] train_loss: 0.1432, val_loss: 0.2258, lr: 0.001000, 3.08s
2023-01-06 15:10:19,416 - INFO - epoch complete!
2023-01-06 15:10:19,416 - INFO - evaluating now!
2023-01-06 15:10:19,602 - INFO - Epoch [66/100] train_loss: 0.1342, val_loss: 0.2140, lr: 0.001000, 3.11s
2023-01-06 15:10:22,484 - INFO - epoch complete!
2023-01-06 15:10:22,484 - INFO - evaluating now!
2023-01-06 15:10:22,670 - INFO - Epoch [67/100] train_loss: 0.1353, val_loss: 0.2106, lr: 0.001000, 3.07s
2023-01-06 15:10:25,570 - INFO - epoch complete!
2023-01-06 15:10:25,570 - INFO - evaluating now!
2023-01-06 15:10:25,755 - INFO - Epoch [68/100] train_loss: 0.1394, val_loss: 0.2097, lr: 0.001000, 3.08s
2023-01-06 15:10:28,625 - INFO - epoch complete!
2023-01-06 15:10:28,625 - INFO - evaluating now!
2023-01-06 15:10:28,808 - INFO - Epoch [69/100] train_loss: 0.1278, val_loss: 0.2179, lr: 0.001000, 3.05s
2023-01-06 15:10:31,849 - INFO - epoch complete!
2023-01-06 15:10:31,849 - INFO - evaluating now!
2023-01-06 15:10:32,032 - INFO - Epoch [70/100] train_loss: 0.1332, val_loss: 0.2116, lr: 0.001000, 3.22s
2023-01-06 15:10:34,922 - INFO - epoch complete!
2023-01-06 15:10:34,922 - INFO - evaluating now!
2023-01-06 15:10:35,106 - INFO - Epoch [71/100] train_loss: 0.1255, val_loss: 0.2308, lr: 0.001000, 3.07s
2023-01-06 15:10:38,022 - INFO - epoch complete!
2023-01-06 15:10:38,022 - INFO - evaluating now!
2023-01-06 15:10:38,213 - INFO - Epoch [72/100] train_loss: 0.1278, val_loss: 0.2299, lr: 0.001000, 3.11s
2023-01-06 15:10:41,113 - INFO - epoch complete!
2023-01-06 15:10:41,114 - INFO - evaluating now!
2023-01-06 15:10:41,296 - INFO - Epoch [73/100] train_loss: 0.1356, val_loss: 0.2447, lr: 0.001000, 3.08s
2023-01-06 15:10:44,181 - INFO - epoch complete!
2023-01-06 15:10:44,182 - INFO - evaluating now!
2023-01-06 15:10:44,368 - INFO - Epoch [74/100] train_loss: 0.1291, val_loss: 0.2287, lr: 0.001000, 3.07s
2023-01-06 15:10:47,266 - INFO - epoch complete!
2023-01-06 15:10:47,266 - INFO - evaluating now!
2023-01-06 15:10:47,450 - INFO - Epoch [75/100] train_loss: 0.1318, val_loss: 0.2242, lr: 0.001000, 3.08s
2023-01-06 15:10:50,244 - INFO - epoch complete!
2023-01-06 15:10:50,245 - INFO - evaluating now!
2023-01-06 15:10:50,428 - INFO - Epoch [76/100] train_loss: 0.1381, val_loss: 0.2141, lr: 0.001000, 2.98s
2023-01-06 15:10:53,218 - INFO - epoch complete!
2023-01-06 15:10:53,219 - INFO - evaluating now!
2023-01-06 15:10:53,405 - INFO - Epoch [77/100] train_loss: 0.1350, val_loss: 0.2162, lr: 0.001000, 2.98s
2023-01-06 15:10:56,308 - INFO - epoch complete!
2023-01-06 15:10:56,308 - INFO - evaluating now!
2023-01-06 15:10:56,491 - INFO - Epoch [78/100] train_loss: 0.1299, val_loss: 0.2206, lr: 0.001000, 3.09s
2023-01-06 15:10:59,367 - INFO - epoch complete!
2023-01-06 15:10:59,367 - INFO - evaluating now!
2023-01-06 15:10:59,551 - INFO - Epoch [79/100] train_loss: 0.1283, val_loss: 0.2297, lr: 0.001000, 3.06s
2023-01-06 15:11:02,455 - INFO - epoch complete!
2023-01-06 15:11:02,456 - INFO - evaluating now!
2023-01-06 15:11:02,640 - INFO - Epoch [80/100] train_loss: 0.1234, val_loss: 0.2469, lr: 0.001000, 3.09s
2023-01-06 15:11:05,541 - INFO - epoch complete!
2023-01-06 15:11:05,542 - INFO - evaluating now!
2023-01-06 15:11:05,729 - INFO - Epoch [81/100] train_loss: 0.1278, val_loss: 0.2564, lr: 0.001000, 3.09s
2023-01-06 15:11:08,738 - INFO - epoch complete!
2023-01-06 15:11:08,739 - INFO - evaluating now!
2023-01-06 15:11:08,926 - INFO - Epoch [82/100] train_loss: 0.1209, val_loss: 0.3086, lr: 0.001000, 3.20s
2023-01-06 15:11:11,719 - INFO - epoch complete!
2023-01-06 15:11:11,719 - INFO - evaluating now!
2023-01-06 15:11:11,904 - INFO - Epoch [83/100] train_loss: 0.1236, val_loss: 0.2898, lr: 0.001000, 2.98s
2023-01-06 15:11:14,626 - INFO - epoch complete!
2023-01-06 15:11:14,627 - INFO - evaluating now!
2023-01-06 15:11:14,812 - INFO - Epoch [84/100] train_loss: 0.1178, val_loss: 0.3048, lr: 0.001000, 2.91s
2023-01-06 15:11:17,756 - INFO - epoch complete!
2023-01-06 15:11:17,757 - INFO - evaluating now!
2023-01-06 15:11:17,943 - INFO - Epoch [85/100] train_loss: 0.1159, val_loss: 0.3054, lr: 0.001000, 3.13s
2023-01-06 15:11:20,873 - INFO - epoch complete!
2023-01-06 15:11:20,873 - INFO - evaluating now!
2023-01-06 15:11:21,057 - INFO - Epoch [86/100] train_loss: 0.1318, val_loss: 0.3106, lr: 0.001000, 3.11s
2023-01-06 15:11:23,967 - INFO - epoch complete!
2023-01-06 15:11:23,967 - INFO - evaluating now!
2023-01-06 15:11:24,150 - INFO - Epoch [87/100] train_loss: 0.1266, val_loss: 0.2572, lr: 0.001000, 3.09s
2023-01-06 15:11:27,005 - INFO - epoch complete!
2023-01-06 15:11:27,005 - INFO - evaluating now!
2023-01-06 15:11:27,189 - INFO - Epoch [88/100] train_loss: 0.1340, val_loss: 0.2440, lr: 0.001000, 3.04s
2023-01-06 15:11:30,066 - INFO - epoch complete!
2023-01-06 15:11:30,067 - INFO - evaluating now!
2023-01-06 15:11:30,250 - INFO - Epoch [89/100] train_loss: 0.1212, val_loss: 0.2297, lr: 0.001000, 3.06s
2023-01-06 15:11:33,143 - INFO - epoch complete!
2023-01-06 15:11:33,144 - INFO - evaluating now!
2023-01-06 15:11:33,327 - INFO - Epoch [90/100] train_loss: 0.1135, val_loss: 0.2326, lr: 0.001000, 3.08s
2023-01-06 15:11:36,192 - INFO - epoch complete!
2023-01-06 15:11:36,192 - INFO - evaluating now!
2023-01-06 15:11:36,374 - INFO - Epoch [91/100] train_loss: 0.1133, val_loss: 0.2244, lr: 0.001000, 3.05s
2023-01-06 15:11:39,246 - INFO - epoch complete!
2023-01-06 15:11:39,247 - INFO - evaluating now!
2023-01-06 15:11:39,428 - INFO - Epoch [92/100] train_loss: 0.1003, val_loss: 0.2208, lr: 0.001000, 3.05s
2023-01-06 15:11:42,303 - INFO - epoch complete!
2023-01-06 15:11:42,303 - INFO - evaluating now!
2023-01-06 15:11:42,487 - INFO - Epoch [93/100] train_loss: 0.0942, val_loss: 0.2274, lr: 0.001000, 3.06s
2023-01-06 15:11:45,350 - INFO - epoch complete!
2023-01-06 15:11:45,351 - INFO - evaluating now!
2023-01-06 15:11:45,543 - INFO - Epoch [94/100] train_loss: 0.0946, val_loss: 0.2283, lr: 0.001000, 3.06s
2023-01-06 15:11:48,509 - INFO - epoch complete!
2023-01-06 15:11:48,510 - INFO - evaluating now!
2023-01-06 15:11:48,692 - INFO - Epoch [95/100] train_loss: 0.0910, val_loss: 0.2278, lr: 0.001000, 3.15s
2023-01-06 15:11:51,514 - INFO - epoch complete!
2023-01-06 15:11:51,514 - INFO - evaluating now!
2023-01-06 15:11:51,696 - INFO - Epoch [96/100] train_loss: 0.0993, val_loss: 0.2249, lr: 0.001000, 3.00s
2023-01-06 15:11:54,530 - INFO - epoch complete!
2023-01-06 15:11:54,530 - INFO - evaluating now!
2023-01-06 15:11:54,714 - INFO - Epoch [97/100] train_loss: 0.0988, val_loss: 0.2262, lr: 0.001000, 3.02s
2023-01-06 15:11:57,603 - INFO - epoch complete!
2023-01-06 15:11:57,603 - INFO - evaluating now!
2023-01-06 15:11:57,787 - INFO - Epoch [98/100] train_loss: 0.1025, val_loss: 0.2319, lr: 0.001000, 3.07s
2023-01-06 15:12:00,654 - INFO - epoch complete!
2023-01-06 15:12:00,655 - INFO - evaluating now!
2023-01-06 15:12:00,841 - INFO - Epoch [99/100] train_loss: 0.1055, val_loss: 0.2396, lr: 0.001000, 3.05s
2023-01-06 15:12:00,841 - INFO - Trained totally 100 epochs, average train time is 2.895s, average eval time is 0.184s
2023-01-06 15:12:00,860 - INFO - Loaded model at 14
2023-01-06 15:12:00,860 - INFO - Saved model at ./libcity/cache/2/model_cache/DeepTTE_Beijing_Taxi_Sample_new_longer30.m
2023-01-06 15:12:00,869 - INFO - Start evaluating ...
2023-01-06 15:12:01,497 - INFO - Evaluate result is saved at ./libcity/cache/2/evaluate_cache/2023_01_06_15_12_01_DeepTTE_Beijing_Taxi_Sample_new_longer30.csv
2023-01-06 15:12:01,507 - INFO - 
          MAE      MAPE           MSE        RMSE  masked_MAE  masked_MAPE    masked_MSE  masked_RMSE       R2      EVAR
1  495.294464  0.216082  474867.65625  689.106445  495.294464     0.216082  474867.65625   689.106445  0.59403  0.671792
